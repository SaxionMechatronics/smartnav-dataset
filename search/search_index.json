{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"SMARTNav: A Visual-Inertial Dataset for Reliable Robotic State Estimation","text":"<p>On this page we present the SMARTNav dataset, containing raw sensor data recorded by aerial and ground robots. Collected across multiple projects targeting autonomous navigation in industrial settings, the dataset primarily supports evaluation of Visual-Inertial Odometry (VIO). More broadly, it is suitable for vision and LiDAR-based SLAM as well as multi-sensor fusion research.</p>"},{"location":"index.html#downloads","title":"Downloads","text":"<p>All sequences are provided as ROS 2 bag files. Because each sequence was captured with different hardware for different projects, the available topics vary; however, stereo images and IMU data are included in every sequence. Currently, all bag files are available exclusively via the Saxion Research Cloud Drive (SRCD).</p> <p>Note: For those sequences of this dataset containing RTK data, px4-msgs (it must be release/1.15 branch) is needed. This dependency will be removed in future versions of dataset. </p> Seq. Name Size[GB] Descriptions Link Snapshot greenhouse_flight_3 6.1 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P SRCD greenhouse_flight_5 3.1 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P SRCD greenhouse_ground_1 7.3 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU \u2003 GPS Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P \u2003 Ouster OS1-128 SRCD greenhouse_ground_2 7.7 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU \u2003 GPS Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P \u2003 Ouster OS1-128 SRCD greenhouse_handheld_1 2.4 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P SRCD optitrack_flight_1 0.5 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD optitrack_flight_2 1.7 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD optitrack_handheld_2 1.1 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD optitrack_handheld_3 1.1 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD outdoor_flight_3 2.5 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD outdoor_flight_4 6.8 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD outdoor_flight_5 8.7 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD outdoor_flight_6 3.5 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD sidewalk_ground_1 35.9 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU \u2003 GPS Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P \u2003 Ouster OS1-128 SRCD corridor_ground_1 25.8 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU Sensors: \u2003 ZEDX camera  \u2003 Ouster OS1-128 SRCD"},{"location":"index.html#ground-truth-and-calibration","title":"Ground Truth and Calibration","text":"<p>In the table below, for each sequence, a bag file containing ground-truth topic and well as the sensor calibration data are provided. The GT topic type and calibration for each sequence may differ. Our evaluation tool can handle and compare these different topic types.</p> Seq. Name GT Source GT Link Calibration greenhouse_flight_3 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml greenhouse_flight_5 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml greenhouse_ground_1 LiDAR SLAM (GLIM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml greenhouse_ground_2 LiDAR SLAM (LeGO-LOAM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml greenhouse_handheld_1 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_flight_1 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_flight_2 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_handheld_2 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_handheld_3 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_3 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_4 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_5 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_6 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml sidewalk_ground_1 LiDAR SLAM (LeGO-LOAM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml corridor_ground_1 LiDAR SLAM (LeGO-LOAM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml"},{"location":"index.html#acknowledgements","title":"Acknowledgements","text":"<p>This work was partly supported by the Netherlands Organization for Scientific Research (NWO) via SIA RAAK-Public project (Van bestrijden naar beheersen van de EPR, No.10.015) and SIA RAAK-MKB project (Smart Greenhouses, No.17.014).</p>"},{"location":"calibration.html","title":"Sensor Calibration for SLAM","text":""},{"location":"course.html","title":"Practical Guide to SLAM","text":""},{"location":"deployment.html","title":"Deployment of SLAM","text":""},{"location":"interpretation.html","title":"Interpretation and Evaluation of SLAM Outputs","text":""},{"location":"sensors.html","title":"Sensor Selection for SLAM","text":""},{"location":"sensors.html#chapter-2-sensor-selection-for-slam","title":"Chapter 2: Sensor Selection for SLAM","text":""},{"location":"sensors.html#objectives","title":"Objectives","text":"<p>In this chapter, you will meet the most common sensor types used in SLAM and see where each one shines or struggles. By the end, you are exected to:</p> <ul> <li>What the common sensors are and what information do they offer.</li> <li>Pick a sensible sensor (or a combination) for your own application.</li> <li>Weigh the usual trade-offs: accuracy vs. cost, compute load, environment, and how mature the open-source algorithms for your application are.</li> <li>Spot the key specifications that matter when buying or configuring sensors for SLAM.</li> </ul>"},{"location":"sensors.html#role-of-sensors-in-slam-architecture","title":"Role of Sensors in SLAM Architecture","text":"<p>SLAM is usually split into two parts: a front-end and a back-end.</p> <p>Front-end: It is the part of SLAM that turns raw sensor measurements into compact, useful cues about motion and the scene. If we use a camera for a SLAM algorithm, we will have visual SLAM. Assuming that the camera gives us images of $640 \\times 480$, we will end up with 307200 pixels. This is an enormous amount of information that is very difficult to comprehend and use. The front-end reduces this load by extracting just the most informative bits, so the rest of the system has something manageable to work with.</p> <p>Back-end: It is the core algorithm, that estimates the robot\u2019s pose and builds the map from those cues.</p> <p>Your sensor choice largely shapes the front-end: what you can extract, how robust it is, and how much computation it needs. In short, the sensor defines the front-end\u2019s job, and through that, it influences the back-end\u2019s workload and accuracy.</p>"},{"location":"sensors.html#21-cameras-for-slam","title":"2.1 Cameras for SLAM","text":""},{"location":"sensors.html#211-image","title":"2.1.1 Image","text":"<p>If you\u2019re new to computer vision, here\u2019s the core idea: an image is a matrix of numbers.  Each cell is a pixel, a tiny dot with a color  that defines how a small part of the scene looks like. It should be clear that by increasing the number of pixels, more detail we be captured about the scene.</p>"},{"location":"sensors.html#212-camera-modalities","title":"2.1.2 Camera modalities","text":"<p>The term modality is referring to what kind of light does the camera measure and how it measures it. Most of the cameras capture the visible light, which is a small part of electromagnetic spectrum. If Figure 1, we display this spectrum and the visible light is highlited by rainbow colors. </p> Figure 1: Where common camera modalities sit on the electromagnetic spectrum. <p>Color cameras: These cameras capture a scene in a similar way to how we see it, in different shades of colors. For each pixel of an image, there are 3 values for red, green, blue (RGB) that their combination will create a uniqe color. But the camera sensor does not 3 values (RGB) at each pixel of the physical sensor directly.</p> <p>To capture color images, a Color Filter Array (CFA) is formed, such that on each pixel either an R, G, or B filter is used. An R filter for instance, only allows the red light frequency to pass. For each pixel, the other two values will be interpolated from the neighboring pixels. This process is also known as Demosaicing. Figure 2 diplays the mechanism in which the color images are formed in the sensor.</p> Figure 2: Monochrome vs. color sensing. Color sensors use a mosaic of R/G/B filters while monochrome does not. <p>Monochrom: Unlike color cameras, there is no filter arrays needed for different pixels. In monochrom cameras, all the pixels are used for capturing some portions of the scene, thus the full potential of the physical pixels of the camera is used to capture the scene details, and no approximation through interpolation is usually needed. This is further depicted in Figure 3 by comparing these two sensing modalities.</p> <p>Another consequence of the color filters is that, they narrow down spectrum of the incoming rays of light, only passing a rather narrow band (related to the R, G, or B color). In Figure2, we have tried to show the range of wavelengths that are covered by R filter, G filter, B filter, and finally no filter (monochrome). These are approximation curves and they slightly differ for each real sensor. These curves are usually provided by the manufacturer. As is evident, a monochrome sensor (gray curve), covers a wider range of spectrums compared to the combination of R,G,B filters together. Specefically, it covers more on a portion of the infra-red band, also known as near-infrared. As a result, each pixel will receive less share of light rays (photons so to say); however, a monochrom camera due to lack of the filters can receive more light rays and is more illuminated compared to RGB cameras, improving performance at the darkness.</p> Figure 4: Comparison between images taken from different ranges of electromagnetic spectrum. Image taken from https://doi.org/10.48550/arXiv.2108.11767. <p>To make the discussion more sensible, Figure 4, brings a comparison between images taken from different electromagnetic bands. For instance, the RGB camera captures nice representation of the lit parts of the scene, also providing more semantics through the color information. However, the same scene captured using a monochrom camera that also captures some light in near-infrared (NIR) band can show more of the shadowed section of the scene, while providing a good geometrical understanding of the lit portion, yet missing the color details. The more the camera moves towards higher wavelengths (more towards middle-infrared and far-infrared) the more it turns into a thermal camera, seeing better at darkness and sensing the heat, yet the more specialized the sensors become that comes at a higher cost.</p>"},{"location":"sensors.html#213-camera-configurations","title":"2.1.3 Camera configurations","text":"<p>Monocular camera: It captures image data through a single lens. The benefit is that it is the most compact and efficient sensor setup in robotics, and SLAM systems. The drawback is that, this makes it more complicated to understand the geometry of the scene. </p> <p>Intuitively, it is not possible to understand the other dimensions of the real-world objects (also known as depth estimation), unless you observe them from at least two different points of view. This 3D understanding about the surrounding objects is central to the SLAM methodologies. Using only a single camera seems to prevent us from understanding the distances of the scene components from us; however, there are a few tricks that will enable depth estimation even using monocular cameras.</p> <p>The traditional approach is to move the camera around and use the observation of the same object in multiple frames at different times. Although the depth estimation will not be at hand at every frame, but after some time, assuming sufficient spatial translations, we should be to look at the same targets from different angles. This allows us to do simple geometrical analysis also known as Triangulation to measure objects' distances from the camera.</p> <p>There is a more recent approach to solve the monocular depth estimation problem, which is taking advantage of deep neural networks. These networks are trained using massive datasets, learning how to estimate the relative distances of object from camera only using a single image. This depth estimation based on experience is kind of similar to the way a human can infer depth if he covers one of his eyes, such that he still can understand distances but mostly because of experience. </p> <p>Stereo camera: If two monocular cameras are attached together, such that they share most of their view, it is a stereo camera setup. This is very similar to human way of observing the environment. In this setup, at every timestamp, the calculation of the scene depth is possible, since we already have two different views of the object. Simlar triangulation techniques, or more complicated methods are possible to be used here. The accuracy of the output depth estimation relies on many factors such as the algorithm. The maximum range of depth estimation depend on how separated the two cameras are from each other (also called the stereo baseline). </p> Figure 1: The comparison between monocular and stereo cameras in terms of 3D scene understanding. <p>In the Figure 5, examples of monocular and stereo cameras are highlighted. As evident, for a monocular camera, only by having dicplacements with respect to an object in the scene (the star shape represents an object), it is possible to create a virtual stereo camera by putting together the pairs of images from different times (given that they are far apart sufficiently). Then, a triangle can be assumed, however, all three side of the triangle are unknown, yet using two consecutive computer vision methods, namely the Five Points method, then Perspective n Points, a value for these sides will be at hand which is proportionally correct, however, does not have the right metric scale. The details for these algorithms is for interested readers. At the end, it should be emphasized that the monocular camera can not yield the right metric values for the depth of the features and only can deliver values for distances of objects that are correct relative to each other (correct proportions). If we want to convert them to correct metric values, another hint (another sensor such as IMU) should be incorporated.</p> <p>By comparison, the stereo camera in Figure 5, forms a similar triangle, however in one time instance and not across two different time instances. Moreover, the triangle that is formed for on object in the scene, has one known side (which is the baseline since we know how much apart are the two cameras from each other). As such, an easier triangulation with correct metric scale is possible without the need for additional hints.</p>"},{"location":"sensors.html#212-camera-based-slam-front-end","title":"2.1.2 Camera-based SLAM Front-end","text":"<p>Cameras provide the richest set of information among the available solutions for SLAM. The real challenge of using and dealing with camera is how to extract the necessary information out of them.</p> <p>When you choose cameras for your SLAM problem, you will often end up with a general architecture. We depict this general architecture in Figure 6. It does not mean that all SLAM algorithms are exactly following this architecture, but gives a general ideea of what happens in the camera-based SLAM.</p> Figure 6: Camera-based SLAM architecture. Taken from the <code>robor_test_3</code> sequence in SMARTNav dataset. <p>As emphasized previously, cameras spit out a lot of pixeled data. The first step is to Extract Features, ie. only keep the information that are necessary and useful. In Figure 6, you can see that some common ways of extracting features are depicted. For instance, extracting the edges in the picture, or extracting feature points (mostly the corner or sharp points in the environment that are easily detectable), or even lines are detected and extracted from image. These are popular feature extraction techniques because of two main reasons. I) It is relatively easy and quick to identify these features using classical computer vision II) It is possible to track these features accross different consecutive image frames. Among these intermediate representations, feature points are the most comonly used method.</p> <p>After feature extraction, it is important to maintain a unique identity for the features across different frames. As an example, if you see the corners of the ceiling of the house, in Figure 6, as a unique feature in the environment, you should be able to detect the same spot at a few moments later and you should be able to understand that this corner is the same that you saw before. If you successfully do this, you have performed a correct Feature Tracking. Figure 6, also shows an example depiction of feature tracking, where the features are tracked across two consecutive image frames. The location of these features in the first and second frame is drawn by the red dots and the green lines have established a connection between the latest and former location of the features.</p> <p>Although, the tracking of the features of environment is essential to the SLAM, providing a sense of relative motion with respect to the environment, it is not enough to realize a 3D measuremt of motion. The feature points should not be only tracked in the 2 dimensional space of the image, but also their 3D location in the space must be determined. So for each feature point, we can estimate a 3D location from the images, that is not trivial, but achievable through either monocular or stere-based camera configurations. In Figure 2, the uplifting of 2 dimensional features from image space into the 3D space is visualized such that a metric distance between the camera and the spatial position of that feature is at hand.</p>"},{"location":"sensors.html#214-sensor-requirements-what-to-buy-why","title":"2.1.4 Sensor requirements (what to buy &amp; why)","text":"<p>The camera to be used in SLAM should have specific characteristics, some of which are listed in the table below. Note that some of more detailed specs are useful when a custom camera sensor and lens is being selected, while many cameras used in robotics, have a good trade-off between these specs.</p> <ul> <li>Field of View (FOV): For SLAM methods, a high FOV, is usually desireable, to see a higher number of features at the same time in the environment, and to be able to pick best features to track. To avoid being blinded or obstructed. It is worth noting that very large field of view (above 120 degrees) will be a bit challenging for calibration (one step before deploying your SLAM) such that the available open-source calibration tool will have difficulties yielding accurate calibration results.</li> </ul> Figure 7: Visualization of the difference between narrow (left image) and wide (right image) FOV, and its effect on feature tracking. <p>Figure 7, compares the effect of having a wider FOV. The red dots are the spots in the image that usually considered as feature points in a the SLAM front-end, and they are extracted using a common feature detection algorithm used in many SLAM methods. In the wider FOV image, it is usually easier to have higher quality features (unique corner points for instance), while in the low field of view, the algorithms has to rely on lower quality features (like the many points insie the trees that are less unique) to maintain a required number of feature points. The other difference is that while having the wide FOV, in order to lose sight of all these features at once, you should have a larger sudden motion from the camera. The drawback of the wide-FOV lenses usually is in the form of the distortion of the image. The straight lines, and planes are curved, speceially close to the borders of the image, and the higher the FOV, the more spherical pattern is induced on the image. These distrotions have to be rectified in the calibration stage.</p> <ul> <li> <p>Resolution: Low resolution can prevent visual SLAM from proper tracking of features. High resolution will introduce unnecessary processing loads to the system, specially keeping the buses (USB for instance) busy, making it difficult to record the data, and adding more delay in position tracking and map updating due to time taken for tracking feature in images. </p> </li> <li> <p>Shutter: Global shutter sensors are ideal for visual SLAM to minimize motion blur. To eliminate rolling shutter effect, that skews moving objects in the image and give unrealistic depiction of scenes in case of motion. Interesting to note that rolling shutter is in contrast with image resolution due to limitation in the technology behind rolling shutter, such that if you want a high-resolution rolling shutter sensor, the price increases. However, most of the cheap rolling shutter camera have an adequate resolution for our application.</p> </li> </ul> Figure 8: Comparison between the image taken from a global shutter camera (left) and the one taken from rolling shutter (right) due to the motion pattern in scene (middle). The image is taken from the <code>corridor_ground_1</code> sequence in SMARTNav dataset. <p>The difference between the image taken from a roller shutter and one taken from global, is illustrated in Figure 8 (the effect is simulated on the rolling shutter image). As evident, the distortion cause by motion, make the rolling shutter image to yield an undrealistic representation of the scene (all the vertical line are bent closer to the edges) along with more blurring on high-speed parts in the image. The amount of rolling shutter effect is dependent on the magnitude of speed. Meaning that in applications that the robot is moving slow or high magnitude and high frequency vibrations are not expected, this effect might be negiligle, however, for more dynamic use cases, a global shutter sensor is advised.</p> <ul> <li> <p>Data Rate: A higher streaming rate of the camera can help in fast motion so that the feature tracks dont get lost. Here, by high we mean higher that the norm in most of the cameras which is usually 30 Frames per Second (FPS); while having an excessively high FPS (&gt; 90 FPS) might be unnecessary due to the fact that most of the SLAM will not be able to handle this high frequency stream of data and they have to drop some of the frames to keep up.</p> </li> <li> <p>Dynamic Range: Dynamic range indicates the ability of the camera to capture details in dark or bright areas of an image. It is defined as the difference between brightest light and darkest dark in the image. Any light above this range will be diplayed as white and anything below it is pitch dark. This difference is also visible in Figure 9.</p> </li> </ul> Figure 9: Difference beween low dynamic range (left image) and high dynamic range (right image) cameras. <ul> <li>Bit-rate: The normal image data that is given by cameras is an array on 8-bit pixels. This means that each pixel value can be reported by a combination of 8 zeros or one. As a result, each pixel might have $2^8 = 256$ values. In simpler terms, a normal 8-bit camera can capture colors in 256 different shades. If we can capture higher bit-rates such as 10, the number shades we can capture will be immediately increased to 1024, which translate to far more details in cases that the lighting of the environment is uniform (dark for instance) or the textures have very close colors.</li> </ul> Figure 10: An example of an 8-image in a low light environment is presented on the left. On the right, example of a 10-bit image from the same environment. The image taken as part of the firefighting robot within the SMART group. <p>The above figure shows on the left side an image that was captured and converted to 8-bit format (256 shades). For this image a hostogram of different pixel values is drawn as well. As visible in the histogram, most of the pixel color are concentrated in one small region, meaning that all colors in the image are close to each other and having a general dark vibe. On the right side, all 10-bit (1024 shade of color) where used. The important thing to note is that our displays and most of computer vision functions are not able to process the higher bit-rates that 8, meaning that an algorithm should be used to convert this higher bit-rate to 8, while maintaing and intensifying the details. The exact details about such an algorithm is beyond this guide, but as a comparison, you can see the result of a better use of extra shades of color and a correct conversion in the right image of Figure 10.</p> <ul> <li>Lens: This item mostly depends on how custom you want your sensor setup to be. If you want to be able to keep your core sensor (resolution, shutter, rate, and connection remain the same) but you want to be able to change or experiment with different FOVs, you can go for a sensor that allows you to change the lens. This information can be found in the datasheets of the sensor, for instance an m-12 lens mount is an ideal option in the market, while those that are mentioned to have a stock lens are not changeable.</li> </ul> Figure 11: A camera with an unchangable stock lens (left), one with an M12 mount (center), and different options for lens yielding different FOVs and sometimes difference frequency band cuts and IR filters (right). <ul> <li>Connection: Flat cable connection to the onboard computer (Jetson) help with faster data transfer and reduces undesirable delays and bandwidth limitations of the USB port. However, they are ideal when the camera is close to the processor board, otherwise, these cable do not have the needed flexibility and USB3 interface will be more ideal as a balance between data transfer speed and design flexibility. USB2 camera are not very desireable for SLAM and generally embedded vision applications. When making the decision about going for a USB3 camera or a camera with flat cables, other than the mechanical design aspects, the compatibility with the processing unit (whether the embedded computer has relevant port) should be taken into account.</li> </ul> Figure 12: Example of a camera having a USB connection (left) and a camera with flat cable connection (right). <ul> <li> <p>Pixel size: The higher the pixel size, the better low light performance.</p> </li> <li> <p>Optical size/format: In case you go with an adjustable lens option for the sensor, you should make sure the the optical format provided in lens and sensor are compatible. This size determines the effective area of sensor that captures the scene. If it is smaller than that of the lens, a big part of the scene taken by lens will go out of image boundaries. If it is larger than that of lens, a big dark area will appear around the captures scene in the final image.</p> </li> <li> <p>Driver: When buying a camera, if the vendor provides you with a driver that is compatible with embedded computers (usually having aarch64/arm64 processor architecture as opposed to amd64 which is common in the desktops) and if they have a ROS2 wrapper for their sensor, it is going to make life much easier and it will be mostly plug and play. </p> </li> </ul>"},{"location":"sensors.html#216-available-solutions-examples","title":"2.1.6 Available solutions (examples)","text":"<p>In the table below, we bring a few example of some commonly available cameras in the market, and compare them base on the discussed criteria.</p> Camera Modality Horizontal FOV Max resolution Shutter Max FPS Dynamic Range (dB) Max bit rate Lens Connection Pixel size ($\u00b5m^2$) Driver Realsense D455 monochrome 87\u00b0 1280 \u00d7 800 global 30 - 10 fixed USB 3.1 3 x 3 Embedded Linux and ROS2 support ZED 2 color 110\u00b0 2208 x 1242 rolling 100 64 8 fixed USB 3.0 2 x 2 Embedded Linux and ROS2 support ZED X color 110\u00b0 1920 x 1200 global 100 71 8 fixed GMSL2 3 x 3 Embedded Linux and ROS2 support Raspberry Pi Camera Module 3 Wide color 102\u00b0 4608 \u00d7 2592 rolling 60 up to 96 in HDR mode 10 stock lens Flat cable 1.4 \u00d7 1.4 Embedded Linux support Arducam multi-cam board with OV9282 base sensors color adjustable 1280 \u00d7 720 global 150 68 10 M12 mount Flat cable 3 \u00d7 3 Embedded Linux support <p>We assigned characteristics of the mentioned sensors in either green or red, a harsh thresholding that of course can vary based on specific applications. For instance, if you prefer having a colored 3D map of the environment, monochrome sensors are probabely not what you are looking for. In this version of the table, we preferred low light performance, thus the monochrome is marked as green and colored sensors are red.</p> <p>Another improtant thing to note is that the maximum of some of these specifications does not happen at the same time. For instance, the max resolution usually leads to a lower FPS than the max FPS reported in the table. However, when we set some of the maximum resolutions to red, it means that for a typical SLAM application with most of the available open-source method, such high resolutions are an overkill and can needlessly increase the sensor price. </p> <p>There are other consideration related to these cameras at the marker, such as out of the box softwares that can give you additional functionalities. For instance the ZED cameras and Intel Realsense, usually have yield an out of the box depth estimation which can be useful in some SLAMs. However, in some cases such as the ZED cameras, it adds another requirement which is GPU-equipped embedded computers (such as Nvidia Jetson family).</p> <p>In a nutshell, for a beginner-level introduction to the world of SLAM or generally computer vision, ready-to-use options such as ZED cameras or Realsense D455 is a good choice. For having more control over the sensor and customizing to your robot, while having maximum control over the suitability of the sensor, custom sensor boards such the Arducam is an attractive option.</p>"},{"location":"sensors.html#218-advantages-and-disadvantages-of-visual-slam","title":"2.1.8 Advantages and disadvantages of visual SLAM","text":"<p>There are many advantages to the cameras, that make them a suitable choice for SLAM in robotics.  - They are rich. They see almost everything that a human doest. - They are usually lightweight, that makes them perfect for robots which weight is a determinig factor. - They are also energy efficient. - Inclusion of the cammera has the minimum design burden compared to LiDAR, RADAR, Compass, GNSS, and even the IMU. Meaning that they can be put flexibly on any part of the robot without much constraint on your design. - They can be used for other purposes. For instance you can put a camera for running your SLAM but also you can use the stream of image to perform another vision task, such as identifying a subject.</p> <p>The cameras also come with some disadvantages: - They are not able to directly understand the geometry of the scene. Understanding the geometry of the scene (how far things are from the camera) is necessary in SLAM and there are techniques to understand the distances but these techniques are not 100% reliable and have many limitations such as range and the textures in the environment. - The volume of data that is to be handled by camera is relatively high. - Due to the handling of the camera data, they often introduce delay to system, happening at the data transfer from sensor to the computer and copy pastings at Operating System (OS) level. - The SLAM algorithms that use camera are usually heavier than the ones using other modalities. This further processing often happens during feature extraction and 3D understanding of the environment - Normal RGB cameras dont see at darkness.  - There are some challenges related to SLAM algorithms such as the motion blur in very dynamic cases. Also in environments with no texture (flat uniform surface or reflective surfaces), the visual SLAM perfrom poorly. Although we introduced some paramters in this section to have a camera with good performance in challenging lighting conditions (low light for instance), however, there are limits to what camera can clearly see.</p>"},{"location":"sensors.html#22-lidar-for-slam","title":"2.2 LiDAR for SLAM","text":"<p>LiDAR stands for Light Detection and Ranging, a technology that is less broad and generic compared to cameras, and is often more suitable and specialized for robotic applications.</p>"},{"location":"sensors.html#221-lidar-mechanism","title":"2.2.1 LiDAR Mechanism","text":"<p>LiDAR operates based on the principle of sending out a laser beam at a very specific and narrow direction in space, and also waits for the reflection of that beam, then, based on the traveling time of the beam (also known as Time of Flight or ToF), having the speed of light in mind, it measures the distance of the nearest object in that direction of the space.</p> <p>This ToF-based range measurement usually occurs multiple times and for multiple directions in space, giving an estimation of the environment geomitrical shape.</p> Figure 13: The concept of LiDAR mechanism taken LiDAR Wikipedia. On the left, a simplified design for LiDAR sensor is displayed, including the laser source and the rotating mirror that reflects the beam in different spatial directions. In the middle figure, the top view of the space around the sensor is shown in form of rectangular room with the LiDAR (blue rectangle) and another round object (green circle). After capturing the time of flight for the beam in each direction and measuring the distance, a 2 dimensional map can be formed as displayed in the right image."},{"location":"sensors.html#222-pointcloud","title":"2.2.2 Pointcloud","text":"<p>Just like the cameras that output an image as their data format, LiDARs produce pointclouds. A pointcloud is a set of 3D points in space. These points have $x, y, z$ coordinates. These are points in the coordinate frame of the sensor, meaning that they are relative to the sensor itself and still do not have any spatial meaning relative to a world-related reference frame. In fact, making sure that these points are coordinates in a world related reference frame is one of the tasks that our SLAM will do.</p> Figure 14: Visualization of LiDAR pointcloud taken from the <code>corridor_ground_1</code> sequence of the SMARTNav dataset. <p>The immidiate outputs of a LiDAR sensor are depicted in Figure 14. The top left image, is a camera image, to give you an understanding of how the environment in front of the robot is supposed to look like. The top right is the 3D pointcloud of the environment. This pointcloud is measured by a LiDAR sensor that has a 360 view, so it sees more than just the front of the robot (unlike the camera). The robot is actually located in the middle of the empty circle. By comparing the geometry of the scene as a human can percieve from the image, you can observe that pointcloud is almost perfectly resempling the area around the robot. The colors of each point is based on its height, where the higher point have a more reddish color. In bottom left you can see the pointcloud closely from camera's point of view, and it is more evident that the 3D geometry of the scene matches the camera image. In bottom right, is another top view, but this time it can be observed that the density of the points is increasing with the distance from the sensor. So a LiDAR sensor has a much denser representation of the scene at its vecinity compared to farther points.</p>"},{"location":"sensors.html#223-configurations-and-examples","title":"2.2.3 Configurations and Examples","text":"<p>LiDAR mechanism and technology is used in different design configurations. Having a general understanding about these designs can help to  identify the best option for each application.</p> <ul> <li> <p>LiDAR Range Finders:   This is the simplest version that only measure the distance in one spatial direction and is not often used in SLAMs but very common in robotics especially the aerial robots. For instance it can be attached to the bottom of a drone to measure a more accurate distance to the ground.</p> </li> <li> <p>2D Planar Scanners:    These are the most commong LiDAR sensors in ground robots. 2D scanner LiDAR, at each scan, give a two dimensional obstacle view, at the same horizontal plane of the sensor. The mechanism of planar scanners is much similar to what is portrayed in Figure 13. Their advantage is their simplicity, good price, but the can not sense anything that is not on the same plane as the sensor plane. </p> </li> </ul> <p>Figure 15, top-left shows and example of such a planar scanner. As can be seen, the sensor can see the table legs in its scanning   rplidar and tilting using a gimbal (hokoyo) - 3D Multi-beam: - Non-repetitive Scan: - Flash LiDAR: - Solid-state sensors</p> Figure 15: Diffetent data patterns for LiDAR sensors. The sensor is assumed to be the black cylinder in the middle of the room, and the 3D point measurements are the red points projected on the environment."},{"location":"sensors.html#224-lidar-based-front-end-architecture","title":"2.2.4 LiDAR-based front-end architecture","text":""},{"location":"sensors.html#225-good-specs-for-lidar","title":"2.2.5 Good specs for LiDAR","text":""},{"location":"sensors.html#226-sensor-examples","title":"2.2.6 Sensor examples","text":""},{"location":"sensors.html#227-advantages-and-disadvantages","title":"2.2.7 Advantages and Disadvantages","text":""},{"location":"sensors.html#23-imu","title":"2.3 IMU","text":""},{"location":"sensors.html#231-concept","title":"2.3.1 Concept","text":""},{"location":"sensors.html#232-imu-data","title":"2.3.2 IMU data","text":""},{"location":"sensors.html#233-benefits","title":"2.3.3 Benefits","text":""},{"location":"sensors.html#234-front-end-of-imu-aided-slam","title":"2.3.4 Front-end of IMU-aided SLAM","text":""},{"location":"sensors.html#235-good-specs-of-imu-sensors","title":"2.3.5 Good specs of IMU sensors","text":""},{"location":"sensors.html#24-other-sensors","title":"2.4 Other sensors","text":""},{"location":"sensors.html#241-wheel-odmetry","title":"2.4.1 Wheel odmetry","text":""},{"location":"sensors.html#242-ultrasonic","title":"2.4.2 Ultrasonic","text":""},{"location":"sensors.html#243-event-camera","title":"2.4.3 Event camera","text":""},{"location":"sensors.html#244-radar","title":"2.4.4 Radar","text":""},{"location":"sensors.html#245-gnss","title":"2.4.5 GNSS","text":""},{"location":"sensors.html#25-practical-considerations","title":"2.5 Practical Considerations","text":""},{"location":"sensors.html#251-damping","title":"2.5.1 Damping","text":""},{"location":"sensors.html#252-interference-magnetic-field-or-ir-camera-and-lidar-may-interfere","title":"2.5.2 Interference (magnetic field or IR camera, and LiDAR may interfere)","text":"<p>--&gt;</p>"},{"location":"sensors.html#quizes","title":"Quizes","text":"Q1. Which of these sensors does not directly measure the external environment? 2D planar LiDAR Monocular camera IMU Thermal camera IMU measures the platform\u2019s motion (accelerations/rotation), not scene geometry or appearance.  Q2. In a SLAM system, which part extracts useful information (features) from raw images? Front-end Back-end Loop closer Map optimizer Your text defines the front-end as turning raw measurements into compact cues (features, lines, etc.). Q3. Which data source usually captures the most accurate geometric detail of the scene in front of the robot? Color image Monochrome image (same resolution) 3D LiDAR RGB-D camera LiDAR directly measures range to build a 3D point cloud; cameras need inference/triangulation and RGB-D is range-limited.  Q4. If we want to use perform a 3D mapping of environment using a low-cost indoor search-and-rescue drone, what will be the best camera choice? Monocular thermal camera Stereo monochrome Stereo color camera Mono color camera Stereo gives per-frame depth; monochrome improves low-light and SNR vs. color CFA.  Q5. Outdoor aerial SLAM at ~100\u00a0m altitude: which stereo parameter matters most to change? Dynamic range Shutter type Modality (color/mono) Baseline Depth precision scales with focal length in pixels and baseline; large distances need a larger baseline. Q6. Which camera example in your table is best for moving from dim indoors to bright sun (high DR jumps)? Intel RealSense D455 ZED 2 Raspberry Pi Camera Module 3 Wide Arducam OV9282 (on multi-cam board) Pi Cam 3 Wide lists \u201cup to 96\u00a0dB in HDR mode,\u201d higher than the others shown. Q7. Pair of front-facing cameras on a car; compute is in the dashboard (long cable run). Which sensor choice is most practical? Arducam multi-cam board (CSI) ZED 2 Raspberry Pi Camera Module 3 Arducam OV9282 bare sensor ZED 2 is integrated stereo over USB\u00a03 (longer run, less CSI cabling complexity than flat cables). Q8. Which camera characteristic does not improve low-light performance directly? Modality (mono vs. color) Shutter type Bit depth / bit rate Pixel size Higher bit depth improves quantization fidelity, not SNR; larger pixels/mono help capture more photons. Q9. A floor-cleaning indoor robot (sports arena): which sensor is the most suitable primary mapper? Monocular camera 2D planar LiDAR RGB-D camera Thermal camera 2D LiDAR is a proven workhorse for indoor navigation and coverage in planar spaces. Q10. Which combo typically yields the most accurate and robust VIO/SLAM? Stereo camera + IMU Planar LiDAR + IMU Monocular camera + IMU 3D LiDAR (alone) Stereo offers metric scale per frame; IMU stabilizes fast motion\u2014this pairing is widely adopted. Q11. Which feature type is shown as red dots linked across frames in your workflow figure? Corner/keypoints Semantic segments Superpixels Dense optical flow The front-end example tracks corner-like feature points (red dots) with lines between frames. Q12. Which shutter type reduces motion-induced geometric skew in fast motion? Global shutter Rolling shutter Electronic first-curtain Mechanical curtain Your text illustrates rolling-shutter distortion vs. global-shutter stability.  Q13. Increasing horizontal FOV from 70\u00b0 to ~110\u00b0 mainly helps the SLAM front-end by\u2026 Seeing more\u2014and better distributed\u2014trackable features Eliminating lens distortion Reducing compute cost Improving color fidelity Wider FOV tends to increase unique features and robustness to temporary occlusions. Q14. A downside of very wide FOV (&gt;120\u00b0) in your notes is\u2026 Calibration becomes harder (distortion models &amp; tools) Lower dynamic range by physics Global shutter becomes impossible No lens options exist You explicitly warn that calibration tools struggle as FOV gets very large. Q15. Bit depth mainly affects\u2026 Number of tonal/shade levels captured Physical sensor size Depth range of stereo Laser safety class 10-bit gives 1024 levels vs 256 in 8-bit; your figure shows the histogram example. Q16. Which is a benefit of monochrome over color for SLAM? Higher sensitivity (no CFA loss) and better low-light SNR Always higher resolution sensor No calibration needed Free depth measurements No color filters \u2192 more photons per pixel; you discuss NIR response and SNR.  Q17. Which connection is usually easier for long runs to a dashboard PC? USB\u00a03 Flat CSI cable GPIO I2C You note CSI/flat cables suit short distances near the compute board; USB3 gives more flexibility.  Q18. What is a LiDAR\u2019s immediate data product? Raster image 3D point cloud (XYZ) IMU delta pose Height map only Your LiDAR section emphasizes ToF \u2192 3D point cloud in sensor coordinates.  Q19. Which LiDAR configuration sweeps a single plane? 2D planar scanner 3D spinning multi-beam Flash LiDAR Non-repetitive sector scan Planar scanners produce a polar sweep in one plane (common for indoor robots).  Q20. What\u2019s the main trade-off of very high resolution for cameras in SLAM? Higher compute and bandwidth, added latency Lower lens distortion automatically Higher dynamic range automatically No calibration needed You warn that high resolution burdens buses/compute and increases tracking latency.  Q21. Which lens option gives you flexibility to change FOV later? M12-mount lens camera Fixed stock lens only No-lens pinhole Any USB webcam Your lens section contrasts fixed stock lenses with M12-mount swappable lenses.  Q22. Which rolling-shutter artifact did you visualize in your figures? Skewed/bent verticals during motion Random dead pixels Banding from PWM lights only Chromatic aberration at edges Your example shows verticals bending near the edges under motion with RS readout."}]}