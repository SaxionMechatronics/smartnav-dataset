{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"SMARTNav: A Visual-Inertial Dataset for Reliable Robotic State Estimation","text":"<p>In this webpage, we present the SMARTNav dataset, which stores the raw sensor data recorded by aerial and ground robots. The data was collected as part of our projects that address the challenges of autonomous navigation in industries where autonomous robots are needed. This data was mainly recorded for the evaluation of Visual-Inertial Odometry (VIO) algorithms; however, it can be used generally for vision or LiDAR-based SLAM and fusion algorithms. </p>"},{"location":"index.html#downloads","title":"Downloads","text":"<p>The sequences of the dataset were recorded in ROS2 bag format, and can be downloaded from the following links. Since each sequence is recorded on a different hardware and for a different project, the content of each bag file is different, but inclusion of stereo images and IMU is constant in all bag files. Currently, all the bag files are only accessible through Saxion Research Cloud Drive (SRCD).</p> <p>Note: For those sequences of this dataset containing RTK data, px4-msgs (it must be release/1.15 branch) is needed. This dependency will be removed in future versions of dataset. </p> Seq. Name Size[GB] Descriptions Link Snapshot greenhouse_flight_3 6.1 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P SRCD greenhouse_flight_5 3.1 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P SRCD greenhouse_ground_1 7.3 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU \u2003 GPS Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P \u2003 Ouster OS1-128 SRCD greenhouse_ground_2 7.7 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU \u2003 GPS Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P \u2003 Ouster OS1-128 SRCD greenhouse_handheld_1 2.4 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P SRCD optitrack_flight_1 0.5 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD optitrack_flight_2 1.7 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD optitrack_handheld_2 1.1 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD optitrack_handheld_3 1.1 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD outdoor_flight_3 2.5 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD outdoor_flight_4 6.8 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD outdoor_flight_5 8.7 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD outdoor_flight_6 3.5 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD sidewalk_ground_1 35.9 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU \u2003 GPS Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P \u2003 Ouster OS1-128 SRCD corridor_ground_1 25.8 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU Sensors: \u2003 ZEDX camera  \u2003 Ouster OS1-128 SRCD"},{"location":"index.html#ground-truth-and-calibration","title":"Ground Truth and Calibration","text":"<p>In the table below, for each sequence, a bag file containing ground-truth topic and well as the sensor calibration data are provided. The GT topic type and calibration for each sequence may differ. Our evaluation tool can handle and compare these different topic types.</p> Seq. Name GT Source GT Link Calibration greenhouse_flight_3 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml greenhouse_flight_5 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml greenhouse_ground_1 LiDAR SLAM (GLIM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml greenhouse_ground_2 LiDAR SLAM (LeGO-LOAM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml greenhouse_handheld_1 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_flight_1 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_flight_2 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_handheld_2 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_handheld_3 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_3 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_4 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_5 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_6 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml sidewalk_ground_1 LiDAR SLAM (LeGO-LOAM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml corridor_ground_1 LiDAR SLAM (LeGO-LOAM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml"},{"location":"index.html#acknowledgements","title":"Acknowledgements","text":"<p>This work was partly supported by the Netherlands Organization for Scientific Research (NWO) via SIA RAAK-Public project (Van bestrijden naar beheersen van de EPR, No.10.015) and SIA RAAK-MKB project (Smart Greenhouses, No.17.014).</p>"},{"location":"calibration.html","title":"Sensor Calibration for SLAM","text":""},{"location":"course.html","title":"Practical Guide to SLAM","text":""},{"location":"deployment.html","title":"Deployment of SLAM","text":""},{"location":"interpretation.html","title":"Interpretation and Evaluation of SLAM Outputs","text":""},{"location":"sensors.html","title":"Chapter 2: Sensor Selection for SLAM","text":""},{"location":"sensors.html#objectives","title":"Objectives","text":"<p>The objective of this chapter is to get an introduction to different sensing modalities that are common in SLAM. Furthermore, we will discuss advantages and disadvantages of each sensing modality.</p> <p>At the end of this chapter, it is expected that the reader will be able to: - Identify the suitable set of sensors for their application - They will be familiar with the common trade-offs regarding the SLAM sensors, including the trade-offs between accuracy, cost, computation, environment, and maturity of algorithms. - They will have an idea of which specifications to consider when choosing sensors for SLAM</p>"},{"location":"sensors.html#role-of-sensors-in-slam-architecture","title":"Role of Sensors in SLAM Architecture","text":"<p>SLAM often has two separate internal components, namely the front-end and back-end. </p> <p>Front-end: It is the module that turns raw sensor measurements of sensors into an intermediate representations that are more compact and informative about robot's or environment's state. For example, if a camera sensor is used for SLAM, it outputs images at a resolution of let's say 640x480. This means that we will have 307200 pixels each conveying some information about the environment. This is an enormous amount of information that is very difficult to comprehend and use. Hence, we need a set of processes to filter out this information and only extract a minimal set of relevant information that are insightful about robot's motion and states.</p> <p>Back-end: It is the core algorithm, usually in the form of optimization or filtering, that takes the intermediate representations and outputs the robot's location and environment's map. The details of SLAM backend is not the focus of this course.</p> <p>As evident from the above definitions, choice of sensor defines the SLAM front-end architecture.</p>"},{"location":"sensors.html#21-cameras-for-slam","title":"2.1 Cameras for SLAM","text":""},{"location":"sensors.html#211-image","title":"2.1.1 Image","text":"<p>If you have previous experince with compputer vision or image processing, you can not find a more obvious information other than explaining what an image is! Otherwise, it is worth introducing you to the direct product of a camera. An image in computers is represented as a matrix of numbers. Each cell in the matrix is called a pixel, a discrete piece of infomation, determining how a tiny portion of the view looks like (Figure 1). As such, it must be intuitively easy to imagine that the higher the number of tiny pixels besides each other, the more clear picture of the view will be formed.</p> Figure 1: Pixels, the building blocks of an image."},{"location":"sensors.html#212-camera-modalities","title":"2.1.2 Camera modalities","text":"<p>Cameras come at many different measurement modalities, such that the images they form will look different or measure different thing. The cameras that are usually common in robotic applications such as SLAM are mostly covering the visible light wavelengths. Figure 2, portrays the electromagnetic spectrum, and the whereabouts of the visible part of the spectrum (rainbow section) clearly indicates the target wavelengths that the cameras usually measure.</p> Figure 2: What each sensing modality of cameras measures. <p>Color cameras: These cameras are meant to capture a scene exactly in the most similar manner to the way we see the environment, by presenting the information in different shades of colors. In order to understand an important characteristics of the cameras that can be influential is robot perception and navigation, we must delve a bit deeper into how cameras work.</p> <p>In the spectrum of the Figure 2, the wavelengths between  380-700 nm is covered by color cameras. However, the way that a color camera can capture the actual color is through a specific mechanism. In fact, a color camera is able to measure 3 values for each pixel, i) the intensity of the blue ii) green and iii) red colors. The sensitivity of the pixel to each of these colors, follows the distribution presented in Figure 2. For instance, a color corresponding to the frequency 400 nm, will mostly excite a blue color and less green and much less red.</p> <p>However, the capturing of these 3 numbers at the same time follows an interesting engineering trick in the design of color sensors. As depicted in Figure 3, color cameras don\u2019t record three color numbers per pixel directly. Instead, a color filter array (CFA), most commonly the Bayer pattern, places red, green, or blue filters over individual sensor pixels in an alternating mosaic (Figure 3). Each pixel measures only one color component, and the camera later uses demosaicing (e.g., nearest-neighbor, bilinear, bicubic, or more advanced methods) to reconstruct full RGB values for every image pixel. The Bayer pattern includes twice as many green filters as red or blue because luminance (perceived detail) is largely carried by the green channel, so oversampling green improves sharpness and noise performance (the choice also aligns with human visual sensitivity). The filters themselves pass only the corresponding wavelength ranges for R, G, and B (see Figure 2). Please refer to this reference for more details.</p> Figure 3: Comparison of monochrom and color camera mechanisms for capturing the image. <p>Monochrom: Unlike color cameras, there is no filter arrays needed for different pixels. In monochrom cameras, all the pixels are used for capturing some portions of the scene, thus the full potential of the physical pixels of the camera is used to capture the scene details, and no approximation through interpolation is usually needed. This is further depicted in Figure 3 by comparing these two sensing modalities.</p> <p>Another consequence of the color filters is that, they narrow down spectrum of the incoming rays of light, only passing a rather narrow band (related to the R, G, or B color). In Figure2, we have tried to show the range of wavelengths that are covered by R filter, G filter, B filter, and finally no filter (monochrome). These are approximation curves and they slightly differ for each real sensor. These curves are usually provided by the manufacturer. As is evident, a monochrome sensor (gray curve), covers a wider range of spectrums compared to the combination of R,G,B filters together. Specefically, it covers more on a portion of the infra-red band, also known as near-infrared. As a result, each pixel will receive less share of light rays (photons so to say); however, a monochrom camera due to lack of the filters can receive more light rays and is more illuminated compared to RGB cameras, improving performance at the darkness.</p> Figure 4: Comparison between images taken from different ranges of electromagnetic spectrum. Image taken from https://doi.org/10.48550/arXiv.2108.11767 <p>To make the discussion more sensible, Figure 4, brings a comparison between images taken from different electromagnetic bands. For instance, the RGB camera captures nice representation of the lit parts of the scene, also providing more semantics through the color information. However, the same scene captured using a monochrom camera that also captures some light in near-infrared (NIR) band can show more of the shadowed section of the scene, while providing a good geometrical understanding of the lit portion, yet missing the color details. The more the camera moves towards higher wavelengths (more towards middle-infrared and far-infrared) the more it turns into a thermal camera, seeing better at darkness and sensing the heat, yet the more specialized the sensors become that comes at a higher cost.</p>"},{"location":"sensors.html#213-camera-configurations","title":"2.1.3 Camera configurations","text":"<p>Monocular camera: It captures image data through a single lens. The benefit is that it is the most compact and efficient sensor setup in robotics, and SLAM systems. The drawback is that, this makes it more complicated to understand the geometry of the scene. </p> <p>Intuitively, it is not possible to understand the other dimensions of the real-world objects (also known as depth estimation), unless you observe them from at least two different points of view. This 3D understanding about the surrounding objects is central to the SLAM methodologies. Using only a single camera seems to prevent us from understanding the distances of the scene components from us; however, there are a few tricks that will enable depth estimation even using monocular cameras.</p> <p>The traditional approach is to move the camera around and use the observation of the same object in multiple frames at different times. Although the depth estimation will not be at hand at every frame, but after some time, assuming sufficient spatial translations, we should be to look at the same targets from different angles. This allows us to do simple geometrical analysis also known as Triangulation to measure objects' distances from the camera.</p> <p>There is a more recent approach to solve the monocular depth estimation problem, which is taking advantage of deep neural networks. These networks are trained using massive datasets, learning how to estimate the relative distances of object from camera only using a single image. This depth estimation based on experience is kind of similar to the way a human can infer depth if he covers one of his eyes, such that he still can understand distances but mostly because of experience. </p> <p>Stereo camera: If two monocular cameras are attached together, such that they share most of their view, it is a stereo camera setup. This is very similar to human way of observing the environment. In this setup, at every timestamp, the calculation of the scene depth is possible, since we already have two different views of the object. Simlar triangulation techniques, or more complicated methods are possible to be used here. The accuracy of the output depth estimation relies on many factors such as the algorithm. The maximum range of depth estimation depend on how separated the two cameras are from each other (also called the stereo baseline). </p> Figure 5: The comparison between monocular and stereo cameras in terms of 3D scene understanding. <p>In the Figure 5, examples of monocular and stereo cameras are highlighted. As evident, for a monocular camera, only by having dicplacements with respect to an object in the scene (the star shape represents an object), it is possible to create a virtual stereo camera by putting together the pairs of images from different times (given that they are far apart sufficiently). Then, a triangle can be assumed, however, all three side of the triangle are unknown, yet using two consecutive computer vision methods, namely the Five Points method, then Perspective n Points, a value for these sides will be at hand which is proportionally correct, however, does not have the right metric scale. The details for these algorithms is for interested readers. At the end, it should be emphasized that the monocular camera can not yield the right metric values for the depth of the features and only can deliver values for distances of objects that are correct relative to each other (correct proportions). If we want to convert them to correct metric values, another hint (another sensor such as IMU) should be incorporated.</p> <p>By comparison, the stereo camera in Figure 5, forms a similar triangle, however in one time instance and not across two different time instances. Moreover, the triangle that is formed for on object in the scene, has one known side (which is the baseline since we know how much apart are the two cameras from each other). As such, an easier triangulation with correct metric scale is possible without the need for additional hints.</p>"},{"location":"sensors.html#212-camera-based-slam-front-end","title":"2.1.2 Camera-based SLAM Front-end","text":"<p>Cameras provide the richest set of information among the available solutions for SLAM. The real challenge of using and dealing with camera is how to extract the necessary information out of them.</p> <p>When you choose cameras for your SLAM problem, you will often end up with a general architecture. We depict this general architecture in Figure 6. It does not mean that all SLAM algorithms are exactly following this architecture, but gives a general ideea of what happens in the camera-based SLAM.</p> Figure 6: Camera-based SLAM architecture. Taken from the <code>robor_test_3</code> sequence in SMARTNav dataset. <p>As emphasized previously, cameras spit out a lot of pixeled data. The first step is to Extract Features, ie. only keep the information that are necessary and useful. In Figure 6, you can see that some common ways of extracting features are depicted. For instance, extracting the edges in the picture, or extracting feature points (mostly the corner or sharp points in the environment that are easily detectable), or even lines are detected and extracted from image. These are popular feature extraction techniques because of two main reasons. I) It is relatively easy and quick to identify these features using classical computer vision II) It is possible to track these features accross different consecutive image frames. Among these intermediate representations, feature points are the most comonly used method.</p> <p>After feature extraction, it is important to maintain a unique identity for the features across different frames. As an example, if you see the corners of the ceiling of the house, in Figure 6, as a unique feature in the environment, you should be able to detect the same spot at a few moments later and you should be able to understand that this corner is the same that you saw before. If you successfully do this, you have performed a correct Feature Tracking. Figure 6, also shows an example depiction of feature tracking, where the features are tracked across two consecutive image frames. The location of these features in the first and second frame is drawn by the red dots and the green lines have established a connection between the latest and former location of the features.</p> <p>Although, the tracking of the features of environment is essential to the SLAM, providing a sense of relative motion with respect to the environment, it is not enough to realize a 3D measuremt of motion. The feature points should not be only tracked in the 2 dimensional space of the image, but also their 3D location in the space must be determined. So for each feature point, we can estimate a 3D location from the images, that is not trivial, but achievable through either monocular or stere-based camera configurations. In Figure 2, the uplifting of 2 dimensional features from image space into the 3D space is visualized such that a metric distance between the camera and the spatial position of that feature is at hand.</p>"},{"location":"sensors.html#214-sensor-requirements-what-to-buy-why","title":"2.1.4 Sensor requirements (what to buy &amp; why)","text":"<ul> <li>Global vs. Rolling shutter \u2014 reduce motion blur; support auto + manual control</li> <li>Dynamic range (DR) \u2014 avoid blown highlights/black crush in high-contrast scenes</li> <li>Frame rate (Rate) \u2014 \u226530 FPS typical; higher for fast robots or VIO</li> <li>Field of view (FOV) \u2014 wider = more features, better robustness to occlusion/fast motion</li> <li>Resolution \u2014 detail vs. compute/memory; common sweet spots (720p\u20131080p)</li> <li>Pixel size \u2014 low-light performance vs. resolution trade-off</li> <li>Optical size/format (e.g., 1/2.3\", 1/1.8\") \u2014 impacts lens choice and SNR</li> <li>Attachment type \u2014 C/CS-mount, M12; rigidity and repeatability of extrinsics</li> <li>Lens \u2014 focal length, distortion, aperture (f-number), focus control, IR cut</li> <li>Connection type \u2014 USB3, GMSL, MIPI, Ethernet; bandwidth, cable length, sync options</li> </ul>"},{"location":"sensors.html#215-failure-modes-mitigations","title":"2.1.5 Failure modes &amp; mitigations","text":"<ul> <li>Motion blur, low texture, repetitive patterns, lighting changes, rolling-shutter skew</li> <li>Wider FOV, higher FPS, proper exposure, feature thresholds, motion models</li> </ul>"},{"location":"sensors.html#216-available-solutions-examples","title":"2.1.6 Available solutions (examples)","text":"<ul> <li>Consumer RGB-D, industrial global-shutter stereo, machine-vision USB3/GigE cameras, event cams</li> <li>Notes on SDKs, ROS 2 drivers, and clock quality</li> </ul>"},{"location":"sensors.html#217-custom-camera-stack-checklist","title":"2.1.7 Custom camera stack checklist","text":"<ul> <li>Pick sensor + lens + mount; rigid bracket; trigger/sync wiring; EMI/shielding; heat &amp; dust</li> </ul>"},{"location":"sensors.html#218-advantages-and-disadvantages-of-visual-slam","title":"2.1.8 Advantages and disadvantages of visual SLAM","text":"<p>There are many advantages to the cameras, that make them a suitable choice for SLAM in robotics.  - They are rich. They see almost everything that a human doest. - They are usually lightweight, that makes them perfect for robots which weight is a determinig factor. - They are also energy efficient. - Inclusion of the cammera has the minimum design burden compared to LiDAR, RADAR, Compass, GNSS, and even the IMU. Meaning that they can be put flexibly on any part of the robot without much constraint on your design. - They can be used for other purposes. For instance you can put a camera for running your SLAM but also you can use the stream of image to perform another vision task, such as identifying a subject.</p> <p>The cameras also come with some disadvantages: - They are not able to directly understand the geometry of the scene. Understanding the geometry of the scene (how far things are from the camera) is necessary in SLAM and there are techniques to understand the distances but these techniques are not 100% reliable and have many limitations such as range and the textures in the environment. - The volume of data that is to be handled by camera is relatively high. - Due to the handling of the camera data, they often introduce delay to system, happening at the data transfer from sensor to the computer and copy pastings at Operating System (OS) level. - The SLAM algorithms that use camera are usually heavier than the ones using other modalities. This further processing often happens during feature extraction and 3D understanding of the environment - Normal RGB cameras dont see at darkness. </p>"},{"location":"sensors.html#22-lidar-for-slam","title":"2.2 LiDAR for SLAM","text":""},{"location":"sensors.html#221-lidar-types","title":"2.2.1 LiDAR types","text":"<ul> <li>2D vs. 3D; rotary/mechanical vs. solid-state; single-return vs. multi-echo</li> <li>Scan pattern, vertical FOV, channel count, min range, beam divergence</li> </ul>"},{"location":"sensors.html#222-requirements-parameters","title":"2.2.2 Requirements &amp; parameters","text":"<ul> <li>Range accuracy &amp; precision</li> <li>Angular resolution &amp; scan rate (deskew needs)</li> <li>Reflectivity handling (dark/bright surfaces)</li> <li>Timestamping &amp; packet timing (essential for motion compensation)</li> <li>Weather/lighting robustness (rain/fog/sun)</li> <li>Connection &amp; power (Ethernet, PPS, NTP/gPTP support)</li> </ul>"},{"location":"sensors.html#223-configuration-operation","title":"2.2.3 Configuration &amp; operation","text":"<ul> <li>Motion compensation (deskew), voxel/downsample filters, ground removal</li> <li>Intensity/reflectivity use, ring models, per-channel calibration</li> </ul>"},{"location":"sensors.html#224-available-solutions-drivers","title":"2.2.4 Available solutions &amp; drivers","text":"<ul> <li>Common 16/32/64-beam units, solid-state units; ROS 2 driver maturity &amp; logs</li> </ul>"},{"location":"sensors.html#225-custom-mounting-checklist","title":"2.2.5 Custom mounting checklist","text":"<ul> <li>Vibration isolation, occlusion cones, overlap with cameras, protective windows</li> </ul>"},{"location":"sensors.html#23-inertial-measurement-unit-imu","title":"2.3 Inertial Measurement Unit (IMU)","text":""},{"location":"sensors.html#231-imu-classes","title":"2.3.1 IMU classes","text":"<ul> <li>Consumer MEMS vs. industrial; bias stability, noise density, bandwidth</li> </ul>"},{"location":"sensors.html#232-requirements-parameters","title":"2.3.2 Requirements &amp; parameters","text":"<ul> <li>Bias stability &amp; Allan variance</li> <li>Sampling rate &amp; latency (\u2265200 Hz typical for VIO/LIO)</li> <li>Time base (PPS-disciplined vs. local clock)</li> <li>Mounting orientation &amp; rigidity</li> </ul>"},{"location":"sensors.html#233-configuration","title":"2.3.3 Configuration","text":"<ul> <li>IMU\u2013camera/LiDAR extrinsics; gravity alignment; filtering; saturation checks</li> </ul>"},{"location":"sensors.html#24-fusion-considerations-when-to-combine-sensors","title":"2.4 Fusion considerations (when to combine sensors)","text":"<ul> <li>Visual\u2013Inertial (robustness in low texture/blur)</li> <li>LiDAR\u2013Inertial (geometry + motion stability)</li> <li>Visual\u2013LiDAR\u2013Inertial (best-of-both, higher complexity)</li> </ul>"},{"location":"sensors.html#25-time-synchronization-timestamp-quality","title":"2.5 Time synchronization &amp; timestamp quality","text":"<ul> <li>Hardware trigger, PPS, gPTP/NTP; inter-sensor latency budgets</li> <li>Verifying sync in logs/bags; compensating residual offsets</li> </ul>"},{"location":"sensors.html#26-mounting-placement-cabling","title":"2.6 Mounting, placement &amp; cabling","text":"<ul> <li>Rigid baselines (stereo), overlapping FOVs (cam\u2013lidar), occlusion &amp; glare avoidance</li> <li>Vibration isolation, thermal management, IP rating, cable strain relief/EMI</li> </ul>"},{"location":"sensors.html#27-compute-bandwidth-storage-planning","title":"2.7 Compute, bandwidth &amp; storage planning","text":"<ul> <li>CPU/GPU needs per stack (VSLAM vs. LIO); RAM/VRAM sizing</li> <li>Link bandwidth (USB3/Ethernet), codec choice, on-robot logging rates &amp; retention</li> </ul>"},{"location":"sensors.html#28-procurement-acceptance-tests","title":"2.8 Procurement &amp; acceptance tests","text":"<ul> <li>Bench tests: exposure sweep, blur test (pan), DR test (backlit scene)</li> <li>Stereo/RGB-D: rectification check, depth-scale sanity, plane-fit RMSE</li> <li>LiDAR: range plane-fit, ring timing, deskew validation</li> <li>IMU: short Allan variance, saturation &amp; clipping checks</li> <li>Sync: cross-sensor timestamp residuals</li> </ul>"},{"location":"sensors.html#29-available-solutions-vs-custom-solution","title":"2.9 \u201cAvailable solutions\u201d vs. \u201cCustom solution\u201d","text":"<ul> <li>COTS kits (faster deployment, vendor support) vs. custom rigs (tailored FOV/baseline/sync)</li> <li>Total cost of ownership: sensors + mounts + time sync + compute + spares</li> </ul>"},{"location":"sensors.html#210-practical-considerations-pitfalls-field-notes","title":"2.10 Practical considerations &amp; pitfalls (field notes)","text":"<ul> <li>Sun/LED flicker; rolling-shutter on drones; lens focus drift; dirty optics</li> <li>LiDAR multipath/rain/fog; vibration-induced extrinsic drift; thermal drift</li> <li>Routine: lens cleaning, time-sync health, extrinsic re-verification schedule</li> </ul>"},{"location":"sensors.html#211-quick-decision-guide-by-scenario","title":"2.11 Quick decision guide (by scenario)","text":"<ul> <li>Indoor, texture-rich, low speed: stereo/RGB-D \u00b1 IMU</li> <li>Outdoor, structure-poor or high speed: 3D LiDAR + IMU; add camera for semantics</li> <li>Budget-constrained: mono + IMU (accept limits), or RGB-D at moderate speeds</li> </ul>"},{"location":"sensors.html#appendix-a-parameter-cheat-sheets","title":"Appendix A \u2014 Parameter cheat-sheets","text":"<ul> <li>Camera: shutter / DR / FPS / FOV / resolution / pixel size / lens / connection \u2192 symptom table</li> <li>LiDAR: channels / FOV / scan rate / voxel size / deskew \u2192 map quality table</li> <li>IMU: noise / bias / sample rate \u2192 drift &amp; observability table</li> </ul>"}]}