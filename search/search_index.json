{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"SMARTNav: A Visual-Inertial Dataset for Reliable Robotic State Estimation","text":"<p>On this page we present the SMARTNav dataset, containing raw sensor data recorded by aerial and ground robots. Collected across multiple projects targeting autonomous navigation in industrial settings, the dataset primarily supports evaluation of Visual-Inertial Odometry (VIO). More broadly, it is suitable for vision and LiDAR-based SLAM as well as multi-sensor fusion research.</p>"},{"location":"index.html#downloads","title":"Downloads","text":"<p>All sequences are provided as ROS 2 bag files. Because each sequence was captured with different hardware for different projects, the available topics vary; however, stereo images and IMU data are included in every sequence. Currently, all bag files are available exclusively via the Saxion Research Cloud Drive (SRCD).</p> <p>Note: For those sequences of this dataset containing RTK data, px4-msgs (it must be release/1.15 branch) is needed. This dependency will be removed in future versions of dataset. </p> Seq. Name Size[GB] Descriptions Link Snapshot greenhouse_flight_3 6.1 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P SRCD greenhouse_flight_5 3.1 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P SRCD greenhouse_ground_1 7.3 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU \u2003 GPS Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P \u2003 Ouster OS1-128 SRCD greenhouse_ground_2 7.7 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU \u2003 GPS Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P \u2003 Ouster OS1-128 SRCD greenhouse_handheld_1 2.4 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P SRCD optitrack_flight_1 0.5 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD optitrack_flight_2 1.7 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD optitrack_handheld_2 1.1 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD optitrack_handheld_3 1.1 Topics: \u2003 stereo images \u2003 IMU \u2003 motion capture Sensors: \u2003 ZED 2 camera \u2003 OptiTrack Prime 13 SRCD outdoor_flight_3 2.5 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD outdoor_flight_4 6.8 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD outdoor_flight_5 8.7 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD outdoor_flight_6 3.5 Topics: \u2003 stereo images \u2003 IMU \u2003 RTK Sensors: \u2003 ZED 2 camera \u2003 Holybro H-RTK F9P SRCD sidewalk_ground_1 35.9 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU \u2003 GPS Sensors: \u2003 ZEDX camera \u2003 Holybro H-RTK F9P \u2003 Ouster OS1-128 SRCD corridor_ground_1 25.8 Topics: \u2003 stereo images \u2003 camera IMU \u2003 LiDAR pointcloud \u2003 LiDAR IMU Sensors: \u2003 ZEDX camera  \u2003 Ouster OS1-128 SRCD"},{"location":"index.html#ground-truth-and-calibration","title":"Ground Truth and Calibration","text":"<p>In the table below, for each sequence, a bag file containing ground-truth topic and well as the sensor calibration data are provided. The GT topic type and calibration for each sequence may differ. Our evaluation tool can handle and compare these different topic types.</p> Seq. Name GT Source GT Link Calibration greenhouse_flight_3 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml greenhouse_flight_5 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml greenhouse_ground_1 LiDAR SLAM (GLIM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml greenhouse_ground_2 LiDAR SLAM (LeGO-LOAM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml greenhouse_handheld_1 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_flight_1 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_flight_2 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_handheld_2 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml optitrack_handheld_3 Motion Capture System download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_3 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_4 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_5 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml outdoor_flight_6 RTK download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yaml sidewalk_ground_1 LiDAR SLAM (LeGO-LOAM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml corridor_ground_1 LiDAR SLAM (LeGO-LOAM algorithm) download left_camera_intrinsic.yamlright_camera_intrinsic.yamlimu_leftcam_extrinsic.yamlimu_rightcam_extrinsic.yamlleftcam_rightcam_extrinsic.yamllidar_imu_extrinsic.yaml"},{"location":"index.html#acknowledgements","title":"Acknowledgements","text":"<p>This work was partly supported by the Netherlands Organization for Scientific Research (NWO) via SIA RAAK-Public project (Van bestrijden naar beheersen van de EPR, No.10.015) and SIA RAAK-MKB project (Smart Greenhouses, No.17.014).</p>"},{"location":"calibration.html","title":"Calibtation of Sensors","text":""},{"location":"calibration.html#objective","title":"Objective","text":"<ol> <li>Understand what sensor calibration is and why it\u2019s critical for localization and mapping.</li> <li>Understand What is intrinisic and extrnisic callibration.</li> <li>Perform intrinsic and extrinsic calibration using open-source tools.</li> </ol> <p>The fundamental objective of sensor calibration in SLAM systems is to ensure accurate spatial perception by establishing precise mathematical relationships between sensor measurements and the physical world. As SLAM systems increasingly rely on multi-sensor fusion architectures, proper calibration becomes essential for maintaining spatial consistency across different sensing modalities. The goal is to minimize systematic errors that can propagate through the SLAM pipeline, ultimately affecting localization accuracy and map quality.</p> <p>Current technological trends indicate a growing complexity in SLAM sensor configurations, with systems commonly integrating LiDAR, cameras, IMUs, wheel encoders, and other specialized sensors. Each sensor type introduces unique calibration challenges related to intrinsic parameters (internal characteristics) and extrinsic parameters (spatial relationships between sensors). The industry is moving toward more robust, automated calibration procedures that can adapt to environmental changes and sensor degradation over time.</p>"},{"location":"calibration.html#common-reference-frames-in-sensor-fusion-and-calibration","title":"Common Reference Frames in Sensor Fusion and Calibration","text":"<p>In localization, mapping, and multi-sensor fusion, consistent definition and transformation between reference frames is essential. Each frame represents a coordinate system attached to a specific entity (e.g., world, robot, sensor). Below are the most common frames used in SLAM and calibration systems.</p> <ol> <li> <p>World Frame: The World frame (or Map frame) is a global, fixed reference system used to express the absolute position and orientation of the robot and environment.The Z\u209a axis typically points upward (or opposite to gravity, depending on convention).The origin is arbitrarily set at the system initialization, often at the robot\u2019s or camera\u2019s starting pose.</p> </li> <li> <p>Odome Frame: is a local reference frame used to track the robot\u2019s motion over time relative to its starting position. It is typically updated incrementally by wheel encoders, visual odometry, or other motion estimation sensors.</p> </li> <li> <p>Body Frame: represents the robot\u2019s physical body, typically attached to the IMU or the vehicle chassis.   It acts as the central reference for all onboard sensors (LiDAR, cameras, GNSS, etc.).In a right-handed coordinate system, the x-axis points forward in the robot\u2019s motion direction, the y-axis points to the left, and the z-axis points upward.</p> </li> <li> <p>Camera Frame: The Camera Frame (C) is centered at the optical center of the camera and follows computer vision conventions. The z-axis points forward along the optical axis, x-axis points right, and y-axis points downward, aligned with image coordinates. This differs from the body or IMU frame (where z points upward). In multi-camera systems, each camera (C1,C2) has its own pose relative to the body frame, defined by the extrinsic transform.This frame is fundamental for projecting 3D world points onto the image plane.</p> </li> <li> <p>Image (Pixel) Frame: The Image Frame (I) is a 2D coordinate system on the camera\u2019s image plane, where each point corresponds to a pixel location. The origin (u=0,v=0) is typically at the top-left corner, with the u-axis pointing right and v-axis pointing down. 3D points from the camera frame are projected into this frame using the camera intrinsic matrix (K) and distortion model.</p> </li> </ol>"},{"location":"calibration.html#camera-callibration","title":"Camera Callibration","text":"<p>Intrinsic Callibration </p> <p>Intrinsic calibration estimates the camera\u2019s internal parameters, describing how 3D points in the camera frame are projected onto the 2D image plane.we use camera intrinsic parameter matrix, describes some of the physical properties of the camera lens. It is a 3x3 matrix with the following form:</p> <ul> <li> <p>fu and fv (fx and fy): These two parameters are the focal length of the camera, usually in pixels. fx for the x-direction and fy for the y-direction. Ideally, fx and fy should be equal, as most cameras have square pixels. However, in practical applications, they may differ due to reasons such as lens distortion and manufacturing errors.</p> </li> <li> <p>cu and cv (cx and cy): These two parameters are the coordinates of the image center (principal point). Ideally, the principal point should be at the exact center of the image. However, in practice, it may deviate due to various reasons (e.g., lens shift, manufacturing error, etc.).</p> </li> </ul> <p>Convert a point Pc=(Xc, Yc, Zc) in the camera coordinate system to a point Pi=(u,v) in the image/pixel coordinate system. The transformation process can be expressed as:</p> <p>Distortion Coefficients: Real lenses usually deviate from the ideal pinhole camera model, introducing significant distortion to images. As a result, camera parameters include Distortion Coefficients in addition to intrinsics and extrinsics. The main types of distortion are radial distortion and tangential distortion.</p> <p>Callibration Target- Camera calibration relies on predictably shaped and easy to localize calibration targets. Knowing the targets to identify in calibration images along with their spacing allows the optimization to reason about how the targets exist in space.Popular targets include checkerboard patterns, Aruco markers, Charuco markers, and circle grids, with potential modifications on those few.</p> <p>Checkerboard patterns are simple and easy to use.OpenCV has a chessboard calibration library that attempts to map points in 3D on a real-world chessboard to 2D camera coordinates.Any object could have been used (a book, a laptop computer, a car, etc.), but a chessboard has unique characteristics that make it well-suited for the job of correcting camera distortions:</p> <ul> <li> <p>It is flat, so you don\u2019t need to deal with the z-axis (z=0), only the x and y-axis. All the points on the chessboard lie on the same plane.</p> </li> <li> <p>There are clear corners and points, making it easy to map points in the 3D real world coordinate system to points on the camera\u2019s 2D pixel coordinate system.</p> </li> <li>The points and corners all occur on straight lines and with the same space.</li> </ul> <p>This tutorial uses a 8x6 checkerboard with 0.23cm squares. Calibration uses the interior vertex points of the checkerboard, so an \u201c9x7\u201d board uses the interior vertex parameter \u201c8x6\u201d as in the example below. </p> <p>There are a number of camera calibration tools available to do this calibration, but if you\u2019re already working in ROS, one of the easier options is the <code>camera_calibration</code> package.</p> <ol> <li> <p>To start first install docker and setup environment.This docker environment installs <code>camera_callibration</code> package with in <code>ros2-humble</code>.       <code>git --recurse-submodules clone git@github.com:eliyaskidnae/slam-tutorial-practical.git #Clone the repository with all submodules (only if you haven't cloned it yet)         cd slam-tutorial-practical/camera_callibration_ws/         docker compose up --build -d          docker compose exec callibration bash --login         source /opt/ros/humble/setup.bash         colcon build</code></p> </li> <li> <p>The next thing we need is data for the calibration to be run on. Normally, you would be able to use a live camera feed for the intrinsic calibration, but to make this training more universally accessible and repeatable, we will be working from bag files. Download <code>rosbag2_callibration1</code> file and put it <code>camera_callibration_ws/resources</code> folder.</p> <p>In the first one, run the first rosbag file on loop inside docker environment. <code>docker compose exec callibration bash --login   cd ~/callibration_ws/resources   ros2 bag play rosbag2_callibration1</code>   In the second terminal, run the camera calibration node</p> <p><code>docker compose exec callibration bash --login   source install/setup.bash   ros2 run camera_calibration cameracalibrator  --size 8x6 --square 0.023 --ros-args --remap image:=/zed/zed_node/left/color/raw/image  --remap camera:=zed/zed_node/left/color/raw</code> We use the above command to calibrate the ZED camera\u2019s left lens using the raw image topic <code>/zed/zed_node/left/color/raw/image</code>. The <code>--size 8x6</code> option specifies that the checkerboard used has 8 inner corners horizontally and 6 vertically, and <code>--square 0.023</code> sets the square size to 0.023 meters. The <code>--remap</code> arguments link the calibration node to the correct image and camera topics <code>namespace</code> recorded in the bag file.</p> </li> <li> <p>You should see a pop-up.In order to get a good calibration you will need to move the checkerboard around in the camera frame such that:checkerboard on the camera\u2019s left, right, top and bottom of field of view</p> </li> <li>X bar - left/right in field of view</li> <li>Y bar - top/bottom in field of view</li> <li>Size bar - toward/away and tilt from the camera</li> <li>checkerboard filling the whole field of view</li> <li>checkerboard tilted to the left, right, top and bottom (Skew)</li> </ol> Figure 1- Size bar-toward/away from the camera  Figure 2- X bar - left/right in field of view Figure 3- Y bar - top/bottom in field of view Figure 4- Skew bar - checkerboard tilted. <ol> <li> <p>When all the 4 bars are green and enough data is available for calibration the CALIBRATE button will light up. Click it to see the results. It takes around the minute for calibration to take place.After the calibration is completed the SAVE and commit buttons light up. And you can also see the result in terminal.</p> <p>A successful calibration typically has a mean reprojection error below 0.3 px.The closer to zero, the better.In our case, the GUI shows a 0.23 px error (displayed as \u201clin\u201d in Figure 5), indicating accurate calibration.</p> </li> <li> <p>To record these parameters down, click save.It will save to <code>/tmp/calibrationdata.tar</code>.gz.Let\u2019s get the files somewhere we can easily reach them</p> <p><code>mkdir ~/calibration_ws/mono_camera   mv /tmp/calibrationdata.tar.gz ~/calibration_ws/mono_camera   cd ~/calibration_ws/src/mono_camera   tar -xvf calibrationdata.tar.gz</code></p> </li> </ol> <p>This records all the original images used for the calibration, as well as the calibration parameters in two files: <code>ost.txt</code> and <code>ost.yaml</code>. Different applications expect intrinsics in a number of different ways, so you\u2019ll likely have to place particular parameters from these files in a certain structure.</p> <p>The camera calibration YAML file stores the camera\u2019s intrinsic parameters for image correction. The main parts are:</p> <ul> <li>Image size \u2013 width and height of the calibration images.  </li> <li>Camera matrix (K) \u2013 defines intrinsic parameters such as focal lengths and optical center.  </li> <li>Distortion coefficients \u2013 describe lens distortion for image undistortion.  </li> <li>Rectification matrix (R) \u2013 aligns images to a common plane; identity for mono cameras.  </li> <li>Projection matrix (P) \u2013 maps 3D camera coordinates to 2D image pixels and may differ from K after rectification.</li> </ul> <p>The calibration results should be applied to the raw image so subsequent image processing can use corrected images. In ROS2, this is done in two steps:</p> <ol> <li> <p>Copy the calibration YAML file to <code>camera_pipeline</code> package.This uses the callibration file to rectify the raw image as <code>image_rect</code> topic.  </p> <p>``` cp ~/calibration_ws/camera0_cal/calibrationdata/ost.yaml \\   ~/calibration_ws/src/camera_pipeline/config/</p> <p>ros2 launch camera_pipeline camera_info.launch.py image_raw:=/zed/zed_node/left/color/raw/image</p> <p>```</p> </li> <li> <p>Open a new terminal and launch RViz with two image displays: <code>/zed/zed_node/left/color/raw/image</code> and <code>/image_rect.</code></p> </li> </ol> <p>Play the rosbag and pause it with the spacebar. As illustrated in the images below, the left (raw) image appears distorted, with the vertical bar inside the red highlighted region noticeably curved. In contrast, the right (rectified) image shows the same bar as a straight line, demonstrating that the camera calibration and rectification process effectively corrected the lens distortion.</p> Figure 5 \u2013 raw image          Figure 6 \u2013 corrected(rectifed) image          <p>Extrinisic Callibration</p> <p>Extrinsic calibration determines the camera\u2019s pose relative to another reference frame, such as the world, body, or another sensor (e.g., camera or LiDAR). It defines how different sensors are spatially related and enables consistent projection of points between coordinate systems. In multi-camera or sensor-fusion setups, accurate extrinsic calibration ensures proper alignment between the camera, IMU, and LiDAR, which is essential for reliable stereo vision, visual-inertial odometry, and SLAM applications.</p> <p>For this experiment, we use the ZED stereo camera to perform extrinsic calibration.   The goal is to estimate the relative pose of the right camera with respect to the left camera, which defines the stereo baseline \u2014 the physical separation and orientation difference between the two lenses.</p> <p>The resulting extrinsic parameters (rotation R and translation T) describe how to transform points from the right camera\u2019s coordinate frame into the left camera\u2019s coordinate frame. These parameters are later used for stereo rectification and depth estimation.</p> <ol> <li> <p>First download the two provided ROS 2 bag files:</p> </li> <li> <p><code>stereo_large_board_bagfile.bag</code> \u2013 recorded using a small checkerboard (2.3 cm squares) </p> </li> <li> <p><code>stereo_small_board_bagfile.bag</code> \u2013 recorded using a large checkerboard (11.8 cm squares) </p> <p>Place both bag files in the <code>camera_callibration_ws/resources/</code> directory of the calibration workspace:</p> </li> <li> <p>Run the bag file and camera_calibration tool from the image_pipeline package to perform stereo calibration:</p> <p>In the first one, run the first rosbag file on loop. <code>docker compose exec callibration bash --login   cd resources/   ros2 bag play rosbag2_stereo_large_board</code></p> <p>In the second terminal, run the camera calibration node   <code>docker compose exec callibration bash --login   ros2 run camera_calibration cameracalibrator --approximate 0.1 --size 8x6 --square 0.118 --ros-args --remap left:=/zed/zed_node/left/color/raw/image --remap right:=/zed/zed_node/right/color/raw/image    --remap left_camera:=zed/zed_node/left/color/raw --remap right_camera:=zed/zed_node/right/color/raw</code>   3. You should see a pop-up.In order to get a good calibration you will need to move the checkerboard around in the camera frame.When all the 4 bars are green and enough data is available for calibration the CALIBRATE button will light up. Click it to see the results. It takes around the minute for calibration to take place.After the calibration is completed the SAVE and COMMIT buttons light up. And you can also see the result in terminal.</p> </li> <li> <p>To record these parameters down, click save.It will save to <code>/tmp/calibrationdata.tar</code>.gz.Let\u2019s get the files somewhere we can easily reach them</p> <p><code>mkdir ~/callibration_ws/resources/stereo_camera_large_board_cali/     mv /tmp/calibrationdata.tar.gz ~/callibration_ws/resources/stereo_camera_large_board_cali/     cd ~/calibration_ws/src/stereo_camera_large_board_cali     tar -xvf calibrationdata.tar.gz</code>   Inside the extracted folder, you will find:<code>left.yaml</code>(parameters of the left camera) and <code>right.yaml</code>(parameters of the right camera) The structure of these files is similar to those obtained from intrinsic calibration. However, to analyze the extrinsic calibration, we focus on the projection matrices (P matrices).</p> <p>Left Camera Projection Matricx </p> <p><code>data: [ 279.01721 ,   0.      ,  318.71177,   0.   ,                        0.     , 279.01721 ,  179.75161,   0.   ,                      0.     ,   0.      ,   1.      ,   0.    ]</code></p> <p>Right Camera Projection Matrix </p> <p><code>data: [ 279.01721,   0.       ,  318.71177, -33.6074 ,               0.     ,  279.01721 ,  179.75161,   0.     ,               0.     ,   0.       ,   1.      ,   0.     ]</code></p> <p>The fourth element in the first row of the right projection matrix (here <code>-33.6074</code>) represents the translation of the right camera along the x-axis relative to the left camera.   This value corresponds to the baseline distance between the two camera centers when scaled by the focal length:</p> <p>Mathematically:   $$     \\text{Baseline} = -\\frac{P_{14}}{f_x}   $$</p> <p>Using the given data:   $$   \\text{Baseline} = -\\frac{-33.6074}{279.01721} \\approx 0.1204 \\, \\text{m}   $$</p> <p>Thus, the two cameras are separated by approximately 12.04 cm, which matches the expected ZED stereo baseline.</p> <p>For parallel stereo cameras, the left and right cameras are almost perfectly aligned.This means the rotation matrix (R) between them is close to the identity matrix:which simplifies stereo processing.If the cameras were not parallel, a QR decomposition or SVD on a normalized version of the projection matrix can be used to to separate the rotation component</p> <p>Now consider the <code>stereo_small_board_bagfile.bag</code> with a small checkerboard (<code>--square 0.023</code> m):</p> <p><code>data: [284.05017 ,   0.       , 294.69019 , -34.89335,             0.     ,  284.05017 , 181.08418 ,   0.     ,             0.     ,   0.       ,   1.      ,   0.     ]</code></p> <p>$$   \\text{Baseline} = -\\frac{-34.89335}{284.05017} \\approx 0.123 \\, \\text{m}   $$</p> <p>Using a large checkerboard, the reprojection error is very low, less than 0.2 pixels, whereas using a small checkerboard results in a much higher error, greater than 1.2 pixels. The manufacturer-specified baseline for the ZED stereo camera is 12 cm, which is closely matched by the baseline obtained with the large checkerboard. In contrast, the high epipolar reprojection error observed with the small checkerboard indicates that this calibration is poor, and the resulting baseline does not align with the expected manufacturer value.</p> <p> Epipolar reprojection error with large checkerboard Epipolar reprojection error with small checkerboard </p> </li> </ol>"},{"location":"calibration.html#camera-imu-callibration","title":"Camera-IMU Callibration","text":"<p>The goal of camera-IMU extrinsic calibration is to accurately determine the transformation that defines the spatial relationship between the camera and the IMU.In this tutorial, we use Kalibr, a widely used tool for camera\u2013IMU calibration.</p> <p>The following are prerequest to use <code>kaibr</code> callibration tool:</p> <p>A. Prepare the calibration target: Kalibr supports multiple target types, but an AprilGrid is strongly recommended. It allows partial visibility of the board while still resolving the pose correctly, making data collection easier. Before starting, print an AprilGrid from the Kalibr wiki and fill out the corresponding aprilgrid.yaml(check kaliber yaml formats) file:    - Count the number of rows and columns, then fill in the values for tagsRows and tagsCols accordingly.    - Measure the size of one AprilTag and set it as tagSize (in meters).    - Measure the spacing (black border gap) between two tags.    - Compute tagSpacing = spacing / tagSize. For our tutoriall we will use a grid with 44 mm tags and 12.5 mm spacing.   <code>target_type: 'aprilgrid'  #gridtype     tagCols: 6                 #number of apriltags     tagRows: 6                 #number of apriltags     tagSize: 0.0445            #size of apriltag, edge to edge [m]     tagSpacing: 0.296          #ratio of space between tags to tagSize     codeOffset: 0            #code offset for the first tag in the aprilboard</code> B. Record the calibration dataset (rosbag): Record a rosbag containing IMU and camera data: To achieve accurate calibration, perform the following motions while keeping the target always in view:</p> <ul> <li>Pitch, yaw, and roll rotations</li> <li>Up/down, left/right, forward/backward translations</li> <li>A short sequence of smooth random motion</li> </ul> <p>Reference motion example: See this YouTube video as an example:</p> <p>C. IMU noise parameters: Kalibr requires IMU noise parameters such as noise density and random walk. These can come from the manufacturer\u2019s datasheet or tools, but it is recommended to compute them using an Allan variance calibration, since IMU noise characteristics can change depending on the physical setup, mounting, and environment. A convenient ROS-based Allan variance tool is available here allan_variance_ros</p> <p>For our tutoriall we will use a manufacturing callibration imu parametrs.   <code>#Accelerometers     accelerometer_noise_density: 1.4e-03   #Noise density (continuous-time)     accelerometer_random_walk:   8.0e-05   #Bias random walk     #Gyroscopes     gyroscope_noise_density:    8.712683324559951815e-5   #Noise density (continuous-time)     gyroscope_random_walk:      0.00074001958110154640244   #Bias random walk     rostopic:                    /zed/zed_node/imu/data_raw      #the IMU ROS topic     update_rate:                 100.0     #Hz (for discretization of the values above)</code></p> <p>Save this file as<code>imu-params.yaml</code>, which we will use as the input for the IMU calibration. After saving it, we can follow the steps below to run the calibration inside our Docker container. 1. To start first clone the docker container along with all its submodules (kaliber packages).This will build the Docker container with ROS 2 and all necessary dependencies for <code>Kaliber</code> package.       ```</p> <pre><code>    git --recurse-submodules clone git@github.com:eliyaskidnae/slam-tutorial-practical.git # Clone the repository with all submodules (only if you haven't cloned it yet)\n    cd slam-tutorial-practical/camera_imu_cal_ws/\n    docker compose up --build -d\n\n  ```\n</code></pre> <ol> <li> <p>Then opens a shell inside the Docker container, builds the Kalibr workspace and  sources the setup file.       <code>cd slam-tutorial-practical/camera_imu_cal_ws/         docker compose exec callibration bash --login         catkin build -DCMAKE_BUILD_TYPE=Release -j4         source devel/setup.bash</code>       check all packages are installed with out error.</p> </li> <li> <p>The next thing we need is data for the calibration to be run on. Normally, you would be able to use a live camera feed for the intrinsic calibration, but to make this training more universally accessible and repeatable, we will be working from bag files. Download <code>kaliber_ros1.bag</code> file and put it <code>camera_imu_cal_ws/resources</code> folder.Put also the configuration files <code>april-grid.yaml</code> and <code>imu_param.yaml</code> inside <code>/camera_imu_cal_ws/resources</code>.</p> <p>check for the bag file if it contains left and right camera topics as well as imu-raw topic.   <code>cd resources/   rosbag info kaliber_ros1.bag</code></p> </li> <li> <p>The kaliber imu-camera calibration requires the intrinisic and extinisic callibration of both cameras.We can use other camera callibration and put it the Kalibr YAML format documentation or perform a new calibration using Kalibr\u2019s camera calibration tool as foolowing command:</p> <p>run the kalibr camera calibration node   <code>rosrun kalibr kalibr_calibrate_cameras --bag resources/kalib_ros1.bag --topics /zed/zed_node/left/color/rect/image /zed/zed_node/right/color/rect/image --models pinhole-radtan pinhole-radtan --target resources/april-grid.yaml --show-extraction</code> When the calibration is complete (it takes many minutes according to the number of image acquired) you will get the file <code>kalib_ros1-camchain.yaml</code> and a full PDF report of the result of the calibration inside ~/callibration_ros1_ws/resources folder.</p> <p>The quality of the camera calibration can be verified by inspecting the reprojection error scatter plots. In these plots, each point represents the difference between the detected AprilGrid corner and its projected location based on the estimated camera model.Since the points are tightly clustered around zero and the error stays below about 0.5 pixels with a Gaussian-like distribution, this indicates a good and reliable calibration.</p> <p> Camera reprojection error </p> </li> <li> <p>Put the imu noise parametr as <code>imu-params.yaml</code>  in  <code>camera_imu_cal_ws/resources</code>.</p> <p>Then run the camera-imu callibration node.   <code>rosrun kalibr  kalibr_calibrate_imu_camera --bag resources/kaliber_ros1.bag --cam resources/kalib_ros1-camchain.yaml --imu resources/imu-params.yaml --target resources/april-grid.yaml</code>   After running kalibr_calibrate_imu_camera node, the camera calibration yaml will be extended by the imu-camera calibrator with imu-camera transformations.We can get also  a PDF report containing the final calibration result and calibration analyses.</p> </li> </ol> <pre><code>cam0:\n  T_cam_imu:\n  - [-0.0012069682380942137, -0.999959553566699, 0.00891260109951475, 0.02374101772612174]\n  - [0.0012353350482965375, -0.008914091742802915, -0.9999595056379624, 0.0019871949034301313]\n  - [0.9999985085863826, -0.001195909314175625, 0.0012460441090049457, -0.004969454993572966]\n  - [0.0, 0.0, 0.0, 1.0]\n  timeshift_cam_imu: 0.008473177395364007\ncam1:\n  T_cam_imu:\n  - [0.007688606532579134, -0.9999287036348504, 0.00913635467321633, -0.09582559937006864]\n  - [0.0012368931016336626, -0.009127107873462909, -0.9999575820990215, 0.0020721431219515286]\n  - [0.9999696772527866, 0.007699581092098007, 0.001166630174431943, -0.004792821181560303]\n  - [0.0, 0.0, 0.0, 1.0]\n  T_cn_cnm1:\n  - [0.999960408866058, -0.00021303049677720928, 0.00889580341679349, -0.1195210465345488]\n  - [0.00021302277813165302, 0.9999999773089986, 1.8151958025746858e-06, 7.989990659499038e-05]\n  - [-0.008895803601630881, 7.98848206438656e-08, 0.9999604315563049, 0.0003876324506602702]\n  - [0.0, 0.0, 0.0, 1.0]\n  timeshift_cam_imu: 0.008706209697421906\n</code></pre> <pre><code>  Lets get the 3x3 rounded roation matrix from transformation matrix.\n</code></pre> <pre><code>cam0_imu:[[0, -1,  0 ]\n          [0,  0, -1 ]\n          [1   0,  0 ]]   \n\ncam1_imu:[[0, -1,  0 ]\n          [0,  0, -1 ]\n          [1   0,  0 ]]\n\n</code></pre> <pre><code>  The rotation matrix tells us how the IMU is oriented relative to the camera. From this result, we can see that the IMU\u2019s X-axis is pointing in the same direction as the camera\u2019s forward Z-axis, meaning both sensors face the same way. The IMU\u2019s Y and Z axes are rotated so they line up with the camera\u2019s horizontal and vertical directions. In simple terms, the IMU is mounted in a way that its forward axis matches the camera\u2019s viewing direction, while the other axes are rotated to properly align the two coordinate frames\n\n  Checking the translation part of the transformation matrix.\n  ```\n   cam0_imu:[0.02374, 0, 0]   cam1_imu:[-0.0958, 0, 0 ] cam0_cam1:[-0.11952, 0, 0]\n  ```\n  The translation part of the transformation matrix describes how far the IMU is located from each camera. For cam0, the IMU is shifted by +0.02374 m along the X-axis, meaning the IMU sits about 2.3 cm to the right of the left camera. For cam1, the translation is \u22120.0958 m, meaning the IMU is about 9.6 cm to the left of the right camera. When we combine these two offsets, we get the total distance between the two cameras. This value is estimated as \u22120.11952 m, meaning the right camera is approximately 11.95 cm to the right of the left camera, which is the stereo baseline of zed-camera(12 cm)\n\n  The quality of the IMU\u2013camera calibration can be assessed by examining the reprojection error scatter plots. A good calibration is indicated when the reprojection errors lie within the 3-sigma bounds and are tightly clustered around zero. Although some outliers may appear, fewer outliers and a stronger concentration near zero generally reflect a more accurate calibration. In our results, the majority of the points remain close to zero, showing that the calibration quality is acceptable and consistent.For more explanation\n\n  &lt;table style=\"border-collapse: collapse; width: 100%; border: none;\"&gt;\n      &lt;tr&gt;\n        &lt;td style=\"padding: 0; vertical-align: top;\"&gt;\n          &lt;figure style=\"margin: 0;\"&gt;\n            &lt;img src=\"images/calib/reprojection-3-sigma.png\" alt=\"Alt 2\" height=\"400\" width=\"500\" style=\"margin: 1; padding: 2; display: block;\"/&gt;\n            &lt;figcaption style=\"font-size: 12px; margin-top: 4px; padding-left: 50px\"&gt;\n              &lt;strong&gt; reprojection error&lt;/strong&gt; \n            &lt;/figcaption&gt;\n          &lt;/figure&gt;\n        &lt;/td&gt;\n      &lt;/tr&gt;\n  &lt;/table&gt;\n\n\n  For more explanation about imu-camera calibration using kalibr package please refer this [video tutoriall](https://www.youtube.com/watch?v=BtzmsuJemgI)\n</code></pre>"},{"location":"course.html","title":"Practical Guide to SLAM","text":""},{"location":"deployment.html","title":"Chapter 4: Deployment of SLAM","text":""},{"location":"deployment.html#41-objectives","title":"4.1 Objectives","text":"<p>In this chapter, we will be running SLAM algorithms on some pre-recorded data. At the end, you should get some insights on:</p> <ul> <li>How does the real-time sensor data inputted to SLAM look like</li> <li>How run an open-source SLAM</li> <li>What are common parameters to tune when running SLAM</li> <li>How to interpret SLAM results</li> <li>How to mitigate SLAM failures </li> </ul>"},{"location":"deployment.html#42-deploying-a-camera-imu-slam","title":"4.2 Deploying a camera-IMU SLAM","text":"<p>In previous chapters, we discussed camera sensors suitable for SLAM. We also explored different modality and configuration of cameras. When it comes to visual SLAM, one of the most repeatedly used sensor configurations is combination of a stereo-camera and IMU sensor. Of course, there many other possible additions to this setup, such as adding wheel odometry, GPS, LiDAR, or more number of cameras. However, the most repeatedly supported sensor configuration is still stereo+IMU setup. That is why there are many commercially ready to use setup follow this configuration. </p> <p>Why is this sensor setup ideal? First, stereo camera pair are able to understand the scene depth up to a certain distance. Increasing the range of depth estimation depends on how far the two cameras are from each other (baseline). Adding the IMU sensor, will make the position estimation in the SLAM more smoth and more reliable. Researches in SLAM prove better accuracy of IMU-aided visual SLAM compared to camera only.</p> <p>Let us assume that we have acquired a stere+IMU sensor setup ideal for our application based on the instructions of chapter 2. Also, let us assume that we have calibrated our sensors according to the instructions in chapter 3. Now, we want to use this information to run a SLAM on our robot. If you dont have a robot already, no worries. We have prepared some data recorded on our real robots within SMARTNav dataset. The data is recorded in ROS2 bag format, meaning that by playing back the bag file, it is as if you are getting real-time stream of sensor data just like in a real robot.</p>"},{"location":"deployment.html#421-vins-fusion-algorithm","title":"4.2.1 VINS-Fusion algorithm","text":"<p>VINS-Fusion is widely used in research and sometomes adopted in industry because it is relatively mature, open-source, and has good performance in many real-world scenarios. It is designed to work with several sensor setups, but the most common configuration (and the one we use in this chapter) is a stereo camera + IMU. By fusing these two sources of information, VINS-Fusion can produce smoother and more robust trajectories than a camera-only SLAM system, especially during fast motions, rotations, or in low-texture regions.</p> <p>At a high level, VINS-Fusion has three main components:</p> <ul> <li> <p>Visual front-end: The visual front-end detects and tracks feature points in the images (for example, corners or small textured patches). These tracked features are used to reconstruct the relative motion of the camera between frames and to triangulate 3D landmarks in the environment. In the stereo case, depth can be obtained directly from the left\u2013right image pair, which improves the stability of the system and helps with scale estimation.</p> </li> <li> <p>Inertial (IMU) integration and optimization back-end: The IMU measurements are continuously integrated (often called IMU preintegration) to predict how the pose should evolve between image frames. This prediction is then combined with the visual measurements in a nonlinear optimization problem. The back-end maintains a sliding window of recent keyframes and IMU measurements and solves for the poses, velocities, IMU biases, and landmark positions that best explain all the data. This optimization-based approach allows VINS-Fusion to correct drift, handle sensor noise, and provide a consistent state estimate.</p> </li> <li> <p>Loop closure and map optimization (Fusion part): Over time, any odometry system will accumulate drift. VINS-Fusion includes a loop closure module that tries to recognize when the robot revisits a previously seen area. It uses image-based place recognition and geometric verification to detect these loop closures. When a loop is confirmed, a pose graph optimization step adjusts the whole trajectory to reduce accumulated error. This \u201cglobal\u201d correction can significantly improve the overall accuracy of the path and map.</p> </li> </ul> <p>From a user perspective, you can think of VINS-Fusion as a pipeline with tunable parameters rather than a black box. You can configure camera intrinsics, stereo baseline, IMU noise parameters, feature detection thresholds, and loop closure options. In this chapter, we will not go deep into the underlying mathematics, but we will see how these parameters affect the behavior of the system when we run it on real data. The goal is that by the end, you will know how to launch VINS-Fusion on a dataset (or real-time stream of data from your robot's sensors), how to recognize when it works well or fails, and which basic parameters you can adjust to improve robustness for your own application.</p>"},{"location":"deployment.html#422-running-a-demo","title":"4.2.2 Running a demo","text":"<p>To run the VINS-Fusion algorithm, we have tried to simplify the proecess of prerequisite installation, compilation, and fixing compatibility issues, often encountered when working with open-source software. To resolve this, please make sure you have docker and Visual Studio Code installed on your system. https://code.visualstudio.com/docs/devcontainers/containers</p> <p>Next, you should clone the github repository that we prepared for this course. You can use the following command in a terminal (if you already have it cloned in the previous chapter, you can skip this step):</p> <pre><code>git clone --recursive https://github.com/SaxionMechatronics/slam-tutorial-practical.git\n</code></pre> <p>Next, navigate to the <code>slam-tutorial-practical/slam_deployment</code> folder and open the VS Code. Inside the VS Code you can usually see a pop-up at the bottom-right corner, suggesting that the folder is a docker container and that you can set up the container. Choose the Reopen in Container option.</p> <p>Another option is to use the Ctrl+Shift+P kreyboard buttons, and look for and option for building and opening a container. The building and opening of the container might take a few minutes, depending on your processors, due to the compilation of all the prerequisites and the SLAM source code itself.  If all goes well, you will be notified by this message at the VS Code's terminal: Done. Press any key to close the terminal. </p> <p>Here you can open a fresh terminal from the VS Code menue: Terminal -&gt; New Terminal</p> <p>Now, make sure you have one the prerecorded data sequence that we have made available on our web-page. Download one of the sequences that their name starts with <code>optitrack</code>. In this example, we use <code>optitrack_handheld_3</code>. These sequences of data are recorded using a stereo camera plus and IMU sensor mounted on a drone that is either flying or being carried by hand at a small room:</p> <p>After you download the zipped file of the sequence, extract it in the <code>slam-tutorial-practical/slam_deployment</code> directory. We extracted it under a folder named <code>data</code>. In the newly opened terminal, use the following launch command to start the SLAM running on the prerecorded sequence:</p> <pre><code>ros2 launch vins vins_rviz.launch.py config:=VINS-Fusion-ROS2/config/zed2_gray/main_conf.yaml bag_folder:=data/optitrack_handheld_3/\n</code></pre> <p>In the above command, there are two parameters that you can change. First one is the <code>config</code> argument, which is the path to a file that all the configurations needed by VINS-Fusion are stored. These configurations contain descriptions of the you sensors and the calibration info needed by the SLAM algorithm to run properly. Additionally, some other algorithm-related parameters are specified in these files. If you want to run this SLAM for your custom setup, you need to prepare a custom config file, which we will talk more about it in the next sections.</p> <p>The other parameter passed in the above command (<code>bag_folder</code>) is the path to a ROS2 bag file, which already downloaded and placed in the workspace. If you wanted to use another sequence you can change this parameter. Note that the sensors used to record this data should be the same sensors that are described in the config file.</p> <p>At the end, you should be able to see the <code>RViz</code> visualization tool opened similar to the following video:</p>    Your browser does not support the video tag.  <p>Let's briefly discuss what you see in this video. At the bottom of the video, the images from the left and right cameras are displayed. On the images, you can see that some feature points are drawn in form of red and green points. The algorithm here, is tracking the spots in the envrionment that easy to identify and track. The red features, are the features that the algorithm has tracked over time. These are points that are tracked between consecutive frames of left camera. Tracking them is essential for estimating the camera motion over time. The green ones, are the features that are matching between the left and right frames, and they help estimating the distance of these points from the camera.</p> <p>After the features are successfully tracked, the algorithm tries estimating the 3D position of these features in the environment, and keeps refining their position. Their 3D positions are drawn with white points cubes in the 3D interactive viewport.</p> <p>The camera's position and orientation is visible by the 3 axes in the viewport. The red axis is the x direction of the camera (front), the green one is the y axis (left), and the blue axis is the the z (up) direction. As the robot is carried in the environment, the history of its positions (its path) is drawn as a curve green. </p> <p>What you saw in this demo, is basically known as the Visual-Inertial Odometry. Our algorithm is fusing the camera image information (through the features it is tracking) with the IMU sensor's data and is estimation the motion of the camera in the environment. If at some point of the estimation, something goes wrong (featureless environment or very noisy IMU), and we get few inaccurate motion estimations, that inaccuracy will bias our estimation position of the camera forever. In other words, the position estimation will deviate from reality, and the odometry method does not have a solution for that. To fix this, we will later discuss the loop closure mechanism that rectifies such drifts.</p>"},{"location":"deployment.html#423-preparing-a-custom-config-file-using-calibration-output","title":"4.2.3 Preparing a custom config file using calibration output","text":"<p>Each open-source SLAM usually uses a slightly different format. Since we introduced the Kalibr package for calibrating the camera-imu, we should be careful about how we should use the value outputted by the calibrator. Since we chose VINS-Fusion as a visual SLAM method in this chapter due to its common use and relative robustness, we should convert Kalibr outputs into VINS-Fusion format. This step is slightly different in different open-source SLAM algorithms. Sometime the algorithms can directly read the Kalibr output and sometime some re-formatting is needed. Generally it is a good idea for our knowledge if we transfer these values manually (as done in this chapter) so that we can also get a sense of how the calibration results go into a SLAM.</p> <p>Here we want to use a sample data recorded using a camera-imu installed on a drone that is flying in indoor environment. You can find the recorded file (ROS2 bag) in SMARTNav dataset, named <code>optitrack_hanfheld_3</code>. This configuration has a stereo camera (<code>cam0</code> and <code>cam1</code>) and an IMU. For the specific case of VINS-Fusion algorithm, for every robot setup we need 3 configuration files required.  These 3 files are:  - <code>cam0</code> intrinsic calibration info  - <code>cam1</code> intrinsic calibration info  - camera-imu and camera-camera extrinsic calibration info along with the algorithm's parameters</p> <p>For preparing these files, the easiest way to to use a previously created config file, and only change the entries according to your sensor setup calibration. In the dev container that we shared with you, there is a config folder for our camera called <code>zed2_gray</code>. You can create a similar folder there and rename it to your custom setup and start modifying the files inside it as we describe below.</p> <p>Let's start with preparing the camera intrinsic file. We will name this file <code>left.yaml</code>. This file has the following format:</p> <pre><code>%YAML:1.0\n---\nmodel_type: PINHOLE\ncamera_name: camera\nimage_width: ...\nimage_height: ...\ndistortion_parameters:\n   k1: ...\n   k2: ...\n   p1: ...\n   p2: ...\nprojection_parameters:\n   fx: ...\n   fy: ...\n   cx: ...\n   cy: ...\n ```\n\nAs can be seen, there are some parameters in the file that need to filled in according to the sensor's calibration. We can find these parameters in the output of Kalibr package (usually a file named `*-camchain.yaml`). In case of a stereo camera calibration, the file has a format like below, and the params needed to prepare `left.yaml` can be found in this file from `cam0` field.:\n ```\ncam0:\n    distortion_coeffs: [d1, d2, p1, p2]\n    intrinsics: [fx, fy, cx, cy]\n    resolution: [image_width, image_height]\n    ... some other paramters ...\n\ncam1:\n    distortion_coeffs: [d1, d2, p1, p2]\n    intrinsics: [fx, fy, cx, cy]\n    resolution: [image_width, image_height]\n    ... some other paramters ...\n ```\n\nSimilarly, you can create/modify a `right.yaml` and you can fill it up based on the `cam1` parameters.\n\nThe above format is assuming that in the calibration phase, you have used the default pinhole camera model and radial-tangential dostortion model. If you need to know more about these models and about each of these parameters are, you can revisit the calibration chapter. \n\nThere is a third file require by the VINS-Fusion which is the most important one. We create this file and will name it `main_conf.yaml`. Inside this file, there are many important paramters to be set. Let's start by filling those that related to sensors and we obtained them from Kalibr package.\n\nIn the main config file, first introduce the individual camera configuration file that you just created. The configurations for that are:\n```yaml\ncam0_calib: \"left.yaml\"\ncam1_calib: \"right.yaml\"\n</code></pre> <p>Then, enter the image resolution parameter. A very important note is that the camera resolution when you performed the sensor calibration should be the same resolution for images that you want to use in SLAM run time. If for instance you want to feed images of size 640x480 to the SLAM, make sure during the calibration the image resolution is the same and whenever the input reslution to SLAM changed, repeat the calibration with the new resolution.</p> <pre><code>image_width: 672\nimage_height: 376\n</code></pre> <p>Next, there is an extrinsics matrix required in this file with the following format:</p> <pre><code>body_T_cam0: !!opencv-matrix\n   rows: 4\n   cols: 4\n   dt: d\n   data:  [r1, r2, r3, t1,\n           r4, r5, r6, t2,\n           r7, r8, r9, t3,\n           0 , 0 , 0 , 1.0]\n</code></pre> <p>The matrix requried in <code>body_T_cam0</code> is a 4x4 matrix. This matrix contains the rotation and translation between the left camera and the IMU sensor (transforming camera to IMU). You can find these information in the outputs of Kalibr package, in a file usually named like <code>*-camchain-imucam.yaml</code>. This file hase this format:</p> <pre><code>cam0:\n  T_cam_imu:\n  - [r1, r2, r3, t1]\n  - [r4, r5, r6, t2]\n  - [r7, r8, r9, t3]\n  - [0 , 0 , 0 , 1.0]\n\n  ...\n\n  timeshift_cam_imu: td\n\n</code></pre> <p>The value from <code>T_cam_imu</code> should be transferred to the <code>body_T_cam0</code>. By convention, transformation matrices in SLAM algorithms are shown is similar formats, such that the <code>T_cam_imu</code> is read \"transformation from <code>imu</code> coordinate frame to the <code>cam</code> frame\". Similarly, the <code>body_T_cam0</code> is read as \"transoformation from <code>cam0</code> to <code>body</code> (body frame is sometimes referred to the imu frame)\".</p> <p>If you follow what the above transformations mean, you will notice that they have the inverse effect of each other. Thus, the output of Kalibr should be inversed and then placed in VINS-Fusion algorithm. We have provided the following interactive python script where you can input any 4x4 transformation matrix and get its inverted version. Use the inverted values inside the <code>main_conf.yaml</code> file that you are creating.</p> <p>The described procedure should be repeated for both cameras and you should create two entries in the <code>main_conf.yaml</code> file named <code>body_T_cam0</code> and <code>body_T_cam1</code>.</p> <p>After that, also import the estimated time offset between the camera and IMU, measured during calibration. This time offset is very important since it shows the delay in which the camera understands motion compared to IMU. In Kalibr output file (<code>*-camchain-imucam.yaml</code>), the parameter is named <code>timeshift_cam_imu</code>. Take the value and put it in our config file:</p> <pre><code>td: 0.008 \n</code></pre> <p>The last sensor-specific parameter to introduce to almost all SLAM algorithms, is the IMU noise characteristics. As discussed in chapter 2, all IMUs have imperfections in their data. The angular velocity and linear acceleration that they produce is either biased against the real value or has a random noise on it. The SLAM method should be able about the extent of the bias and noise. Since, it allows the SLAM to know how much can it rely on the IMU during the fusion. You can change these value in 2 ways.     - First, you can do IMU calibration. In this course, we did not cover this calibration in chapter 3, however, if you want to know the exact noise characteristic of the IMU, tools like Allan Variance calibrators might help you. This step is mostly advisable if you have a high-grade IMU sensor that you know is relatively reliable.    - Second, for most of the cheap IMU sensors usually used in SLAM systems, using the default values used by SLAM implementations, usually yields a good trade-off. You can slightly change these value by either increasing them or decreasing them if you want to tune them for your application.</p> <pre><code>acc_n: 0.1          \ngyr_n: 0.01         \nacc_w: 0.001         \ngyr_w: 0.0001       \ng_norm: 9.805        \n</code></pre> <p>So far, we have introduced all the parameters specific to our sensor setup. All sensor intrinsic and exterinsic calibrations and the time offsets. However, the VINS-Fusion algorithm is a capable SLAM that can also estimate and enhance the sensor extrinsics and time offsets in real-time. Unless you are very confident about your calibration, it is a good idea to allow the VINS to use your calibration info as an initial estimate and improve it on the go. For that, enable these two configs:</p> <pre><code># Set the values to 0 for disabling and to 1 for enabling \nestimate_extrinsic: 1   \nestimate_td: 1 \n</code></pre> <p>Lastly, we should make sure our sensor data stream (or data from replaying a bag file) is correctly introduced to the algorithm, so that when we run the SLAM, it starts estimating the state using the sensor data. For that, make sure these parameters are correct:</p> <pre><code>imu_topic: \"some imu topic name\"\nimage0_topic: \"left camera topic name\"\nimage1_topic: \"right camera topic name\"\n</code></pre>"},{"location":"deployment.html#424-vins-fusion-algorithm-parameters","title":"4.2.4 VINS-Fusion algorithm parameters","text":"<p>Other than the sensor-specific parameters, there are some other parameters that are used in VINS-Fusion and they affect the internal algorithm. Although these are specific to this algorithm, but knowing about them is useful since many other open-source algorithms share similar concepts/parameters.</p> <p>The VINS-Fusion supports 3 different sensor configurations:    - Stereo camera    - Monocular camera + IMU    - Stereo camera + IMU It can be controlled via the following parameters:</p> <pre><code>#support: 1 imu 1 cam; 1 imu 2 cam: 2 cam; \nimu: 1        \nnum_of_cam: 2  \n</code></pre> <p>Then, the front-end performance of the SLAM can be adjusted using the following parameters.</p> <pre><code>#feature traker paprameters\nmax_cnt: 150            # max feature number in feature tracking\nmin_dist: 15            # min distance between two features \nfreq: 10                # frequence (Hz) of publish tracking result. At least 10Hz for good estimation. If set 0, the frequence will be same as raw image \nF_threshold: 1.0        # ransac threshold (pixel)\nshow_track: 1           # publish tracking image as topic\nflow_back: 1            # perform forward and backward optical flow to improve feature tracking accuracy\n</code></pre> <pre><code>#optimization parameters\nmax_solver_time: 0.04  # max solver itration time (ms), to guarantee real time\nmax_num_iterations: 8   # max solver itrations, to guarantee real time\nkeyframe_parallax: 10.0 # keyframe selection threshold (pixel)\n</code></pre>"},{"location":"deployment.html#425-loop-closure","title":"4.2.5 Loop closure","text":"<p>As previously discussed, when you only run visual-inertial odometry, you will inevitably deal with the drift problem. This is why, visual-inertial odometry, alone can not be a very reliable source of navigation for your robot, and you will require some other accurate sensor that once a while, corrects the drifts of odometry. Examples of such sensors can be GPS for outdoor robots, or wireless beacons for indoor robots.</p> <p>The purely vision-based solution to this is the Loop Closure. It is only applicable if you have a robot that may revisit the same location multiple times. Loop closure is basically the ability to identify a previously seen area and based on that, correct the odometry estimation (to zero the accumulated error and drift). To put this into perspective, let's redo out demo, this time not only running the odometry, but also the loop closure enable. To this end, you can use the following launch command inside the container:</p> <pre><code>ros2 launch vins vins_lc_rviz.launch.py config:=VINS-Fusion-ROS2/config/zed2_gray/main_conf.yaml bag_folder:=data/optitrack_handheld_3/\n</code></pre> <p>You should be able to see the <code>RViz</code> openned and the result should look something like the following video:</p>    Your browser does not support the video tag.  <p>The difference is that now, there are two axes displayed in 3D viewport. One is thinner and that is the same odometry estimated position and another is a thicker one which is the corrected position of the odometry by loop closure. There is also a new path visualized, which is the corrected path after each loop detection. As you can see, at the beginning, the two position estimations match each other almost perfectly, but as time passes, the green trajectory is drifting from the rectangular path that camera is going on, while the blue curve, corrects itself every while and remains closer to rectangular path that it is supposed to be.</p> <p>Although the above observation intuitivly shows how the loop closure improves the quality of your localization, it is not a definitive and numeric measure of accuracy. We can go over such analysis in the next sections.</p>"},{"location":"deployment.html#45-deploying-a-lidar-imu-slam","title":"4.5 Deploying a LiDAR-IMU SLAM","text":""},{"location":"deployment.html#451-running-fastlio","title":"4.5.1 Running FASTLIO","text":""},{"location":"deployment.html#452-running-glim-or-lego-loam","title":"4.5.2 Running GLIM or Lego-LOAM","text":""},{"location":"deployment.html#453-parameters","title":"4.5.3 Parameters","text":""},{"location":"deployment.html#46-practical-consideration","title":"4.6 Practical consideration","text":""},{"location":"deployment.html#461-processing-unit","title":"4.6.1 Processing unit","text":""},{"location":"deployment.html#462-standalone-or-ros-based","title":"4.6.2 Standalone or ROS-based","text":""},{"location":"deployment.html#463-qos-in-ros","title":"4.6.3 QoS in ROS","text":""},{"location":"deployment.html#464-being-real-time-and-low-delay","title":"4.6.4 Being real-time and low delay","text":""},{"location":"deployment.html#465-using-in-control-loops","title":"4.6.5 Using in control loops","text":""},{"location":"interpretation.html","title":"Interpretation and Evaluation of SLAM Outputs","text":""},{"location":"introduction.html","title":"Chapter 1: Introduction to SLAM","text":""},{"location":"introduction.html#11-objective","title":"1.1 Objective","text":"<p>Figure 1 illustrates the topics that will be discussed in this chapter. After completing this chapter, participants will be able to:</p> <ul> <li> <p>Understand the concept and purpose of SLAM in autonomous systems.</p> </li> <li> <p>Explain why SLAM is important for robots.</p> </li> <li> <p>Identify key challenges and limitations of SLAM in real-world applications.</p> </li> <li> <p>Recognize the role of data preparation in achieving successful SLAM results.</p> </li> </ul> Figure 1: Illustration on the topics to be highlighted in this chapter."},{"location":"introduction.html#12-introduction-the-need-for-slam","title":"1.2 Introduction: The Need for SLAM","text":"<p>Autonomous navigation has become one of the central capabilities of modern robotics. Whether it is a drone flying indoors, a warehouse robot delivering packages, or an autonomous robots driving through city streets, they faces the same two fundamental questions:</p> <p>Where am I? (localization)</p> <p>What does my environment look like? (mapping)</p> <p>The process of answering these two questions at the same time is called Simultaneous Localization and Mapping (SLAM). SLAM allows a robot to build a map of an unknown environment while simultaneously determining its position within the so-far built map as simply demonstrated in Figure 2. In human terms, imagine entering a dark, unfamiliar room with no GPS or prior map. You start walking slowly, feeling the walls, and mentally building a map while estimating your position relative to it. This is essentially what a robot must do in a new environment (called Navigation).</p> <p>SLAM enables a robot to construct a map of its surroundings while simultaneously tracking its own motion within that map based on data from onboard sensors such as cameras, LiDAR, and inertial measurement units (IMUs).</p> Figure 2. SLAM is very necessary to enable robots to conduct navigation and collect data."},{"location":"introduction.html#13-the-concept-of-slam","title":"1.3 The Concept of SLAM","text":"<p>SLAM is fundamentally a state estimation problem. It involves estimating the robot\u2019s pose (position and orientation) and a model of the surrounding environment from sequential sensor data/frames. Thus, the SLAM process involves two tasks as shown in Figure 3:</p> <ul> <li> <p>Localization: Determining the robot\u2019s position and orientation within the environment.</p> </li> <li> <p>Mapping: Creating a representation (map) of the environment based on sensor data.</p> </li> </ul> <p>Both tasks depend on each other and the robot cannot accurately localize itself without a map, and it cannot build a correct map without knowing its position. SLAM solves this problem by using probabilistic estimation and continuous updates as the robot moves.</p> <p>Inputs:</p> <ul> <li>Sensor measurements (images, LiDAR scans, IMU data)</li> <li>Control or motion inputs (wheel odometry, velocity commands)</li> </ul> <p>And outputs:</p> <ul> <li>The trajectory of the robot (motion over time).</li> <li>The map of the environment.</li> </ul> <p>Although many SLAM algorithms exist (filter-based, optimization-based, graph-based, etc.), this course focuses on the conceptual understanding and engineers will treat SLAM as a functional system  (a black box that takes in properly prepared data and produces a map and trajectory as outputs).</p> Figure 3. SLAM techniques take robots' sensory data as inputs and produce maps and trajectories"},{"location":"introduction.html#14-slam-pipeline","title":"1.4 SLAM Pipeline","text":"<p>The SLAM system is generally divided into two main processes that work together:</p> <p>The Front-End (Real-Time)</p> <p>It is responsible for data ingestion, feature extraction, and local motion estimation to estimate the robot's pose (position and orientation) and construct the initial local map structure. This process needs to be fast and accurate.</p> <p>The Back-End (Optimization)</p> <p>It is responsible for error correction, data association, and global consistency of the entire map and trajectory. It relies heavily on detecting when the robot revisits a place (Loop Closure). It should be less fast than front-end process but more accurate to correct all position errors with respect to the realworld. Different optimization methods could be used based on the sensor data and the tyoe of maps needed for the industrial application, e.g., 2D or 3D, dense or sparse, accurate or feasible and so on.</p> Figure 4. SLAM main and general pipline."},{"location":"introduction.html#15-slam-importance","title":"1.5 SLAM Importance","text":"<p>SLAM is the foundation of autonomous perception and navigation. Without it, robots cannot move intelligently in unknown areas. It provides the spatial awareness required for tasks like path planning, obstacle avoidance, and environment understanding.</p> <p>Practical applications include:</p> <ul> <li> <p>Autonomous vehicles: Navigating complex roads and cities.</p> </li> <li> <p>Aerial drones: Flying indoors or in GPS-denied areas.</p> </li> <li> <p>Service robots: Operating in factories, warehouses, greenhouses, forests, seas, underwater and so on.</p> </li> <li> <p>Exploration robots: Planetary rovers, underwater vehicles, and subterranean robots or planetary surfaces.</p> </li> </ul> Figure 5. SLAM can be integrated into different robotics platforms for various purposes."},{"location":"introduction.html#16-challenges-in-slam","title":"1.6 Challenges in SLAM","text":"<p>Although SLAM may seem a straightforward conceptually, real-world conditions make it challenging such as introducing uncertainty, noise, and dynamic changes that complicate both the trajectory estimation. Engineers and researchers should deal with uncertainty, noise, and incomplete data to produce reliable, robust and scalable applications. </p> <p>Common challenges include:</p> <ul> <li>Sensor Limitations</li> </ul> <p>Cameras struggle in low light or with motion blur.</p> <p>LiDARs fail on reflective or transparent surfaces.</p> <p>IMUs drift over time if not corrected.</p> <ul> <li>Environmental Complexity</li> </ul> <p>Dynamic objects (people, cars, doors, robots) create false data associations.</p> <p>Repetitive or featureless environments (white walls, glass) make feature tracking unreliable.</p> <p>Changing lighting or weather affects visual perception.</p> <ul> <li>Data Association Errors</li> </ul> <p>Recognizing the same place from different viewpoints is difficult.</p> <p>Wrong associations cause mapping errors or loop closure failures.</p> Figure 6. Real and practical challenges of applying SLAM in industrial settings."},{"location":"introduction.html#17-limitations-in-slam","title":"1.7 Limitations in SLAM","text":"<p>Even with advanced algorithms, SLAM has inherent limitations that engineers must understand before deploying it in real systems.</p> <ul> <li> <p>Dependence on Sensor Quality: The accuracy of SLAM depends entirely on the data it receives. Low-cost sensors can introduce noise and distortions that degrade performance.</p> </li> <li> <p>Accumulated Drift: Over long trajectories, small errors in motion estimation accumulate, leading to inconsistencies unless corrected by other resources such as GPS fusion.</p> </li> <li> <p>Computation and Memory: Large-scale mapping requires significant computational resources, which may not be available on small mobile robots.</p> </li> <li> <p>Calibration Sensitivity: Calibration ensures to produce geometric consistency by sensors such as camera or lidar. Moreover, misalignment between sensors (e.g., between a camera and an IMU) can severely affect results.</p> </li> </ul> <p>Understanding these limitations helps engineers to design systems that work with SLAM rather than expecting it to solve all navigation problems perfectly.</p>"},{"location":"introduction.html#18-the-role-of-data-preparation","title":"1.8 The Role of Data Preparation","text":"<p>The most underestimated aspect of SLAM is data preparation. The success of any SLAM system depends critically on the quality of the input data. A well-prepared dataset makes SLAM robust, while poor data can make even the best algorithm fail.</p> <p>In this course, data preparation refers to three main steps: sensor selection, calibration and data collection strategy.</p> <ul> <li>Sensor Selection</li> <li> <p>Choose sensors based on the operating environment.</p> </li> <li> <p>Cameras are small and affordable but require good lighting.</p> </li> <li> <p>LiDARs provide accurate 3D structure but are heavier and costly.</p> </li> <li> <p>IMUs provide motion cues but drift quickly. A combination (e.g., visual-inertial or LiDAR-inertial) is often used.</p> </li> <li> <p>Calibration</p> </li> <li> <p>Intrinsic calibration: Corrects for lens distortion and optical errors.</p> </li> <li> <p>Extrinsic calibration: Defines spatial relationships between multiple sensors (e.g., between a camera and IMU).</p> </li> <li> <p>Deployment and Data Collection Strategy</p> </li> <li> <p>Understand the operational environments and relevant challenges of robot navigation.</p> </li> <li> <p>Select the most appropriate SLAM method that aligns the determined challenges.</p> </li> <li> <p>Move the robot smoothly, avoid abrupt rotations, and capture diverse views.</p> </li> <li> <p>Plan trajectories that include revisiting some areas.</p> </li> <li> <p>Clear interpretation of the collected data and the outputs. </p> </li> </ul> Figure 7. The data preparation is the keypoint to ensure high quality and robust results by SLAM techniques."},{"location":"introduction.html#19-summary","title":"1.9 Summary","text":"<p>SLAM represents a cornerstone of autonomous navigation and perception. This chapter introduced the brief concept of SLAM, discussed its importance, examined common challenges and limitations, and emphasized the critical role of data preparation. Successful SLAM operation relies not only on algorithmic sophistication but also on well-prepared and accurately calibrated sensory data.</p> <p>The subsequent chapters will expand on practical aspects of data acquisition, calibration techniques, and data preperation for ensuring high-quality SLAM performance.</p>"},{"location":"sensors.html","title":"Sensor Selection for SLAM","text":""},{"location":"sensors.html#chapter-2-sensor-selection-for-slam","title":"Chapter 2: Sensor Selection for SLAM","text":""},{"location":"sensors.html#objectives","title":"Objectives","text":"<p>In this chapter, you will meet the most common sensor types used in SLAM and see where each one shines or struggles. By the end, you are expected to:</p> <ul> <li>What the common sensors are and what information do they offer.</li> <li>Pick a sensible sensor (or a combination) for your own application.</li> <li>Weigh the usual trade-offs: accuracy vs. cost, compute load, environment, and how mature the open-source algorithms for your application are.</li> <li>Spot the key specifications that matter when buying or configuring sensors for SLAM.</li> </ul>"},{"location":"sensors.html#role-of-sensors-in-slam-architecture","title":"Role of Sensors in SLAM Architecture","text":"<p>SLAM is usually split into two parts: a front-end and a back-end.</p> <p>Front-end: It is the part of SLAM that turns raw sensor measurements into compact, useful cues about motion and the scene. If we use a camera for a SLAM algorithm, we will have visual SLAM. Assuming that the camera gives us images of $640\\times480$, we will end up with 307200 pixels. This is an enormous amount of information that is very difficult to comprehend and use. The front-end reduces this load by extracting just the most informative bits, so the rest of the system has something manageable to work with.</p> <p>Back-end: It is the core algorithm, that estimates the robot\u2019s pose and builds the map from those cues.</p> <p>Your sensor choice largely shapes the front-end: what you can extract, how robust it is, and how much computation it needs. In short, the sensor defines the front-end\u2019s job, and through that, it influences the back-end\u2019s workload and accuracy.</p>"},{"location":"sensors.html#21-cameras-for-slam","title":"2.1 Cameras for SLAM","text":""},{"location":"sensors.html#211-image","title":"2.1.1 Image","text":"<p>If you\u2019re new to computer vision, here\u2019s the core idea: an image is a matrix of numbers.  Each cell is a pixel, a tiny dot with a color  that defines how a small part of the scene looks like. It should be clear that by increasing the number of pixels, more detail we be captured about the scene.</p>"},{"location":"sensors.html#212-camera-based-slam-front-end","title":"2.1.2 Camera-based SLAM Front-end","text":"<p>Cameras provide the richest information among the common sensors used for SLAM. The real challenge in using cameras is not getting the data, but extracting the useful information from all those pixels.</p> <p>When you choose cameras for your SLAM problem, you will often end up with a similar front-end architecture. Having a basic understanding of this architecture is very useful, since it gives you an idea of why specific sensors are better and more suitable to SLAM while others are not. A general example is shown in Figure 1. This does not mean that all SLAM algorithms follow this architecture exactly, but it gives a good overall idea of what typically happens in camera-based SLAM.</p> Figure 1: Camera-based SLAM architecture. Taken from the <code>robor_test_3</code> sequence in SMARTNav dataset. <p>As emphasized previously, cameras produce a lot of pixel data. The first step is to Extract Features, i.e., to keep only the parts of the image that are necessary and useful. In Figure 1, you can see some common ways of extracting features. For instance, we can detect:</p> <ul> <li>Edges in the image,</li> <li>Feature points (often corners or sharp points in the environment that are easy to detect),</li> <li>Or lines in the scene.</li> </ul> <p>These are popular feature types for two main reasons: (i) It is relatively easy and fast to detect them using classical computer vision techniques. (ii) It is possible to track these features across different consecutive image frames. Among these intermediate representations, feature points are the most commonly used in SLAM.</p> <p>After feature extraction, it is important to maintain a unique identity for each feature across frames. For example, if you see the corner of the house's rooftop (as in Figure 1) as a unique feature in the environment, you should be able to detect the same corner a few moments later and recognize that it is the same feature you saw before. If you successfully do this, you have performed correct Feature Tracking. Figure 1 also shows a simple depiction of feature tracking, where features are tracked across two consecutive image frames. The positions of the features in the first and second frames are marked by red dots, and the green lines connect the former and current locations of the same features.</p> <p>The robustness of SLAM methods is tightly related to the success of the feature extraction and tracking. Our choice of camera sensors can affect this. Generally, having an image with minmum blurring and sharper details that maintains a constant color composure of the details, regardless of light intensity, helps a lot. Moreover, the field of view should be large enough so that you do not lose the features you were teacking with a rapid motion. But we will touch on these requirement later on.</p> <p>Although tracking the features in the environment is essential for SLAM and provides a sense of relative motion with respect to the scene, it is not enough to measure motion in full 3D. The feature points should not only be tracked in the 2D image; we also need to estimate their 3D locations in space.</p> <p>For each feature point, we would like to estimate a 3D position based on the images. This is not trivial, but it is achievable using either monocular or stereo-based camera configurations. In Figure 1, this \u201cuplifting\u201d of 2D features from the image plane into 3D space is visualized: from this, a metric distance between the camera and the 3D position of that feature becomes available. Note that this step is highly dependent on correct knowledge about the camera's physical paramters such as its Focal Length. These estimates usaully come from precise calibration of the sensor that we will discuss in later chaters.</p>"},{"location":"sensors.html#213-camera-modalities","title":"2.1.3 Camera modalities","text":"<p>The term modality is referring to what kind of light does a camera measure and how it measures it. Most of the cameras capture the visible light, which is a small part of electromagnetic spectrum. In Figure 2, we display this spectrum and the visible light is highlited by rainbow colors. Here, the color, monochrome, NIR, and thermal cameras are more interesting for us, since they are often used for SLAM.</p> Figure 2: Where common camera modalities sit on the electromagnetic spectrum. <p>Color cameras: These cameras capture a scene in a similar way to how we see it, in different shades of colors. For each pixel of an image, there are 3 values for red, green, blue (RGB) that their combination will create a uniqe color. But the camera sensor does not read 3 values (RGB) at each pixel of the physical sensor directly.</p> <p>To capture color images, a Color Filter Array (CFA) is formed, such that on each pixel either an R, G, or B filter is used. An R filter for instance, only allows the red light frequency to pass. For each pixel, the other two values will be interpolated from the neighboring pixels. This process is also known as Demosaicing. Figure 3 diplays the mechanism in which the color images are formed in the sensor. </p> <p>Why are the working principle of color camera important for us? To answer that, lets understand the monochrome mechanism as well.</p> <p>Monochrome: Unlike color cameras, monochrome cameras do not use a color filter array. All pixels on the sensor are used directly to capture light from the scene. This means that the full resolution of the sensor is used for details, no color interpolation (demosaicing) is needed, and the image is in shades of gray instead of color. The monochrome sensor is also portrayed along with a color sensor in Figure 3, for better comparison.</p> <p>Now going back to the relevance of the monochrome vs. color mechanism for someone who wants to use one of these cameras for their robot's navigation:</p> <p>Monochrome cameras usually give sharper detail and cleaner measurements from the same number of pixels. Another difference is that each filter of a color camera only passes a narrow band of wavelengths (related to R, G, or B). In Figure 2, we show approximate curves for the wavelength ranges covered by the R, G, and B filters (red, green, blue bell curves), and by a monochrome sensor with no filter (the gray bell curve). As you can see, a monochrome sensor covers a wider range of the spectrum compared to the combination of R, G, and B filters. In particular, it often captures more of the near-infrared range. Because of this, in a color camera, each pixel receives less light (fewer photons) because of the filters, while in a monochrome camera, more light reaches each pixel, so the image is brighter and looks better under low-light conditions.</p> <p>In summary, monochrome sensors give more details and perform better in darkness. Does this mean they are ideal for SLAM? It depends. If you prioritize more reliable feature tracking and pose estimation even in low-light, monochrome is the choice, while if you prefer more informative maps, colors might still be needed. Keep in mind that the modality is not the only determinig factor for choosing a sensor and we introduce more criteria later.</p> Figure 3: Monochrome vs. color sensing. Color sensors use a mosaic of R/G/B filters while monochrome does not. <p>To make this discussion more concrete, Figure 4 compares images of the same scene taken with cameras sensitive to different parts of the electromagnetic spectrum.</p> <p>An RGB color camera captures a nice, human-like view of the lit areas in the scene. Because it sees color, it also gives us more semantic information; for example, it can help distinguish car's body from building walls based on their colors.</p> <p>A monochrome camera that is also sensitive to some near-infrared (NIR) light can show more detail in shadowed areas. Objects that look very dark in visible light might still reflect NIR light, so they become more visible. The image is still grayscale, but it can provide a strong geometric understanding of the scene (edges, shapes, textures), even when parts of it are poorly lit.</p> <p>As we move to cameras that work at longer wavelengths (towards mid-infrared and far-infrared), the camera behaves more like a thermal camera. These cameras can see heat. They work much better in complete darkness and can highlight warm objects like humans, engines, or electronics. However, the sensors and lenses for these wavelengths are more specialized and usually more expensive, and the images often contain less fine detail than visible or NIR images.</p> Figure 4: Comparison between images taken from different ranges of electromagnetic spectrum. Image taken from https://doi.org/10.48550/arXiv.2108.11767."},{"location":"sensors.html#214-camera-configurations","title":"2.1.4 Camera configurations","text":"<p>Monocular camera: A monocular camera is simply a camera with a single lens. It captures one image at a time, just like the camera on your phone. From a robotics and SLAM point of view, this setup has some clear benefits. It is compact, cheap, consumes minimal power, and makes the design easy. The main drawback is that it makes 3D understanding of the scene more difficult.</p> <p>Intuitively, if you only see the world from a single point of view, it is hard to tell how far away things are. This \u201chow far?\u201d question is called Depth Estimation. To reliably estimate depth using pure geometry, you normally need to see the same object from at least two different viewpoints. This 3D understanding of the surroundings is central to most SLAM systems.</p> <p>So at first glance, using only one camera seems to prevent us from knowing the distance to objects in the scene. Fortunately, there are a couple of tricks that help.</p> <p>First, there is a classical computer vision approach to this problem. Even with a single camera, we can still move it around. If the camera observes the same object in multiple frames taken at different positions, we effectively create multiple viewpoints over time. With enough motion (especially sideways motion, not just rotation), we can use basic geometric reasoning, known as Triangulation, to compute how far objects are from the camera. The downside is that depth is not available immediately in each frame. The SLAM system needs some time and movement before it can build a good 3D understanding of the scene.</p> <p>A more recent approach uses deep neural networks to estimate depth from a single image. These networks are trained on large datasets where the correct depth is known. Over time, they learn patterns like:</p> <ul> <li>\u201cThings higher in the image might be farther away.\u201d</li> <li>\u201cObjects that look smaller might be more distant.\u201d</li> <li>\u201cCertain textures and shading suggest particular shapes and distances.\u201d</li> </ul> <p>This is somewhat similar to how humans can still judge distance even if they close one eye. We lose the geometric stereo cues, but we still have experience-based cues: relative size, perspective lines, shadows, and so on.</p> <p>Stereo camera: A stereo camera setup consists of two monocular cameras mounted next to each other, pointing in (almost) the same direction and sharing most of their field of view. This is very similar to how our two eyes see the world.</p> <p>Because we have two views at the same time, we can estimate depth in every frame. For each point in the scene, we find the matching pixel in the left and right images. From the difference in their positions (called disparity) and the known distance between the cameras (knwon as baseline), we can compute how far that point is from the cameras using triangulation. More separation between the cameras makes depth estimation more accurate at longer distances, but it can also make the system bulkier and more sensitive to calibration errors.</p> <p>In Figure 4, examples of depth estimation using monocular and stereo cameras are shown. The star shape represents an example object in the scene.</p> <p>With a monocular camera, we only have one image at each time instant. To get depth, we wait until the camera has moved. As the camera moves, it sees the same object from different viewpoints. If these viewpoints are sufficiently far apart in space (not just small rotations), we can create a kind of virtual baseline by pairing images from different times. Using these pairs, we can form triangles between the camera positions and the object and apply triangulation to find the distances.</p> Advanced note: In the virtual triangle formed by monocular camera's motion, all three sides are unknown. Yet using a classic computer vision method, namely the Five Points algorithm, you can first have a scaled estimation of motion between two frames (the baseline side of triangle solved), then you can triangulate to find the other two.  Figure 4: The comparison between monocular and stereo cameras in terms of 3D scene understanding. <p>However, there is an important limitation. With a single camera, all distances are only determined up to a scale factor. In other words: we can reconstruct the shape of the scene and the relative distances between objects correctly, but we do not know the true metric values (distances in meters) just from the camera alone. This is known as the scale ambiguity in monocular SLAM. To recover the true metric scale, we need an extra hint, such as using a complementary sensor capable of metric measurments (IMU, wheel odometry, or LiDAR).</p> <p>With a stereo camera, we also form a triangle between the two camera centers and the scene point. But now we know one side of the triangle exactly: the baseline (the distance between the two cameras), because we physically measured it and calibrated the system. This makes some big differences. Firstly, depth can be computed in a single time instant. Moreover, the result is directly in metric units (e.g., meters), without needing extra sensors for scale. This is why stereo cameras are very attractive for SLAM when you want both real-time and metric depth with a relatively simple sensor setup.</p>"},{"location":"sensors.html#215-sensor-requirements","title":"2.1.5 Sensor requirements","text":"<p>The camera used in SLAM should have some specific characteristics. Knowing about these characteristics might help you choosing the best option for running SLAM on your robot.</p> <ul> <li>Field of View (FOV):</li> </ul> <p>For SLAM methods, a wide field of view is usually desirable. A wider FOV lets the camera see more of the environment at once, which means: (i) More potential features in each frame. (ii) A better chance to pick stable, well-distributed features to track. (iii) Less chance of losing everything when the robot makes a quick motion or something briefly blocks part of the view.</p> <p>However, very large fields of view (above roughly 120\u00b0) become more challenging to calibrate (a step you must do before running your SLAM system). In practice, many open-source calibration tools struggle to produce accurate calibration for extreme wide-angle or fisheye lenses.</p> <p>Figure 5 compares the effect of having a wider FOV. The red dots indicate spots in the image that are selected as feature points in the SLAM front-end using a common feature detection algorithm.</p> <p>In the wide-FOV image, it is usually easier to find high-quality features (for example, clear and unique corner points). In the narrow-FOV image, the algorithm often has to rely on lower-quality features (for instance, many similar points inside the trees that are less unique) to maintain a sufficient number of feature points.</p> <p>Another difference is robustness to motion. With a wide FOV, you need a larger sudden motion of the camera to lose sight of all features at once, because the camera sees more of its surroundings. This helps make SLAM more stable during fast rotations or small occlusions.</p> <p>The main drawback of wide-FOV lenses is image distortion. Straight lines and flat surfaces appear curved, especially near the image borders. The wider the FOV, the more the image looks bent or spherical. These distortions must be corrected (rectified) during the calibration stage; otherwise, the SLAM front-end will have a harder time interpreting geometry correctly.</p> Figure 5: Visualization of the difference between narrow (left image) and wide (right image) FOV, and its effect on feature tracking. <ul> <li>Resolution:</li> </ul> <p>If the resolution is too low, visual SLAM may not be able to detect and track enough good features in the image, especially at longer distances. On the other hand, very high resolution introduces unnecessary processing load. The higher number of pixels allocates more CPU/GPU just to tracking features across frames. Also the communication links such as USB ports will be a bottleneck. Ultimately, more time will be spent on each image, thus, updating the map or the position estimation will be done at longer intervals.</p> <p>In practice, you should choose a resolution that is high enough to see stable features (e.g., corners, textures) at the distances you care about, but not so high that it slows down your SLAM system. For that, HD resolution is about a good trade-off.</p> <ul> <li>Shutter:</li> </ul> <p>For visual SLAM, global shutter sensors are ideal, because they minimize motion-related distortions. In a global shutter camera, all pixels are exposed at the same time, so fast motion does not bend or skew objects in the image. </p> <p>In rolling shutter cameras, the image is captured line by line from top to bottom. If either the camera or the scene is moving during this time, straight objects can appear curved. This is called the rolling shutter effect and it can give an unrealistic depiction of the scene.</p> <p>The difference between global and rolling shutter is illustrated in Figure 6 (here, the rolling shutter effect is simulated). On the left, you see the accurate representation of the scene as a global shutter camera would capture. In the middle, the motion of the component in the image is visualized, where all vectors are originating from and expansion point. On the right, the rolling shutter image of the same scene is displayed. As you can see, motion causes the rolling shutter image to give an unrealistic representation. Vertical lines become bent, especially near the image borders. Fast-moving parts of the image become more blurred and distorted.</p> Figure 6: Comparison between the image taken from a global shutter camera (left) and the one taken from rolling shutter (right) due to the motion pattern in scene (middle). The image is taken from the <code>corridor_ground_1</code> sequence in SMARTNav dataset. <p>The strength of the rolling shutter effect depends on how fast things move in the scene (or how fast the camera moves). For slow-moving robots, or setups without strong vibrations, the effect can be almost negligible, and a rolling shutter camera can be perfectly usable and more affordable. For more dynamic applications (fast drones, agile ground robots, strong vibrations), a global shutter sensor is strongly recommended.</p> <p>Many low-cost cameras are rolling shutter but offer adequate resolution for SLAM. Global shutter cameras with high resolution tend to be more expensive, but they provide cleaner geometry for motion estimation.</p> <ul> <li>Data Rate:</li> </ul> <p>A higher frame rate (more images per second) can help in fast motion scenarios, because feature tracks are less likely to be lost between frames. Here, \u201chigh\u201d means higher than the usual 30 frames per second (FPS) that many cameras provide. However, excessively high FPS (for example, above 90 FPS) is often unnecessary in practice: Most SLAM systems cannot process such a high-frequency stream in real time. They will have to drop frames to keep up, which wastes bandwidth and processing. In most robotics applications, something in the range of 30\u201360 FPS is a good compromise between smooth tracking and computational load.</p> <ul> <li>Dynamic Range:</li> </ul> <p>Dynamic range describes the camera\u2019s ability to capture details in both dark and bright regions of an image at the same time. It is often defined as the ratio between the brightest intensity and the darkest intensity the sensor can represent. Any light above this range will be shown as pure white (overexposed, no detail). Any light below this range will be pure black (underexposed, no detail).</p> <p>For SLAM, higher dynamic range guarantees the system can still detect and track features when part of the scene is in deep shadow and another part is brightly lit (for example, a robot exiting a dark corridor into sunlight).</p> Figure 7: Difference beween low dynamic range (left image) and high dynamic range (right image) cameras. <ul> <li>Bit-rate:</li> </ul> <p>The normal image data given by most cameras is an array of 8-bit pixels. This means that each pixel value is represented by 8 bits (8 zeros or ones). As a result, each pixel can take $2^8 = 256$ different values. In simpler terms, a standard 8-bit camera can represent each color channel in 256 different shades.</p> <p>If we use a higher bit depth, such as 10-bit, the number of possible values per pixel jumps to $2^10 = 1024$. This means the camera can represent much finer differences in intensity. This is especially useful when the lighting is very uniform (for example, a dark hallway), or the textures in the scene have colors that are very close to each other.</p> <p>In Figure 8, the left image was captured and converted to 8-bit format (256 shades). Below or next to this image, a histogram of pixel values is shown. This histogram shows how many times a particular color intensity has repeated among all the pixels. As you can see in the histogram, most pixel values are concentrated in a small region: all colors in the image are very similar, giving the image a generally dark appearance with little detail.</p> <p>On the right side, a 10-bit image (1024 shades) from the same environment is shown. The key point to note is that our displays and most standard computer vision libraries still work internally with 8-bit images. This means that when using 10-bit data, we almost always need an algorithm to map or compress the 10-bit range back into 8 bits, while preserving and enhancing as much detail as possible.</p> <p>The exact details of such tone-mapping or compression algorithms are beyond the scope of this guide. However, as a comparison, you can already see the benefit in the right image of Figure 8. Thanks to the extra shades and a proper conversion to 8-bit for display, more details in the dark environment become visible and more useful for SLAM.</p> Figure 8: An example of an 8-image in a low light environment is presented on the left. On the right, example of a 10-bit image from the same environment. The image taken as part of the firefighting robot within the SMART group. <ul> <li>Lens:</li> </ul> <p>This item mostly depends on how custom you want your sensor setup to be. If you want to keep your core sensor (resolution, shutter, frame rate, and connection) the same, but still be able to change or experiment with different FOVs, you should choose a camera that allows you to swap lenses.</p> <p>This information is usually found in the sensor\u2019s datasheet. For instance, an M12 lens mount is a very common and flexible option on the market. Cameras that are specified as having a stock lens (and no lens mount type mentioned) typically do not allow you to change the lens.</p> Figure 9: A camera with an unchangable stock lens (left), one with an M12 mount (center), and different options for lens yielding different FOVs and sometimes difference frequency band cuts and IR filters (right). <ul> <li>Connection:</li> </ul> <p>A flat cable connection (often CSI/MIPI) to the onboard computer (e.g., an Nvidia Jetson) allows for fast data transfer and reduces delays and bandwidth limitations compared to USB. This is ideal when the camera is mounted close to the processing board.</p> <p>However, flat cables have limited length and flexibility. If the camera needs to be mounted farther away or in a more complex mechanical setup, a USB3 camera is often a better compromise between data rate and design flexibility. Finally, USB2 cameras are generally not desirable for SLAM or embedded vision applications because of their lower bandwidth and higher latency.</p> <p>When deciding between USB3 and a flat-cable interface, you should consider the mechanical design (distance, routing, flexibility), and the ports available on your embedded computer (whether it supports CSI/MIPI, USB3, etc.).</p> Figure 10: Example of a camera having a USB connection (left) and a camera with flat cable connection (right). <ul> <li>Pixel size: In simple terms, larger pixels can collect more light (more photons) in the same exposure time. This usually leads to i) better low-light performance (less noise in dark scenes) and ii) better signal-to-noise ratio and sometimes improved dynamic range.</li> </ul> <p>The trade-off is that for a given sensor size, larger pixels mean fewer pixels overall (lower resolution). So you typically balance resolution against pixel size. For dark environments or challenging lighting, slightly larger pixels can make SLAM feature detection and tracking more robust.</p> <ul> <li>Optical size/format:</li> </ul> <p>If you choose a camera with an adjustable lens, you must ensure that the optical format (sometimes listed as 1/2.3\", 1/1.8\", etc.) of the lens and the sensor are compatible. This size determines the effective area of the sensor that is used to capture the image:</p> <p>If the sensor\u2019s active area is smaller than what the lens is designed for, part of the image circle will fall outside the sensor. You will only see the central portion of what the lens projects. Effectively, you get cropped version of what the lens captures. If the sensor is larger than the lens\u2019s image circle, you may get dark corners or a big dark ring, because the lens cannot fully cover the sensor. Matching the optical format helps you use the full sensor area without wasting pixels or getting unwanted dark regions.</p> <ul> <li>Driver:</li> </ul> <p>When buying a camera, it is better to check whether the vendor provides i) a driver that supports embedded computers (often with aarch64 / arm64 architectures, as opposed to amd64 on typical desktops), and preferrably ii) a ROS2 wrapper or ROS2 driver package for the sensor. If both are available, integration becomes much easier and the camera is often close to plug-and-play for robotics applications and trying many open-source SLAM algorithms. Otherwise, you may need to write your own drivers or ROS wrappers, which takes time and expertise.</p>"},{"location":"sensors.html#216-available-solutions","title":"2.1.6 Available solutions","text":"<p>Now, let\u2019s put the knowledge we have gained about cameras into a small sensor selection exercise. Consider the following imaginary robotic application:</p> <p>We want to use SLAM as the main source of navigation information for a small quadcopter drone. The drone will get its local position from the SLAM localization output and will use the generated map to plan collision-free trajectories. We should pick a camera sensor that best fits this application.</p> <p>In the table below, we list a few examples of commonly available cameras on the market and compare them based on the criteria discussed in the previous sections, assuming the above application. Note that the specs in the table are approximate and may change with firmware versions and product revisions. The goal here is not exact numbers, but how to think about the trade-offs.</p> Camera Modality Horizontal FOV Max resolution Shutter Max FPS Dynamic Range (dB) Max bit rate Lens Connection Pixel size ($\u00b5m^2$) Driver Realsense D455 monochrome 87\u00b0 1280 \u00d7 800 global 30 - 10 fixed USB 3.1 3 x 3 Embedded Linux and ROS2 support ZED 2 color 110\u00b0 2208 x 1242 rolling 100 64 8 fixed USB 3.0 2 x 2 Embedded Linux and ROS2 support ZED X color 110\u00b0 1920 x 1200 global 100 71 8 fixed GMSL2 3 x 3 Embedded Linux and ROS2 support Raspberry Pi Camera Module 3 Wide color 102\u00b0 4608 \u00d7 2592 rolling 60 up to 96 in HDR mode 10 stock lens Flat cable 1.4 \u00d7 1.4 Embedded Linux support Arducam multi-cam board with OV9282 base sensors color adjustable 1280 \u00d7 720 global 150 68 10 M12 mount Flat cable 3 \u00d7 3 Embedded Linux support <p>We have highlighted the characteristics with green or red as a simple, binary rating for this specific drone application. This is deliberately a bit harsh and can easily change for other use cases.</p> <p>For example, if you want a colored 3D map of the environment, monochrome sensors are probably not what you are looking for, even though they tend to perform better in low light. In this version of the table, we prioritized low-light performance, so monochrome are marked in green, while purely color-only options are marked in red for that specific criterion.</p> <p>Another important point is that the maximum of some specifications does not occur simultaneously with others. For example, the maximum resolution of a camera often forces a lower FPS than the Max FPS reported in the table (which is usually measured at a smaller resolution or with cropping). When we mark some maximum resolutions as red, it means that, for a typical SLAM application using most available open-source methods, such very high resolutions are often an overkill. They increase processing load and sensor cost without providing proportional benefits.</p> <p>There are also practical considerations that are not fully captured in the table. For instance, cameras like ZED or Intel RealSense often ship with their own SDKs and out-of-the-box depth estimation, which can be directly used or combined with SLAM methods. However, some of these solutions typically require a GPU-equipped embedded computer, which adds to cost and power consumption.</p> <p>In a nutshell, for a beginner-level introduction to SLAM or computer vision on a small drone, ready-to-use options like the ZED cameras or Intel RealSense D455 are attractive choices. They are well-documented, come with SDKs, and already integrate with ROS2. For more advanced users who want tighter control over the sensor setup and better customization to their robot (choice of lens, baseline, FOV, etc.), custom sensor boards such as the Arducam solutions become very attractive, especially when you want to design your own stereo or multi-camera rig.</p>"},{"location":"sensors.html#217-advantages-and-disadvantages-of-visual-slam","title":"2.1.7 Advantages and disadvantages of visual SLAM","text":"<p>There are many advantages to the cameras, that make them a suitable choice for SLAM in robotics. </p> <ul> <li>They are rich. They see almost everything that a human does.</li> <li>They are usually lightweight, that makes them perfect for robots which weight is a determinig factor.</li> <li>They are also energy efficient.</li> <li>Inclusion of the camera has the minimum design burden compared to LiDAR, RADAR, Compass, GNSS, and even the IMU. Meaning that they can be mounted quite flexibly on any part of the robot without much constraint on your design.</li> <li>They can be used for other purposes. For instance, you can put a camera for running your SLAM but also you can use the stream of images to perform another vision task, such as object detection.</li> </ul> <p>The cameras also come with some disadvantages:</p> <ul> <li>They are not able to directly understand the geometry of the scene. This is a necessary task for SLAM and the vision-based techniques for depth estimation are not 100% reliable. Vision-based depth estimation is limited by range and the scene's texture.</li> <li>When processing the camera data, you often face delays in the system, due to the data transfer from sensor to the computer, copy-pastings at Operating System (OS) level, and the processing time of each frame. These delays can be problematic, especially when you want to control a robot using the visual SLAM outputs. </li> <li>The SLAM algorithms that use camera are usually heavier than the ones using other sensors. This further processing often happens during feature extraction and depth estimation. As such, you need a more capable computer.</li> <li>Even if you pick the best sensor in terms of dynamic range, pixel size, infra-red capturing, and etc. you still will not see after a certain level of darkness. </li> <li>In the environments with no texture (uniform, or reflective surfaces), the visual SLAM perfroms poorly.</li> </ul>"},{"location":"sensors.html#22-lidar-for-slam","title":"2.2 LiDAR for SLAM","text":"<p>LiDAR stands for Light Detection and Ranging. Compared to cameras, it is a less general-purpose sensor. LiDAR does not see texture or color, but it is often more specialized and better suited for many robotic applications where understanding the geometry of environment is the main focus.</p>"},{"location":"sensors.html#221-lidar-mechanism","title":"2.2.1 LiDAR Mechanism","text":"<p>LiDAR operates by sending out a laser beam in a very specific and narrow direction in space, then waiting for the reflection of that beam. Based on the travel time of the light (also known as Time of Flight, or ToF), and knowing the speed of light, the sensor can measure the distance to the nearest object in that direction. This ToF-based range measurement is repeated many times in many different directions, giving an estimate of the geometric shape of the environment.</p> Figure 11: The concept of LiDAR mechanism taken LiDAR Wikipedia. On the left, a simplified design for LiDAR sensor is displayed, including the laser source and the rotating mirror that reflects the beam in different spatial directions. In the middle figure, the top view of the space around the sensor is shown in form of rectangular room with the LiDAR (blue rectangle) and another round object (green circle). After capturing the time of flight for the beam in each direction and measuring the distance, a 2 dimensional map can be formed as displayed in the right image."},{"location":"sensors.html#222-pointcloud","title":"2.2.2 Pointcloud","text":"<p>Just like cameras output images as their data format, LiDARs output pointclouds. A pointcloud is a set of 3D points in space. Each point has $x, y, z$ coordinates.  These coordinates are expressed in the sensor\u2019s own coordinate frame, meaning they are defined relative to the LiDAR itself. At this stage, they do not yet have any spatial meaning with respect to a fixed, world-related reference frame. One of the key tasks of SLAM is to estimate the motion of the sensor and transform these points so that they are expressed in a global (world) coordinate frame.</p> <p>The immediate outputs of a LiDAR sensor are illustrated in Figure 12. The top-left image is a camera image, included to give an intuitive view of what the environment in front of the robot looks like.</p> <p>The top-right image shows the 3D pointcloud of the environment. This pointcloud was captured by a LiDAR sensor with a 360\u00b0 field of view, so it sees much more than just the front of the robot (unlike the camera). The robot is located at the center of the empty circular region.</p> <p>By comparing the geometry of the scene as you perceive it in the camera image with the pointcloud, you can see that the pointcloud closely resembles the structure of the environment around the robot. The color of each point encodes its height. Higher points are more reddish, lower points tend to be more greenish.</p> <p>The bottom-left image shows the pointcloud from the camera\u2019s point of view. From this perspective, it becomes even clearer that the 3D structure in the pointcloud matches the scene visible in the camera image.</p> <p>The bottom-right image is another top view, this time highlighting how the density of points changes with distance from the sensor. You can observe that points are much denser in the vicinity of the LiDAR and become more sparse as the distance increases. This means that LiDAR provides a very detailed representation nearby, with gradually lower resolution farther away.</p> Figure 12: Visualization of LiDAR pointcloud taken from the <code>corridor_ground_1</code> sequence of the SMARTNav dataset. <p>This geometric, 3D-centric data format (pointclouds) is what makes LiDAR particularly powerful for SLAM, especially when accurate distance measurements and obstacle shapes are more important than texture or color.</p>"},{"location":"sensors.html#223-configurations-and-examples","title":"2.2.3 Configurations and Examples","text":"<p>LiDAR technology can be implemented in different design configurations. Having a general understanding of these designs helps you identify the best option for each application.</p> <ul> <li>LiDAR Range Finders:</li> </ul> <p>This is the simplest version. It only measures the distance in one spatial direction (a single beam). It is not often used directly in SLAM, but it is very common in robotics, especially for aerial robots. For instance, it can be attached to the bottom of a drone to measure the distance to the ground more accurately.</p> <ul> <li>2D Planar Scanners: </li> </ul> <p>These are very common in ground robots. A 2D scanner LiDAR provides, at each scan, a two-dimensional view of obstacles in a single horizontal plane around the sensor.   The mechanism of planar scanners is very similar to what is shown in Figure 11. A beam is rotated around, and distances are measured in many directions, but all in one plane.</p> <p>Some of the advantages of planar scanners inclue: simplicity, relatively low price, and mature and widely tested SLAM algorithms. However, their main limitation is that they cannot sense anything outside that plane.</p> <p>Figure 18 (top-left) shows an example of such a planar scanner. As you can see, the sensor can detect the table legs in its scanning plane, but it cannot understand that 2 meters above the floor there may be free space with no obstacles.</p> <p>If the robot is a wheeled ground robot that stays on the same flat floor, a 2D LiDAR gives a very useful and consistent picture of the surrounding obstacles. But if the robot is an aerial robot with extra degrees of freedom (it can change height, roll, or pitch), the 2D LiDAR only gives a partial view of the environment. Even for ground robots, if the floor is uneven or if we have a legged robot that frequently tilts (changes orientation), a planar LiDAR is no longer optimal, because the scanning plane no longer corresponds to a fixed, meaningful slice of the environment.</p> <p>SLAM algorithms based on 2D LiDARs also typically generate 2D maps of the environment. Figure 13 shows an example of such a 2D LiDAR-based SLAM. At the bottom, a simulated robot (blue) is moving between rooms. At the top, its momentary scans are drawn as red points. Over time, a 2D occupancy map is built, where black areas indicate obstacles and gray areas indicate free space.</p> Figure 13: Example of a 2 dimensional SLAM algorithm (Hector SLAM) that uses planar scans. Image taken from https://doi.org/10.1155/2020/8867937.  <p>An example of such a LiDAR in the market is the RPLIDAR family of sensors.</p> Figure 14: RPLiDAR a2, an example of a planar scanner.  <ul> <li>3D Multi-beam:</li> </ul> <p>Unlike the single-beam mechanism shown in Figure 11, these sensors simultaneously send out multiple beams from multiple lasers stacked vertically. Each laser emits in a slightly different vertical direction, so together they sense the environment in full 3D, beyond just one scanning plane (as you can see in Figure 18, top-right).</p> <p>The pointcloud generated by such a sensor is composed of several rings around the LiDAR. Each ring is essentially a planar scan from one of the laser channels. One important point is that if the LiDAR is kept still in a static environment, it will always produce the same pointcloud pattern over time. The structure is highly repeatable from scan to scan.</p> <p>There are several advantages to these LiDARs. They often provide a full 360\u00b0 view around the sensor. The data is structured and repeatable, which can be useful for algorithm design.</p> <p>There are also key disadvantages. They usually contain moving parts (rotating head), which can affect long-term reliability and might add vibrations to the system. Moreover, they are more expensive than 2D planar scanners. The vertical resolution is limited, so the pointcloud can be sparse, and thin objects may fall between rings and be under-sampled.</p> <p>Another major drawback is motion distortion, similar to the rolling shutter effect in cameras. Each full sweep of the beams takes tens of milliseconds to complete. If the robot or objects in the scene move during this time, the resulting pointcloud will be slightly warped compared to the true shape of the environment.</p> <p>An example of such LiDARs is the Ouster OS1-128 sensor.</p> Figure 15: The Ouster OS1-128 sensor.  <ul> <li>Non-repetitive Scan:</li> </ul> <p>There are also LiDAR designs that do not use a conventional mechanical rotation to generate a repetitive scan pattern. One example is the Livox Mid-40 sensor, which generates a pseudo-random scanning pattern within a relatively small field of view (instead of a full 360\u00b0).</p> <p>This LiDAR uses the Risley prism mechanism, as shown in Figure 16. The laser beam passes through two rotating prisms that spin in opposite directions, producing a pseudo-random scanning pattern. The resulting pattern is illustrated in Figure 17. As time passes, the scanned area becomes denser and denser.</p> <p>Figure 18 bottom-left, also shows a 3D pointcloud from such a sensor. Compared to multi-beam 360\u00b0 LiDARs, the Mid-40 produces a much denser representation of what lies in a narrow FOV in front of the sensor. This helps address the sparsity problem discussed for multi-beam LiDARs. A relatively lower price compared to many 360\u00b0 spinning LiDARs is another advantage.</p> <p>However, there are important limitations. To obtain a very dense pointcloud, the environment must stay still for a while, so this type of sensor is not ideal for fast-moving robots or dynamic scenes. Because each instantaneous scan has an irregular pattern, many traditional feature-matching and registration algorithms in LiDAR-based SLAM front-ends find it harder to work with this type of data. Lastly, the limited FOV means that the sensor can lose features quickly during rotations, and it can be easily obstructed if the robot is close to a wall or sees only a flat surface.</p> <p>There are newer variants of this non-repetitive scanning idea that provide 360\u00b0 coverage while keeping some of the benefits of pseudo-random patterns, which helps mitigate some of the issues mentioned above.</p> Figure 16: The Risely prism mechanism, used in Livox Mid-40 LiDAR to create non-repetitive scans. Image taken from https://doi.org/10.3390/s21144722.  Figure 17: The non-repititive pattern of   <ul> <li>Solid-state sensors: </li> </ul> <p>These LiDARs are similar in spirit to multi-beam sensors, with the main difference that they do not use mechanical rotation to achieve coverage. Instead, they use tiny mechanisms such as Optical Phased Arrays (OPA) to steer the laser beam towards different directions in space electronically.</p> <p>In practice, the beam is still pointed at different directions over time, so solid-state LiDARs share most of the characteristics of multi-beam rotating sensors in terms of their applicability to SLAM. The main differences are: i) They lack mechanical rotations, which can improve long-term usage. ii) They usually have a smaller field of view (FOV) compared to 360\u00b0 spinning LiDARs.</p> <p>However, the resolution in a given direction can be higher, resulting in denser pointclouds in that limited FOV. This contrasts with multi-beam rotating sensors, where the density is constrained by the number of vertical beams and the rotation speed.</p> <p>This difference is also visible in Figure 18, bottom-right, where the solid-state sensor provides a dense 3D description in a narrower viewing cone rather than a full 360\u00b0 surround view.</p> Figure 18: Diffetent data patterns for LiDAR sensors. The sensor is assumed to be the black cylinder in the middle of the room, and the 3D point measurements are the red points projected on the environment."},{"location":"sensors.html#224-lidar-based-front-end-architecture","title":"2.2.4 LiDAR-based front-end architecture","text":"<p>An overall architecture of LiDAR-based SLAM methods looks like Figure 19. Compared to visual SLAM, this architecture is a bit simpler, because we do not need any step to infer 3D positions of features: the sensor data is already in an accurate 3D format (the pointcloud).</p> <p>Similar to visual SLAM, we still need to estimate how consecutive scans are related to each other. It is usually simpler and faster to do this not on the raw pointcloud directly, but on some extracted features. Feature extraction is therefore useful here as well. It speeds up processing and focuses the computation on the most informative parts of the data.</p> <p>Unlike images, features in pointclouds are less intuitive. One example of a popular feature representation is shown in Figure 19, known as Scan Context. The idea is to convert a 3D pointcloud into a 2D descriptor (a kind of image-like projection). This 2D representation can then be compared more easily and quickly with other scans using efficient vision-style methods.</p> <p>After extracting features for individual scans, they can be matched with each other. This process is often called registration. Once registration is done, we can estimate the rigid transformation (translation and rotation) that aligns two consecutive scans. These relative motions are then passed to the SLAM back-end, which uses them to optimize the robot\u2019s trajectory and the map over time, keeping localization and mapping errors as small as possible.</p> Figure 19: The general front-end architecture of LiDAR-based SLAM."},{"location":"sensors.html#226-advantages-and-disadvantages","title":"2.2.6 Advantages and Disadvantages","text":"<p>LiDAR sensors have many advantages over the cameras, which makes them the most reliable senor for SLAM:</p> <ul> <li>They capture the metric geometry of the scene directly, without any need for estimating the depth. They work even if the scene has few visual features, as long as there is some geometric structure.</li> <li>They are much less sensitive to lighting conditions, whether it is strong daylight or complete darkness at night. The main issues arise only with surfaces that do not reflect the laser well. In general, they are a great solution for night-time SLAM.</li> <li>Depending on the sensor type, LiDARs can cover large scenes, measure objects farther away, and some models provide very wide FOVs, making them suitable for many outdoor robots.</li> </ul> <p>Despite many advantages of LiDAR, there are certain drawbacks of these sensors:</p> <ul> <li>They are typically more expensive, heavier, and consume more power than cameras. These requirements grow as you demand more fidelity (higher resolution, more beams, longer range). Many LiDARs are not ideal for small, lightweight drones.</li> <li>LiDAR pointclouds are often sparse, especially at longer ranges. They may miss fine details and small objects, or only sample them with a few points.</li> <li>LiDARs do not capture color information. If you use a LiDAR-based SLAM map, your main clue is the shape and geometry of the environment; you do not get semantic cues that are strongly tied to color.</li> <li>Many LiDARs are mechanically complex, often relying on rotating mechanisms, which may reduce robust long-term use.</li> <li>Motion distortion is usually more severe than in global-shutter cameras. Because a LiDAR scan is built over a certain time window, fast motion of the robot or the environment can warp the pointcloud. As a result, strong deskewing (motion compensation) is often needed in dynamic environments.</li> </ul>"},{"location":"sensors.html#23-imu","title":"2.3 IMU","text":""},{"location":"sensors.html#231-concept","title":"2.3.1 Concept","text":"<p>An Inertial Measurement Unit (IMU) is one of the classic sensors used in navigation. Unlike cameras or LiDARs, IMUs do not sense the environment directly. Instead, they measure the motion of the robot itself. The IMUs commonly used in robotics measure six values: i) Angular velocity (how fast the robot is rotating) around 3 axes ii) Linear acceleration (how fast its speed is changing) along 3 axes. So in total, we get 3D angular velocity plus 3D linear acceleration.</p> Figure 22: The data that IMU sensors capture from the motion of the robot. <p>Why are these 6 values important? In principle, if you integrate the IMU measurements over time:</p> <ul> <li>Integrating linear acceleration gives you linear velocity.</li> <li>Integrating velocity again gives you position.</li> <li>Integrating angular velocity gives you orientation (how the robot is rotated).</li> </ul> <p>This idea is known as Dead Reckoning and, at first glance, it looks like it could solve the full robot localization problem using only the IMU.</p> <p>However, in practice, IMU measurements are noisy and have biases. When you integrate them over time, you also integrate the noise and bias. As a result, the estimated position and orientation drift away from reality. The longer you rely on pure IMU integration, the larger this drift becomes, and the more inaccurate your pose estimate will be.</p>"},{"location":"sensors.html#232-benefits-of-imu-addition-to-slam","title":"2.3.2 Benefits of IMU addition to SLAM","text":"<p>Despite the problems of pure dead reckoning, IMUs have a very useful property that perfectly complements other SLAM sensors, and that is their high sampling rate. An IMU often runs at hundreds of samples per second, while LiDAR is usually around 10\u201320 Hz, and cameras are typically tens of frames per second. Because of this, an IMU can fill the gaps between measurements from environment-sensing sensors (LiDAR and camera). </p> <p>Consider the following scenarios where the IMU is extremely useful as a complementary sensor:</p> <ul> <li>LiDAR and camera-based SLAM systems rely on feature matching between consecutive frames. Sometimes this fails beacause: i) The camera or LiDAR view is temporarily blocked. ii) The feature matching algorithm cannot find reliable matches. iii) The environment is nearly featureless (blank walls, fog, smoke). In these cases, IMU measurements can keep tracking the motion for short periods, reducing drift until features reappear.</li> <li> <p>When the robot moves quickly, the view in front of the sensor changes rapidly. Lower-frequency sensors (cameras, LiDAR) may miss some of that motion, but the high-frequency IMU can capture it. Fusing IMU data with SLAM helps reduce the overall drift and improves stability in dynamic maneuvers.</p> </li> <li> <p>With only a monocular camera, SLAM can usually reconstruct the scene only up to scale (no absolute meters). When you add an IMU, its acceleration and rotation measurements provide extra information that can be used to recover the correct metric scale of the trajectory and the map.</p> </li> <li> <p>A major issue in LiDAR SLAM is scan distortion due to motion. Many LiDARs provide a timestamp for each point in the pointcloud. If we know the short-term motion between these timestamps (from the IMU), we can deskew the scan. We correct each point\u2019s position according to the estimated motion and obtain a pointcloud that more accurately represents the environment.</p> </li> <li> <p>Fusing IMU data in the SLAM pipeline can also smooth the estimated trajectory, reducing jitter in the estimated motion of the camera or LiDAR. This often leads to a more stable and robust SLAM system, especially for control and path planning.</p> </li> </ul> <p>Although IMU-based dead reckoning is not suitable for long durations, it is very useful over short intervals, until a new frame arrives. In SLAM practice, IMU data is mostly used in the SLAM back-end. It is also used in front-end to reduce motion distortion in both images and pointclouds.</p> <p>Most SLAM-ready sensors on the market, including many LiDARs and stereo cameras, already come with an integrated IMU, providing tightly synchronized IMU data together with their main measurements. This is very convenient for sensor fusion. However, if you are building your own sensor stack from scratch and you need an IMU, look mainly for low noise density, and low bias instability. While keeping the cost reasonable. Using very expensive, high-grade IMUs often goes against the goal of building a low-cost navigation stack for a typical robot (one of strong motivations of SLAM). A few common IMU choices in low to mid-cost systems are Bosch BMI270, Bosch BMI160, and InvenSense ICM-20602.</p>"},{"location":"sensors.html#24-summary","title":"2.4 Summary","text":"<p>This chapter gave a hands-on guide to choosing sensors for SLAM so your pipeline starts with clean, useful data. We covered cameras (color vs. mono, mono vs. stereo), why specs like FOV, resolution, shutter, FPS, dynamic range, bit depth, pixel size, lens/format, connection, drivers matter, and how they affect feature quality, latency, and calibration. We also skimmed LiDAR types (2D planar, 3D multi-beam, non-repetitive, solid-state), their scan patterns and motion distortion, plus the IMU as a high-rate aid for scale, deskewing, and smoothing.</p> <p>Bottom line: SLAM robustness is tightly tied to sensor data quality and the sensor\u2019s ability to capture the environment under your real conditions (low light, textureless walls, fast motion, vibration). With the trade-offs and examples here, you should be able to match specs to your robot and task and pick a sensor stack (camera, LiDAR, IMU) that gives your SLAM front-end reliable, information-rich input.</p>"},{"location":"sensors.html#quizzes","title":"Quizzes","text":"Q1. Which of these sensors does not directly measure the external environment? 2D planar LiDAR Monocular camera IMU Thermal camera IMU measures the platform\u2019s motion (accelerations/rotation), not scene geometry or appearance.  Q2. In a SLAM system, which part extracts useful information (features) from raw images? Front-end Back-end Loop closer Map optimizer Your text defines the front-end as turning raw measurements into compact cues (features, lines, etc.). Q3. Which data source usually captures the most accurate geometric detail of the scene in front of the robot? Color image Monochrome image (same resolution) 3D LiDAR RGB-D camera LiDAR directly measures range to build a 3D point cloud; cameras need inference/triangulation and RGB-D is range-limited.  Q4. If we want to use perform a 3D mapping of environment using a low-cost indoor search-and-rescue drone, what will be the best camera choice? Monocular thermal camera Stereo monochrome Stereo color camera Mono color camera Stereo gives per-frame depth; monochrome improves low-light and SNR vs. color CFA.  Q5. Outdoor aerial SLAM at ~100\u00a0m altitude: which stereo parameter matters most to change? Dynamic range Shutter type Modality (color/mono) Baseline Depth precision scales with focal length in pixels and baseline; large distances need a larger baseline. Q6. Which camera example in your table is best for moving from dim indoors to bright sun (high DR jumps)? Intel RealSense D455 ZED 2 Raspberry Pi Camera Module 3 Wide Arducam OV9282 (on multi-cam board) Pi Cam 3 Wide lists \u201cup to 96\u00a0dB in HDR mode,\u201d higher than the others shown. Q7. Pair of front-facing cameras on a car; compute is in the dashboard (long cable run). Which sensor choice is most practical? Arducam multi-cam board (CSI) ZED 2 Raspberry Pi Camera Module 3 Arducam OV9282 bare sensor ZED 2 is integrated stereo over USB\u00a03 (longer run, less CSI cabling complexity than flat cables). Q8. Which camera characteristic does not improve low-light performance directly? Modality (mono vs. color) Shutter type Bit depth / bit rate Pixel size Higher bit depth improves quantization fidelity, not SNR; larger pixels/mono help capture more photons. Q9. Assume you are building a wheeled robot for floor-cleaning in sports arenas. And also, you have decided to use SLAM for navigation. Which sensor is the most suitable choice to be used for your SLAM? Monocular camera 2D planar LiDAR RGB-D camera Thermal camera Since we talk about a ground wheeled robot, it means the robot body has two degrees of freedom and only moves in a 2D plane. Also, you design it for arena floor which is usually perfectly flat. Hence, a planar 2D LiDAR would be sufficient and cost-efficient. Q10. Which combo typically yields the most accurate and robust VIO/SLAM? Stereo camera + IMU Planar LiDAR + IMU Monocular camera + IMU 3D LiDAR (alone) Stereo offers metric scale per frame; IMU stabilizes fast motion\u2014this pairing is widely adopted. Q11. Which feature type is shown as red dots linked across frames in your workflow figure? Corner/keypoints Semantic segments Superpixels Dense optical flow The front-end example tracks corner-like feature points (red dots) with lines between frames. Q12. Which shutter type reduces motion-induced geometric skew in fast motion? Global shutter Rolling shutter Electronic first-curtain Mechanical curtain Your text illustrates rolling-shutter distortion vs. global-shutter stability.  Q13. Increasing horizontal FOV from 70\u00b0 to ~110\u00b0 mainly helps the SLAM front-end by\u2026 Seeing more\u2014and better distributed\u2014trackable features Eliminating lens distortion Reducing compute cost Improving color fidelity Wider FOV tends to increase unique features and robustness to temporary occlusions. Q14. A downside of very wide FOV (&gt;120\u00b0) in your notes is\u2026 Calibration becomes harder (distortion models &amp; tools) Lower dynamic range by physics Global shutter becomes impossible No lens options exist You explicitly warn that calibration tools struggle as FOV gets very large. Q15. Bit depth mainly affects\u2026 Number of tonal/shade levels captured Physical sensor size Depth range of stereo Laser safety class 10-bit gives 1024 levels vs 256 in 8-bit; your figure shows the histogram example. Q16. Which is a benefit of monochrome over color for SLAM? Higher sensitivity (no CFA loss) and better low-light SNR Always higher resolution sensor No calibration needed Free depth measurements No color filters \u2192 more photons per pixel; you discuss NIR response and SNR.  Q17. Which connection is usually easier for long runs to a dashboard PC? USB\u00a03 Flat CSI cable GPIO I2C You note CSI/flat cables suit short distances near the compute board; USB3 gives more flexibility.  Q18. What is a LiDAR\u2019s immediate data product? Raster image 3D point cloud (XYZ) IMU delta pose Height map only Your LiDAR section emphasizes ToF \u2192 3D point cloud in sensor coordinates.  Q19. Which LiDAR configuration sweeps a single plane? 2D planar scanner 3D spinning multi-beam Flash LiDAR Non-repetitive sector scan Planar scanners produce a polar sweep in one plane (common for indoor robots).  Q20. What\u2019s the main trade-off of very high resolution for cameras in SLAM? Higher compute and bandwidth, added latency Lower lens distortion automatically Higher dynamic range automatically No calibration needed You warn that high resolution burdens buses/compute and increases tracking latency.  Q21. Which lens option gives you flexibility to change FOV later? M12-mount lens camera Fixed stock lens only No-lens pinhole Any USB webcam Your lens section contrasts fixed stock lenses with M12-mount swappable lenses.  Q22. Which rolling-shutter artifact did you visualize in your figures? Skewed/bent verticals during motion Random dead pixels Banding from PWM lights only Chromatic aberration at edges Your example shows verticals bending near the edges under motion with RS readout."}]}