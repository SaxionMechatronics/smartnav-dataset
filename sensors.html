
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="introduction.html">
      
      
        <link rel="next" href="calibration.html">
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Sensor Selection for SLAM - SMARTNav Dataset</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="css/custom.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-2-sensor-selection-for-slam" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="index.html" title="SMARTNav Dataset" class="md-header__button md-logo" aria-label="SMARTNav Dataset" data-md-component="logo">
      
  <img src="images/SMART-RG-White-transparent.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SMARTNav Dataset
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Sensor Selection for SLAM
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="index.html" title="SMARTNav Dataset" class="md-nav__button md-logo" aria-label="SMARTNav Dataset" data-md-component="logo">
      
  <img src="images/SMART-RG-White-transparent.png" alt="logo">

    </a>
    SMARTNav Dataset
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Practical Guide to SLAM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Practical Guide to SLAM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="introduction.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Sensor Selection for SLAM
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="sensors.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Sensor Selection for SLAM
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#objectives" class="md-nav__link">
    <span class="md-ellipsis">
      Objectives
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#role-of-sensors-in-slam-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Role of Sensors in SLAM Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#21-cameras-for-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Cameras for SLAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 Cameras for SLAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#211-image" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.1 Image
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-camera-modalities" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Camera modalities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#213-camera-configurations" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.3 Camera configurations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-camera-based-slam-front-end" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Camera-based SLAM Front-end
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#214-sensor-requirements-what-to-buy-why" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.4 Sensor requirements (what to buy &amp; why)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#215-failure-modes-mitigations" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.5 Failure modes &amp; mitigations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#216-available-solutions-examples" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.6 Available solutions (examples)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#217-custom-camera-stack-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.7 Custom camera stack checklist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#218-advantages-and-disadvantages-of-visual-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.8 Advantages and disadvantages of visual SLAM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-lidar-for-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 LiDAR for SLAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 LiDAR for SLAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-lidar-types" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.1 LiDAR types
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-requirements-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.2 Requirements &amp; parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223-configuration-operation" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.3 Configuration &amp; operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-available-solutions-drivers" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.4 Available solutions &amp; drivers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#225-custom-mounting-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.5 Custom mounting checklist
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23-inertial-measurement-unit-imu" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 Inertial Measurement Unit (IMU)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 Inertial Measurement Unit (IMU)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#231-imu-classes" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.1 IMU classes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#232-requirements-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.2 Requirements &amp; parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#233-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.3 Configuration
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#24-fusion-considerations-when-to-combine-sensors" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 Fusion considerations (when to combine sensors)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25-time-synchronization-timestamp-quality" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 Time synchronization &amp; timestamp quality
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26-mounting-placement-cabling" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 Mounting, placement &amp; cabling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#27-compute-bandwidth-storage-planning" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 Compute, bandwidth &amp; storage planning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#28-procurement-acceptance-tests" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 Procurement &amp; acceptance tests
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#29-available-solutions-vs-custom-solution" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 “Available solutions” vs. “Custom solution”
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#210-practical-considerations-pitfalls-field-notes" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 Practical considerations &amp; pitfalls (field notes)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#211-quick-decision-guide-by-scenario" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 Quick decision guide (by scenario)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.11 Quick decision guide (by scenario)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendix-a-parameter-cheat-sheets" class="md-nav__link">
    <span class="md-ellipsis">
      Appendix A — Parameter cheat-sheets
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="calibration.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Calibtation of Sensors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="deployment.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deployment of SLAM Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="interpretation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Interpretation of SLAM Results
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#objectives" class="md-nav__link">
    <span class="md-ellipsis">
      Objectives
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#role-of-sensors-in-slam-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Role of Sensors in SLAM Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#21-cameras-for-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Cameras for SLAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 Cameras for SLAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#211-image" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.1 Image
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-camera-modalities" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Camera modalities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#213-camera-configurations" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.3 Camera configurations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-camera-based-slam-front-end" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Camera-based SLAM Front-end
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#214-sensor-requirements-what-to-buy-why" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.4 Sensor requirements (what to buy &amp; why)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#215-failure-modes-mitigations" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.5 Failure modes &amp; mitigations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#216-available-solutions-examples" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.6 Available solutions (examples)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#217-custom-camera-stack-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.7 Custom camera stack checklist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#218-advantages-and-disadvantages-of-visual-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.8 Advantages and disadvantages of visual SLAM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-lidar-for-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 LiDAR for SLAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 LiDAR for SLAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-lidar-types" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.1 LiDAR types
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-requirements-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.2 Requirements &amp; parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223-configuration-operation" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.3 Configuration &amp; operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-available-solutions-drivers" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.4 Available solutions &amp; drivers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#225-custom-mounting-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.5 Custom mounting checklist
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23-inertial-measurement-unit-imu" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 Inertial Measurement Unit (IMU)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 Inertial Measurement Unit (IMU)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#231-imu-classes" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.1 IMU classes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#232-requirements-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.2 Requirements &amp; parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#233-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      2.3.3 Configuration
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#24-fusion-considerations-when-to-combine-sensors" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 Fusion considerations (when to combine sensors)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25-time-synchronization-timestamp-quality" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 Time synchronization &amp; timestamp quality
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26-mounting-placement-cabling" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 Mounting, placement &amp; cabling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#27-compute-bandwidth-storage-planning" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 Compute, bandwidth &amp; storage planning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#28-procurement-acceptance-tests" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 Procurement &amp; acceptance tests
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#29-available-solutions-vs-custom-solution" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 “Available solutions” vs. “Custom solution”
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#210-practical-considerations-pitfalls-field-notes" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 Practical considerations &amp; pitfalls (field notes)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#211-quick-decision-guide-by-scenario" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 Quick decision guide (by scenario)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.11 Quick decision guide (by scenario)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendix-a-parameter-cheat-sheets" class="md-nav__link">
    <span class="md-ellipsis">
      Appendix A — Parameter cheat-sheets
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="chapter-2-sensor-selection-for-slam">Chapter 2: Sensor Selection for SLAM</h1>
<h2 id="objectives">Objectives</h2>
<p>The objective of this chapter is to get an introduction to different sensing modalities that are common in SLAM. Furthermore, we will discuss advantages and disadvantages of each sensing modality.</p>
<p>At the end of this chapter, it is expected that the reader will be able to:
- Identify the suitable set of sensors for their application
- They will be familiar with the common trade-offs regarding the SLAM sensors, including the trade-offs between accuracy, cost, computation, environment, and maturity of algorithms.
- They will have an idea of which specifications to consider when choosing sensors for SLAM</p>
<h2 id="role-of-sensors-in-slam-architecture">Role of Sensors in SLAM Architecture</h2>
<p>SLAM often has two separate internal components, namely the front-end and back-end. </p>
<p><strong>Front-end</strong>: It is the module that turns raw sensor measurements of sensors into an intermediate representations that are more compact and informative about robot's or environment's state.
For example, if a camera sensor is used for SLAM, it outputs images at a resolution of let's say 640x480. This means that we will have 307200 pixels each conveying some information about the environment. This is an enormous amount of information that is very difficult to comprehend and use. Hence, we need a set of processes to filter out this information and only extract a minimal set of relevant information that are insightful about robot's motion and states.</p>
<p><strong>Back-end</strong>: It is the core algorithm, usually in the form of optimization or filtering, that takes the intermediate representations and outputs the robot's location and environment's map. The details of SLAM backend is not the focus of this course.</p>
<p>As evident from the above definitions, choice of sensor defines the SLAM front-end architecture.</p>
<h2 id="21-cameras-for-slam">2.1 Cameras for SLAM</h2>
<h3 id="211-image">2.1.1 Image</h3>
<p>If you have previous experince with compputer vision or image processing, you can not find a more obvious information other than explaining what an image is! Otherwise, it is worth introducing you to the direct product of a camera. An image in computers is represented as a matrix of numbers. Each cell in the matrix is called a pixel, a discrete piece of infomation, determining how a tiny portion of the view looks like (Figure 1). As such, it must be intuitively easy to imagine that the higher the number of tiny pixels besides each other, the more clear picture of the view will be formed.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Pixels concept" src="images/image_pixels.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 1: Pixels, the building blocks of an image.</em></td>
</tr>
</tbody>
</table>
<h3 id="212-camera-modalities">2.1.2 Camera modalities</h3>
<p>Cameras come at many different measurement modalities, such that the images they form will look different or measure different thing. The cameras that are usually common in robotic applications such as SLAM are mostly covering the visible light wavelengths. Figure 2, portrays the electromagnetic spectrum, and the whereabouts of the visible part of the spectrum (rainbow section) clearly indicates the target wavelengths that the cameras usually measure.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Spectrum" src="images/light_spectrum.drawio.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 2: What each sensing modality of cameras measures.</em></td>
</tr>
</tbody>
</table>
<p><strong>Color cameras</strong>: These cameras are meant to capture a scene exactly in the most similar manner to the way we see the environment, by presenting the information in different shades of colors. In order to understand an important characteristics of the cameras that can be influential is robot perception and navigation, we must delve a bit deeper into how cameras work.</p>
<p>In the spectrum of the Figure 2, the wavelengths between  380-700 nm is covered by color cameras. However, the way that a color camera can capture the actual color is through a specific mechanism. In fact, a color camera is able to measure 3 values for each pixel, i) the intensity of the blue ii) green and iii) red colors. The sensitivity of the pixel to each of these colors, follows the distribution presented in Figure 2. For instance, a color corresponding to the frequency 400 nm, will mostly excite a blue color and less green and much less red.</p>
<p>However, the capturing of these 3 numbers at the same time follows an interesting engineering trick in the design of color sensors. As depicted in Figure 3, color cameras don’t record three color numbers per pixel directly. Instead, a color filter array (CFA), most commonly the Bayer pattern, places red, green, or blue filters over individual sensor pixels in an alternating mosaic (Figure 3). Each pixel measures only one color component, and the camera later uses demosaicing (e.g., nearest-neighbor, bilinear, bicubic, or more advanced methods) to reconstruct full RGB values for every image pixel. The Bayer pattern includes twice as many green filters as red or blue because luminance (perceived detail) is largely carried by the green channel, so oversampling green improves sharpness and noise performance (the choice also aligns with human visual sensitivity). The filters themselves pass only the corresponding wavelength ranges for R, G, and B (see Figure 2). Please refer to <a href="https://ieeexplore.ieee.org/document/1407714">this reference</a> for more details.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="CFA" src="images/filter_array.drawio.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 3: Comparison of monochrom and color camera mechanisms for capturing the image.</em></td>
</tr>
</tbody>
</table>
<p><strong>Monochrom</strong>: Unlike color cameras, there is no filter arrays needed for different pixels. In monochrom cameras, all the pixels are used for capturing some portions of the scene, thus the full potential of the physical pixels of the camera is used to capture the scene details, and no approximation through interpolation is usually needed. This is further depicted in Figure 3 by comparing these two sensing modalities.</p>
<p>Another consequence of the color filters is that, they narrow down spectrum of the incoming rays of light, only passing a rather narrow band (related to the R, G, or B color). In Figure2, we have tried to show the range of wavelengths that are covered by R filter, G filter, B filter, and finally no filter (monochrome). These are approximation curves and they slightly differ for each real sensor. These curves are usually provided by the manufacturer. As is evident, a monochrome sensor (gray curve), covers a wider range of spectrums compared to the combination of R,G,B filters together. Specefically, it covers more on a portion of the infra-red band, also known as near-infrared. As a result, each pixel will receive less share of light rays (photons so to say); however, a monochrom camera due to lack of the filters can receive more light rays and is more illuminated compared to RGB cameras, improving performance at the darkness.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Cameras images" src="images/cameras_images.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 4: Comparison between images taken from different ranges of electromagnetic spectrum. Image taken from <a href="https://doi.org/10.48550/arXiv.2108.11767">https://doi.org/10.48550/arXiv.2108.11767</a></em></td>
</tr>
</tbody>
</table>
<p>To make the discussion more sensible, Figure 4, brings a comparison between images taken from different electromagnetic bands. For instance, the RGB camera captures nice representation of the lit parts of the scene, also providing more semantics through the color information. However, the same scene captured using a monochrom camera that also captures some light in near-infrared (NIR) band can show more of the shadowed section of the scene, while providing a good geometrical understanding of the lit portion, yet missing the color details. The more the camera moves towards higher wavelengths (more towards middle-infrared and far-infrared) the more it turns into a thermal camera, seeing better at darkness and sensing the heat, yet the more specialized the sensors become that comes at a higher cost.</p>
<h3 id="213-camera-configurations">2.1.3 Camera configurations</h3>
<p><strong>Monocular camera</strong>: It captures image data through a single lens. The benefit is that it is the most compact and efficient sensor setup in robotics, and SLAM systems. The drawback is that, this makes it more complicated to understand the geometry of the scene. </p>
<p>Intuitively, it is not possible to understand the other dimensions of the real-world objects (also known as depth estimation), unless you observe them from at least two different points of view. This 3D understanding about the surrounding objects is central to the SLAM methodologies. Using only a single camera seems to prevent us from understanding the distances of the scene components from us; however, there are a few tricks that will enable depth estimation even using monocular cameras.</p>
<p>The traditional approach is to move the camera around and use the observation of the same object in multiple frames at different times. Although the depth estimation will not be at hand at every frame, but after some time, assuming sufficient spatial translations, we should be to look at the same targets from different angles. This allows us to do simple geometrical analysis also known as <em>Triangulation</em> to measure objects' distances from the camera.</p>
<p>There is a more recent approach to solve the monocular depth estimation problem, which is taking advantage of deep neural networks. These networks are trained using massive datasets, learning how to estimate the relative distances of object from camera only using a single image. This depth estimation based on experience is kind of similar to the way a human can infer depth if he covers one of his eyes, such that he still can understand distances but mostly because of experience. </p>
<p><strong>Stereo camera</strong>: If two monocular cameras are attached together, such that they share most of their view, it is a stereo camera setup. This is very similar to human way of observing the environment. In this setup, at every timestamp, the calculation of the scene depth is possible, since we already have two different views of the object. Simlar triangulation techniques, or more complicated methods are possible to be used here. The accuracy of the output depth estimation relies on many factors such as the algorithm. The maximum range of depth estimation depend on how separated the two cameras are from each other (also called the stereo baseline). </p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Cameras images" src="images/stereo_vs_mono.drawio.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 5: The comparison between monocular and stereo cameras in terms of 3D scene understanding.</em></td>
</tr>
</tbody>
</table>
<p>In the Figure 5, examples of monocular and stereo cameras are highlighted. As evident, for a monocular camera, only by having dicplacements with respect to an object in the scene (the star shape represents an object), it is possible to create a virtual stereo camera by putting together the pairs of images from different times (given that they are far apart sufficiently). Then, a triangle can be assumed, however, all three side of the triangle are unknown, yet using two consecutive computer vision methods, namely the <em>Five Points</em> method, then <em>Perspective n Points</em>, a value for these sides will be at hand which is proportionally correct, however, does not have the right metric scale. The details for these algorithms is for interested readers. At the end, it should be emphasized that the monocular camera can not yield the right metric values for the depth of the features and only can deliver values for distances of objects that are correct relative to each other (correct proportions). If we want to convert them to correct metric values, another hint (another sensor such as IMU) should be incorporated.</p>
<p>By comparison, the stereo camera in Figure 5, forms a similar triangle, however in one time instance and not across two different time instances. Moreover, the triangle that is formed for on object in the scene, has one known side (which is the baseline since we know how much apart are the two cameras from each other). As such, an easier triangulation with correct metric scale is possible without the need for additional hints.</p>
<h3 id="212-camera-based-slam-front-end">2.1.2 Camera-based SLAM Front-end</h3>
<p>Cameras provide the richest set of information among the available solutions for SLAM. The real challenge of using and dealing with camera is how to extract the necessary information out of them.</p>
<p>When you choose cameras for your SLAM problem, you will often end up with a general architecture. We depict this general architecture in Figure 6. It does not mean that all SLAM algorithms are exactly following this architecture, but gives a general ideea of what happens in the camera-based SLAM.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Camera-based SLAM general workflow." src="images/camera_based_SLAM.drawio.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 6: Camera-based SLAM architecture. Taken from the <code>robor_test_3</code> sequence in SMARTNav dataset.</em></td>
</tr>
</tbody>
</table>
<p>As emphasized previously, cameras spit out a lot of pixeled data. The first step is to <em>Extract Features</em>, ie. only keep the information that are necessary and useful. In Figure 6, you can see that some common ways of extracting features are depicted. For instance, extracting the edges in the picture, or extracting feature points (mostly the corner or sharp points in the environment that are easily detectable), or even lines are detected and extracted from image. These are popular feature extraction techniques because of two main reasons. I) It is relatively easy and quick to identify these features using classical computer vision II) It is possible to track these features accross different consecutive image frames. Among these intermediate representations, feature points are the most comonly used method.</p>
<p>After feature extraction, it is important to maintain a unique identity for the features across different frames. As an example, if you see the corners of the ceiling of the house, in Figure 6, as a unique feature in the environment, you should be able to detect the same spot at a few moments later and you should be able to understand that this corner is the same that you saw before. If you successfully do this, you have performed a correct <em>Feature Tracking</em>. Figure 6, also shows an example depiction of feature tracking, where the features are tracked across two consecutive image frames. The location of these features in the first and second frame is drawn by the red dots and the green lines have established a connection between the latest and former location of the features.</p>
<p>Although, the tracking of the features of environment is essential to the SLAM, providing a sense of relative motion with respect to the environment, it is not enough to realize a 3D measuremt of motion. The feature points should not be only tracked in the 2 dimensional space of the image, but also their 3D location in the space must be determined. So for each feature point, we can estimate a 3D location from the images, that is not trivial, but achievable through either monocular or stere-based camera configurations. In Figure 2, the uplifting of 2 dimensional features from image space into the 3D space is visualized such that a metric distance between the camera and the spatial position of that feature is at hand.</p>
<h3 id="214-sensor-requirements-what-to-buy-why">2.1.4 Sensor requirements (what to buy &amp; why)</h3>
<ul>
<li><strong>Global vs. Rolling shutter</strong> — reduce motion blur; support auto + manual control</li>
<li><strong>Dynamic range (DR)</strong> — avoid blown highlights/black crush in high-contrast scenes</li>
<li><strong>Frame rate (Rate)</strong> — ≥30 FPS typical; higher for fast robots or VIO</li>
<li><strong>Field of view (FOV)</strong> — wider = more features, better robustness to occlusion/fast motion</li>
<li><strong>Resolution</strong> — detail vs. compute/memory; common sweet spots (720p–1080p)</li>
<li><strong>Pixel size</strong> — low-light performance vs. resolution trade-off</li>
<li><strong>Optical size/format</strong> (e.g., 1/2.3", 1/1.8") — impacts lens choice and SNR</li>
<li><strong>Attachment type</strong> — C/CS-mount, M12; rigidity and repeatability of extrinsics</li>
<li><strong>Lens</strong> — focal length, distortion, aperture (f-number), focus control, IR cut</li>
<li><strong>Connection type</strong> — USB3, GMSL, MIPI, Ethernet; bandwidth, cable length, sync options</li>
</ul>
<h3 id="215-failure-modes-mitigations">2.1.5 Failure modes &amp; mitigations</h3>
<ul>
<li>Motion blur, low texture, repetitive patterns, lighting changes, rolling-shutter skew</li>
<li>Wider FOV, higher FPS, proper exposure, feature thresholds, motion models</li>
</ul>
<h3 id="216-available-solutions-examples">2.1.6 Available solutions (examples)</h3>
<ul>
<li>Consumer RGB-D, industrial global-shutter stereo, machine-vision USB3/GigE cameras, event cams</li>
<li>Notes on SDKs, ROS 2 drivers, and clock quality</li>
</ul>
<h3 id="217-custom-camera-stack-checklist">2.1.7 Custom camera stack checklist</h3>
<ul>
<li>Pick sensor + lens + mount; rigid bracket; trigger/sync wiring; EMI/shielding; heat &amp; dust</li>
</ul>
<h3 id="218-advantages-and-disadvantages-of-visual-slam">2.1.8 Advantages and disadvantages of visual SLAM</h3>
<p>There are many advantages to the cameras, that make them a suitable choice for SLAM in robotics. 
- They are rich. They see almost everything that a human doest.
- They are usually lightweight, that makes them perfect for robots which weight is a determinig factor.
- They are also energy efficient.
- Inclusion of the cammera has the minimum design burden compared to LiDAR, RADAR, Compass, GNSS, and even the IMU. Meaning that they can be put flexibly on any part of the robot without much constraint on your design.
- They can be used for other purposes. For instance you can put a camera for running your SLAM but also you can use the stream of image to perform another vision task, such as identifying a subject.</p>
<p>The cameras also come with some disadvantages:
- They are not able to directly understand the geometry of the scene. Understanding the geometry of the scene (how far things are from the camera) is necessary in SLAM and there are techniques to understand the distances but these techniques are not 100% reliable and have many limitations such as range and the textures in the environment.
- The volume of data that is to be handled by camera is relatively high.
- Due to the handling of the camera data, they often introduce delay to system, happening at the data transfer from sensor to the computer and copy pastings at Operating System (OS) level.
- The SLAM algorithms that use camera are usually heavier than the ones using other modalities. This further processing often happens during feature extraction and 3D understanding of the environment
- Normal RGB cameras dont see at darkness. </p>
<hr />
<h2 id="22-lidar-for-slam">2.2 LiDAR for SLAM</h2>
<h3 id="221-lidar-types">2.2.1 LiDAR types</h3>
<ul>
<li>2D vs. 3D; rotary/mechanical vs. solid-state; single-return vs. multi-echo</li>
<li>Scan pattern, vertical FOV, channel count, min range, beam divergence</li>
</ul>
<h3 id="222-requirements-parameters">2.2.2 Requirements &amp; parameters</h3>
<ul>
<li><strong>Range accuracy &amp; precision</strong></li>
<li><strong>Angular resolution &amp; scan rate</strong> (deskew needs)</li>
<li><strong>Reflectivity handling</strong> (dark/bright surfaces)</li>
<li><strong>Timestamping &amp; packet timing</strong> (essential for motion compensation)</li>
<li><strong>Weather/lighting robustness</strong> (rain/fog/sun)</li>
<li><strong>Connection &amp; power</strong> (Ethernet, PPS, NTP/gPTP support)</li>
</ul>
<h3 id="223-configuration-operation">2.2.3 Configuration &amp; operation</h3>
<ul>
<li>Motion compensation (deskew), voxel/downsample filters, ground removal</li>
<li>Intensity/reflectivity use, ring models, per-channel calibration</li>
</ul>
<h3 id="224-available-solutions-drivers">2.2.4 Available solutions &amp; drivers</h3>
<ul>
<li>Common 16/32/64-beam units, solid-state units; ROS 2 driver maturity &amp; logs</li>
</ul>
<h3 id="225-custom-mounting-checklist">2.2.5 Custom mounting checklist</h3>
<ul>
<li>Vibration isolation, occlusion cones, overlap with cameras, protective windows</li>
</ul>
<hr />
<h2 id="23-inertial-measurement-unit-imu">2.3 Inertial Measurement Unit (IMU)</h2>
<h3 id="231-imu-classes">2.3.1 IMU classes</h3>
<ul>
<li>Consumer MEMS vs. industrial; bias stability, noise density, bandwidth</li>
</ul>
<h3 id="232-requirements-parameters">2.3.2 Requirements &amp; parameters</h3>
<ul>
<li><strong>Bias stability &amp; Allan variance</strong></li>
<li><strong>Sampling rate &amp; latency</strong> (≥200 Hz typical for VIO/LIO)</li>
<li><strong>Time base</strong> (PPS-disciplined vs. local clock)</li>
<li><strong>Mounting orientation &amp; rigidity</strong></li>
</ul>
<h3 id="233-configuration">2.3.3 Configuration</h3>
<ul>
<li>IMU–camera/LiDAR extrinsics; gravity alignment; filtering; saturation checks</li>
</ul>
<hr />
<h2 id="24-fusion-considerations-when-to-combine-sensors">2.4 Fusion considerations (when to combine sensors)</h2>
<ul>
<li>Visual–Inertial (robustness in low texture/blur)</li>
<li>LiDAR–Inertial (geometry + motion stability)</li>
<li>Visual–LiDAR–Inertial (best-of-both, higher complexity)</li>
</ul>
<hr />
<h2 id="25-time-synchronization-timestamp-quality">2.5 Time synchronization &amp; timestamp quality</h2>
<ul>
<li>Hardware trigger, PPS, gPTP/NTP; inter-sensor latency budgets</li>
<li>Verifying sync in logs/bags; compensating residual offsets</li>
</ul>
<hr />
<h2 id="26-mounting-placement-cabling">2.6 Mounting, placement &amp; cabling</h2>
<ul>
<li>Rigid baselines (stereo), overlapping FOVs (cam–lidar), occlusion &amp; glare avoidance</li>
<li>Vibration isolation, thermal management, IP rating, cable strain relief/EMI</li>
</ul>
<hr />
<h2 id="27-compute-bandwidth-storage-planning">2.7 Compute, bandwidth &amp; storage planning</h2>
<ul>
<li>CPU/GPU needs per stack (VSLAM vs. LIO); RAM/VRAM sizing</li>
<li>Link bandwidth (USB3/Ethernet), codec choice, on-robot logging rates &amp; retention</li>
</ul>
<hr />
<h2 id="28-procurement-acceptance-tests">2.8 Procurement &amp; acceptance tests</h2>
<ul>
<li><strong>Bench tests:</strong> exposure sweep, blur test (pan), DR test (backlit scene)</li>
<li><strong>Stereo/RGB-D:</strong> rectification check, depth-scale sanity, plane-fit RMSE</li>
<li><strong>LiDAR:</strong> range plane-fit, ring timing, deskew validation</li>
<li><strong>IMU:</strong> short Allan variance, saturation &amp; clipping checks</li>
<li><strong>Sync:</strong> cross-sensor timestamp residuals</li>
</ul>
<hr />
<h2 id="29-available-solutions-vs-custom-solution">2.9 “Available solutions” vs. “Custom solution”</h2>
<ul>
<li>COTS kits (faster deployment, vendor support) vs. custom rigs (tailored FOV/baseline/sync)</li>
<li>Total cost of ownership: sensors + mounts + time sync + compute + spares</li>
</ul>
<hr />
<h2 id="210-practical-considerations-pitfalls-field-notes">2.10 Practical considerations &amp; pitfalls (field notes)</h2>
<ul>
<li>Sun/LED flicker; rolling-shutter on drones; lens focus drift; dirty optics</li>
<li>LiDAR multipath/rain/fog; vibration-induced extrinsic drift; thermal drift</li>
<li>Routine: lens cleaning, time-sync health, extrinsic re-verification schedule</li>
</ul>
<hr />
<h2 id="211-quick-decision-guide-by-scenario">2.11 Quick decision guide (by scenario)</h2>
<ul>
<li><strong>Indoor, texture-rich, low speed:</strong> stereo/RGB-D ± IMU</li>
<li><strong>Outdoor, structure-poor or high speed:</strong> 3D LiDAR + IMU; add camera for semantics</li>
<li><strong>Budget-constrained:</strong> mono + IMU (accept limits), or RGB-D at moderate speeds</li>
</ul>
<hr />
<h3 id="appendix-a-parameter-cheat-sheets">Appendix A — Parameter cheat-sheets</h3>
<ul>
<li><strong>Camera:</strong> shutter / DR / FPS / FOV / resolution / pixel size / lens / connection → symptom table</li>
<li><strong>LiDAR:</strong> channels / FOV / scan rate / voxel size / deskew → map quality table</li>
<li><strong>IMU:</strong> noise / bias / sample rate → drift &amp; observability table</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": [], "search": "assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>