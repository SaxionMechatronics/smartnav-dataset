
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="introduction.html">
      
      
        <link rel="next" href="calibration.html">
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Sensor Selection for SLAM - SMARTNav Dataset</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="css/custom.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-2-sensor-selection-for-slam" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="index.html" title="SMARTNav Dataset" class="md-header__button md-logo" aria-label="SMARTNav Dataset" data-md-component="logo">
      
  <img src="images/SMART-RG-White-transparent.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SMARTNav Dataset
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Sensor Selection for SLAM
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="index.html" title="SMARTNav Dataset" class="md-nav__button md-logo" aria-label="SMARTNav Dataset" data-md-component="logo">
      
  <img src="images/SMART-RG-White-transparent.png" alt="logo">

    </a>
    SMARTNav Dataset
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Practical Guide to SLAM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Practical Guide to SLAM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="introduction.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Sensor Selection for SLAM
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="sensors.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Sensor Selection for SLAM
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#objectives" class="md-nav__link">
    <span class="md-ellipsis">
      Objectives
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#role-of-sensors-in-slam-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Role of Sensors in SLAM Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#21-cameras-for-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Cameras for SLAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 Cameras for SLAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#211-image" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.1 Image
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-camera-modalities" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Camera modalities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#213-camera-configurations" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.3 Camera configurations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-camera-based-slam-front-end" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Camera-based SLAM Front-end
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#214-sensor-requirements-what-to-buy-why" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.4 Sensor requirements (what to buy &amp; why)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#216-available-solutions-examples" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.6 Available solutions (examples)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#218-advantages-and-disadvantages-of-visual-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.8 Advantages and disadvantages of visual SLAM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-lidar-for-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 LiDAR for SLAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 LiDAR for SLAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-lidar-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.1 LiDAR Mechanism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-pointcloud" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.2 Pointcloud
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quizes" class="md-nav__link">
    <span class="md-ellipsis">
      Quizes
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="calibration.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Calibtation of Sensors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="deployment.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deployment of SLAM Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="interpretation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Interpretation of SLAM Results
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#objectives" class="md-nav__link">
    <span class="md-ellipsis">
      Objectives
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#role-of-sensors-in-slam-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Role of Sensors in SLAM Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#21-cameras-for-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Cameras for SLAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 Cameras for SLAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#211-image" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.1 Image
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-camera-modalities" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Camera modalities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#213-camera-configurations" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.3 Camera configurations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-camera-based-slam-front-end" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Camera-based SLAM Front-end
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#214-sensor-requirements-what-to-buy-why" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.4 Sensor requirements (what to buy &amp; why)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#216-available-solutions-examples" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.6 Available solutions (examples)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#218-advantages-and-disadvantages-of-visual-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.8 Advantages and disadvantages of visual SLAM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-lidar-for-slam" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 LiDAR for SLAM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 LiDAR for SLAM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-lidar-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.1 LiDAR Mechanism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-pointcloud" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.2 Pointcloud
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quizes" class="md-nav__link">
    <span class="md-ellipsis">
      Quizes
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<style>
.quiz{border:1px solid #e5e7eb;border-radius:12px;padding:12px;margin:16px 0;background:#fafafa}
.quiz h4{margin:0 0 8px 0}
.quiz .choices{display:grid;gap:8px;margin:8px 0}
.quiz button.choice{all:unset;display:block;padding:10px 12px;border:1px solid #d1d5db;border-radius:10px;cursor:pointer;background:#fff}
.quiz button.choice:hover{background:#f3f4f6}
.quiz button.choice.correct{border-color:#16a34a;background:#ecfdf5}
.quiz button.choice.wrong{border-color:#dc2626;background:#fef2f2}
.quiz .feedback{display:none;margin-top:8px;font-size:.95em}
.quiz.answered .feedback{display:block}
.quiz .explain{color:#4b5563}
</style>
<script>
document.addEventListener('DOMContentLoaded', () => {
  document.querySelectorAll('.quiz').forEach(q => {
    const choices = q.querySelectorAll('button.choice');
    choices.forEach(btn => btn.addEventListener('click', () => {
      if (q.classList.contains('answered')) return; // one try
      // mark selection
      const correct = btn.dataset.correct === 'true';
      btn.classList.add(correct ? 'correct' : 'wrong');
      // show the correct one if user was wrong
      if (!correct) q.querySelector('button.choice[data-correct="true"]')?.classList.add('correct');
      // lock and reveal feedback
      q.classList.add('answered');
      choices.forEach(b => b.disabled = true);
      // optional: custom feedback text
      const fb = q.querySelector('.feedback');
      if (fb && !fb.dataset.init) {
        fb.innerHTML = correct ? (fb.dataset.ok || "✅ Correct!") : (fb.dataset.no || "❌ Not quite.");
        fb.dataset.init = '1';
      }
    }));
  });
});
</script>

<h1 id="chapter-2-sensor-selection-for-slam">Chapter 2: Sensor Selection for SLAM</h1>
<h2 id="objectives">Objectives</h2>
<p>In this chapter, you will meet the most common sensor types used in SLAM and see where each one shines or struggles. By the end, you are exected to:</p>
<ul>
<li>What the common sensors are and what information do they offer.</li>
<li>Pick a sensible sensor (or a combination) for your own application.</li>
<li>Weigh the usual trade-offs: accuracy vs. cost, compute load, environment, and how mature the open-source algorithms for your application are.</li>
<li>Spot the key specifications that matter when buying or configuring sensors for SLAM.</li>
</ul>
<h2 id="role-of-sensors-in-slam-architecture">Role of Sensors in SLAM Architecture</h2>
<p>SLAM is usually split into two parts: a <em>front-end</em> and a <em>back-end</em>.</p>
<p><strong>Front-end</strong>: It is the part of SLAM that turns raw sensor measurements into compact, useful cues about motion and the scene.
If we use a camera for a SLAM algorithm, we will have <em>visual</em> SLAM.
Assuming that the camera gives us images of $640 \times 480$, we will end up with 307200 pixels. This is an enormous amount of information that is very difficult to comprehend and use. The front-end reduces this load by extracting just the most informative bits, so the rest of the system has something manageable to work with.</p>
<p><strong>Back-end</strong>: It is the core algorithm, that estimates the robot’s pose and builds the map from those cues.</p>
<p>Your sensor choice largely shapes the front-end: what you can extract, how robust it is, and how much computation it needs. In short, the sensor defines the front-end’s job, and through that, it influences the back-end’s workload and accuracy.</p>
<h2 id="21-cameras-for-slam">2.1 Cameras for SLAM</h2>
<h3 id="211-image">2.1.1 Image</h3>
<p>If you’re new to computer vision, here’s the core idea: an image is a matrix of numbers.  Each cell is a pixel, a tiny dot with a color  that defines how a small part of the scene looks like. It should be clear that by increasing the number of pixels, more detail we be captured about the scene.</p>
<!--
| ![Pixels concept](images/image_pixels.png) |
|:--:|
|*Figure 1: Pixels, the building blocks of an image.*|
-->

<h3 id="212-camera-modalities">2.1.2 Camera modalities</h3>
<p>The term modality is referring to what kind of light does the camera measure and how it measures it. Most of the cameras capture the <em>visible light</em>, which is a small part of electromagnetic spectrum. If Figure 1, we display this spectrum and the visible light is highlited by rainbow colors. </p>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/light_spectrum.drawio.png" alt="1" style="width:80%">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 1: Where common camera modalities sit on the electromagnetic spectrum.</em>
  </div>
</div>

<!-- | ![Spectrum](images/light_spectrum.drawio.png) |
|:--:|
|*Figure 1: Where common camera modalities sit on the electromagnetic spectrum.*| -->

<p><strong>Color cameras</strong>: These cameras capture a scene in a similar way to how we see it, in different shades of colors. For each pixel of an image, there are 3 values for red, green, blue (RGB) that their combination will create a uniqe color. But the camera sensor does not 3 values (RGB) at each pixel of the physical sensor directly.</p>
<p>To capture color images, a <em>Color Filter Array (CFA)</em> is formed, such that on each pixel either an R, G, or B filter is used. An R filter for instance, only allows the red light frequency to pass. For each pixel, the other two values will be interpolated from the neighboring pixels. This process is also known as <em>Demosaicing</em>. Figure 2 diplays the mechanism in which the color images are formed in the sensor.</p>
<!-- 
In the spectrum of the Figure 2, the wavelengths between  380-700 nm is covered by color cameras. However, the way that a color camera can capture the actual color is through a specific mechanism. In fact, a color camera is able to measure 3 values for each pixel, i) the intensity of the blue ii) green and iii) red colors. The sensitivity of the pixel to each of these colors, follows the distribution presented in Figure 2. For instance, a color corresponding to the frequency 400 nm, will mostly excite a blue color and less green and much less red.
-->

<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/filter_array.drawio.png" alt="1" style="width:80%">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 2: Monochrome vs. color sensing. Color sensors use a mosaic of R/G/B filters while monochrome does not.</em>
  </div>
</div>

<!-- | ![CFA](images/filter_array.drawio.png) |
|:--:|
|*Figure 2: Monochrome vs. color sensing. Color sensors use a mosaic of R/G/B filters while monochrome does not.*| -->

<p><strong>Monochrom</strong>: Unlike color cameras, there is no filter arrays needed for different pixels. In monochrom cameras, all the pixels are used for capturing some portions of the scene, thus the full potential of the physical pixels of the camera is used to capture the scene details, and no approximation through interpolation is usually needed. This is further depicted in Figure 3 by comparing these two sensing modalities.</p>
<p>Another consequence of the color filters is that, they narrow down spectrum of the incoming rays of light, only passing a rather narrow band (related to the R, G, or B color). In Figure2, we have tried to show the range of wavelengths that are covered by R filter, G filter, B filter, and finally no filter (monochrome). These are approximation curves and they slightly differ for each real sensor. These curves are usually provided by the manufacturer. As is evident, a monochrome sensor (gray curve), covers a wider range of spectrums compared to the combination of R,G,B filters together. Specefically, it covers more on a portion of the infra-red band, also known as near-infrared. As a result, each pixel will receive less share of light rays (photons so to say); however, a monochrom camera due to lack of the filters can receive more light rays and is more illuminated compared to RGB cameras, improving performance at the darkness.</p>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/cameras_images.png" alt="1" style="width:80%">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 4: Comparison between images taken from different ranges of electromagnetic spectrum. Image taken from <a href="https://doi.org/10.48550/arXiv.2108.11767">https://doi.org/10.48550/arXiv.2108.11767</a>.</em>
  </div>
</div>

<!-- | ![Cameras images](images/cameras_images.png) |
|:--:|
|*Figure 4: Comparison between images taken from different ranges of electromagnetic spectrum. Image taken from [https://doi.org/10.48550/arXiv.2108.11767](https://doi.org/10.48550/arXiv.2108.11767)*| -->

<p>To make the discussion more sensible, Figure 4, brings a comparison between images taken from different electromagnetic bands. For instance, the RGB camera captures nice representation of the lit parts of the scene, also providing more semantics through the color information. However, the same scene captured using a monochrom camera that also captures some light in near-infrared (NIR) band can show more of the shadowed section of the scene, while providing a good geometrical understanding of the lit portion, yet missing the color details. The more the camera moves towards higher wavelengths (more towards middle-infrared and far-infrared) the more it turns into a thermal camera, seeing better at darkness and sensing the heat, yet the more specialized the sensors become that comes at a higher cost.</p>
<h3 id="213-camera-configurations">2.1.3 Camera configurations</h3>
<p><strong>Monocular camera</strong>: It captures image data through a single lens. The benefit is that it is the most compact and efficient sensor setup in robotics, and SLAM systems. The drawback is that, this makes it more complicated to understand the geometry of the scene. </p>
<p>Intuitively, it is not possible to understand the other dimensions of the real-world objects (also known as depth estimation), unless you observe them from at least two different points of view. This 3D understanding about the surrounding objects is central to the SLAM methodologies. Using only a single camera seems to prevent us from understanding the distances of the scene components from us; however, there are a few tricks that will enable depth estimation even using monocular cameras.</p>
<p>The traditional approach is to move the camera around and use the observation of the same object in multiple frames at different times. Although the depth estimation will not be at hand at every frame, but after some time, assuming sufficient spatial translations, we should be to look at the same targets from different angles. This allows us to do simple geometrical analysis also known as <em>Triangulation</em> to measure objects' distances from the camera.</p>
<p>There is a more recent approach to solve the monocular depth estimation problem, which is taking advantage of deep neural networks. These networks are trained using massive datasets, learning how to estimate the relative distances of object from camera only using a single image. This depth estimation based on experience is kind of similar to the way a human can infer depth if he covers one of his eyes, such that he still can understand distances but mostly because of experience. </p>
<p><strong>Stereo camera</strong>: If two monocular cameras are attached together, such that they share most of their view, it is a stereo camera setup. This is very similar to human way of observing the environment. In this setup, at every timestamp, the calculation of the scene depth is possible, since we already have two different views of the object. Simlar triangulation techniques, or more complicated methods are possible to be used here. The accuracy of the output depth estimation relies on many factors such as the algorithm. The maximum range of depth estimation depend on how separated the two cameras are from each other (also called the stereo baseline). </p>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/stereo_vs_mono.drawio.png" alt="1" style="width:80%">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 1: The comparison between monocular and stereo cameras in terms of 3D scene understanding.</em>
  </div>
</div>

<!-- | ![Cameras images](images/stereo_vs_mono.drawio.png) |
|:--:|
|*Figure 5: The comparison between monocular and stereo cameras in terms of 3D scene understanding.*| -->

<p>In the Figure 5, examples of monocular and stereo cameras are highlighted. As evident, for a monocular camera, only by having dicplacements with respect to an object in the scene (the star shape represents an object), it is possible to create a virtual stereo camera by putting together the pairs of images from different times (given that they are far apart sufficiently). Then, a triangle can be assumed, however, all three side of the triangle are unknown, yet using two consecutive computer vision methods, namely the <em>Five Points</em> method, then <em>Perspective n Points</em>, a value for these sides will be at hand which is proportionally correct, however, does not have the right metric scale. The details for these algorithms is for interested readers. At the end, it should be emphasized that the monocular camera can not yield the right metric values for the depth of the features and only can deliver values for distances of objects that are correct relative to each other (correct proportions). If we want to convert them to correct metric values, another hint (another sensor such as IMU) should be incorporated.</p>
<p>By comparison, the stereo camera in Figure 5, forms a similar triangle, however in one time instance and not across two different time instances. Moreover, the triangle that is formed for on object in the scene, has one known side (which is the baseline since we know how much apart are the two cameras from each other). As such, an easier triangulation with correct metric scale is possible without the need for additional hints.</p>
<h3 id="212-camera-based-slam-front-end">2.1.2 Camera-based SLAM Front-end</h3>
<p>Cameras provide the richest set of information among the available solutions for SLAM. The real challenge of using and dealing with camera is how to extract the necessary information out of them.</p>
<p>When you choose cameras for your SLAM problem, you will often end up with a general architecture. We depict this general architecture in Figure 6. It does not mean that all SLAM algorithms are exactly following this architecture, but gives a general ideea of what happens in the camera-based SLAM.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Camera-based SLAM general workflow." src="images/camera_based_SLAM.drawio.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 6: Camera-based SLAM architecture. Taken from the <code>robor_test_3</code> sequence in SMARTNav dataset.</em></td>
</tr>
</tbody>
</table>
<p>As emphasized previously, cameras spit out a lot of pixeled data. The first step is to <em>Extract Features</em>, ie. only keep the information that are necessary and useful. In Figure 6, you can see that some common ways of extracting features are depicted. For instance, extracting the edges in the picture, or extracting feature points (mostly the corner or sharp points in the environment that are easily detectable), or even lines are detected and extracted from image. These are popular feature extraction techniques because of two main reasons. I) It is relatively easy and quick to identify these features using classical computer vision II) It is possible to track these features accross different consecutive image frames. Among these intermediate representations, feature points are the most comonly used method.</p>
<p>After feature extraction, it is important to maintain a unique identity for the features across different frames. As an example, if you see the corners of the ceiling of the house, in Figure 6, as a unique feature in the environment, you should be able to detect the same spot at a few moments later and you should be able to understand that this corner is the same that you saw before. If you successfully do this, you have performed a correct <em>Feature Tracking</em>. Figure 6, also shows an example depiction of feature tracking, where the features are tracked across two consecutive image frames. The location of these features in the first and second frame is drawn by the red dots and the green lines have established a connection between the latest and former location of the features.</p>
<p>Although, the tracking of the features of environment is essential to the SLAM, providing a sense of relative motion with respect to the environment, it is not enough to realize a 3D measuremt of motion. The feature points should not be only tracked in the 2 dimensional space of the image, but also their 3D location in the space must be determined. So for each feature point, we can estimate a 3D location from the images, that is not trivial, but achievable through either monocular or stere-based camera configurations. In Figure 2, the uplifting of 2 dimensional features from image space into the 3D space is visualized such that a metric distance between the camera and the spatial position of that feature is at hand.</p>
<h3 id="214-sensor-requirements-what-to-buy-why">2.1.4 Sensor requirements (what to buy &amp; why)</h3>
<p>The camera to be used in SLAM should have specific characteristics, some of which are listed in the table below. Note that some of more detailed specs are useful when a custom camera sensor and lens is being selected, while many cameras used in robotics, have a good trade-off between these specs.</p>
<ul>
<li><strong>Field of View (FOV)</strong>:
For SLAM methods, a high FOV, is usually desireable, to see a higher number of features at the same time in the environment, and to be able to pick best features to track. To avoid being blinded or obstructed. It is worth noting that very large field of view (above 120 degrees) will be a bit challenging for calibration (one step before deploying your SLAM) such that the available open-source calibration tool will have difficulties yielding accurate calibration results.</li>
</ul>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/ftsfisheye-vs-conventional_r.jpg" alt="1" style="width:45%">
  <img src="images/ftsfisheye-vs-conventional.jpg" alt="1" style="width:45%">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 7: Visualization of the difference between narrow (left image) and wide (right image) FOV, and its effect on feature tracking.</em>
  </div>
</div>

<!-- | ![Small FOV](images/ftsfisheye-vs-conventional_r.jpg) | ![Large FOV](images/ftsfisheye-vs-conventional.jpg) |
|---|---|
|*Figure 7: Visualization of the difference between narrow (left image) and wide (right image) FOV, and its effect on feature tracking.*| -->

<p>Figure 7, compares the effect of having a wider FOV. The red dots are the spots in the image that usually considered as feature points in a the SLAM front-end, and they are extracted using a common feature detection algorithm used in many SLAM methods. In the wider FOV image, it is usually easier to have higher quality features (unique corner points for instance), while in the low field of view, the algorithms has to rely on lower quality features (like the many points insie the trees that are less unique) to maintain a required number of feature points. The other difference is that while having the wide FOV, in order to lose sight of all these features at once, you should have a larger sudden motion from the camera. The drawback of the wide-FOV lenses usually is in the form of the distortion of the image. The straight lines, and planes are curved, speceially close to the borders of the image, and the higher the FOV, the more spherical pattern is induced on the image. These distrotions have to be rectified in the calibration stage.</p>
<ul>
<li>
<p><strong>Resolution</strong>:
Low resolution can prevent visual SLAM from proper tracking of features. High resolution will introduce unnecessary processing loads to the system, specially keeping the buses (USB for instance) busy, making it difficult to record the data, and adding more delay in position tracking and map updating due to time taken for tracking feature in images. </p>
</li>
<li>
<p><strong>Shutter</strong>:
Global shutter sensors are ideal for visual SLAM to minimize motion blur. To eliminate rolling shutter effect, that skews moving objects in the image and give unrealistic depiction of scenes in case of motion. Interesting to note that rolling shutter is in contrast with image resolution due to limitation in the technology behind rolling shutter, such that if you want a high-resolution rolling shutter sensor, the price increases. However, most of the cheap rolling shutter camera have an adequate resolution for our application.</p>
</li>
</ul>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/img_gs.png" alt="1" style="width:30%">
  <img src="images/rs_fast_midRS_blur_vectors.png" alt="1" style="width:30%">
  <img src="images/rs_fast_midRS_blur.png" alt="1" style="width:30%">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 8: Comparison between the image taken from a global shutter camera (left) and the one taken from rolling shutter (right) due to the motion pattern in scene (middle). The image is taken from the <code>corridor_ground_1</code> sequence in SMARTNav dataset.</em>
  </div>
</div>

<!-- | ![Global shutter](images/img_gs.png) | ![Motion](images/rs_fast_midRS_blur_vectors.png) | ![Rolling](images/rs_fast_midRS_blur.png) |
|---|---|---|
|*Figure 8: Comparison between the image taken from a global shutter camera (left) and the one taken from rolling shutter (right) due to the motion pattern in scene (middle). The image is taken from the `corridor_ground_1` sequence in SMARTNav dataset.*| -->

<p>The difference between the image taken from a roller shutter and one taken from global, is illustrated in Figure 8 (the effect is simulated on the rolling shutter image). As evident, the distortion cause by motion, make the rolling shutter image to yield an undrealistic representation of the scene (all the vertical line are bent closer to the edges) along with more blurring on high-speed parts in the image. The amount of rolling shutter effect is dependent on the magnitude of speed. Meaning that in applications that the robot is moving slow or high magnitude and high frequency vibrations are not expected, this effect might be negiligle, however, for more dynamic use cases, a global shutter sensor is advised.</p>
<ul>
<li>
<p><strong>Data Rate</strong>:
A higher streaming rate of the camera can help in fast motion so that the feature tracks dont get lost. Here, by high we mean higher that the norm in most of the cameras which is usually 30 Frames per Second (FPS); while having an excessively high FPS (&gt; 90 FPS) might be unnecessary due to the fact that most of the SLAM will not be able to handle this high frequency stream of data and they have to drop some of the frames to keep up.</p>
</li>
<li>
<p><strong>Dynamic Range</strong>:
Dynamic range indicates the ability of the camera to capture details in dark or bright areas of an image. It is defined as the difference between brightest light and darkest dark in the image. Any light above this range will be diplayed as white and anything below it is pitch dark. This difference is also visible in Figure 9.</p>
</li>
</ul>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/low_DR.png" alt="1" style="width:45%">
  <img src="images/high_DR.png" alt="1" style="width:45%">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 9: Difference beween low dynamic range (left image) and high dynamic range (right image) cameras.</em>
  </div>
</div>

<!-- | ![Low DR](images/low_DR.png) | ![Hight DR](images/high_DR.png) |
|---|---|
|*Figure 9: Difference beween low dynamic range (left image) and high dynamic range (right image) cameras.*| -->

<ul>
<li><strong>Bit-rate</strong>:
The normal image data that is given by cameras is an array on 8-bit pixels. This means that each pixel value can be reported by a combination of 8 zeros or one. As a result, each pixel might have $2^8 = 256$ values. In simpler terms, a normal 8-bit camera can capture colors in 256 different shades.
If we can capture higher bit-rates such as 10, the number shades we can capture will be immediately increased to 1024, which translate to far more details in cases that the lighting of the environment is uniform (dark for instance) or the textures have very close colors.</li>
</ul>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/8-bit_image.png" alt="1" style="width:45%">
  <img src="images/10-bit_image.png" alt="1" style="width:45%">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 10: An example of an 8-image in a low light environment is presented on the left. On the right, example of a 10-bit image from the same environment. The image taken as part of the <a href="https://www.saxion.nl/onderzoek/smart-industry/smart-mechatronics-and-robotics/firebot">firefighting robot</a> within the SMART group.</em>
  </div>
</div>

<!-- | ![Low bit rate](images/8-bit_image.png) | ![Hight bit rate](images/10-bit_image.png) |
|---|---|
|*Figure 10: An example of an 8-image in a low light environment.*|*The example of a 10-bit image from the same environment. The image taken as part of the [firefighting robot](https://www.saxion.nl/onderzoek/smart-industry/smart-mechatronics-and-robotics/firebot) within the SMART group.*| -->

<p>The above figure shows on the left side an image that was captured and converted to 8-bit format (256 shades). For this image a hostogram of different pixel values is drawn as well. As visible in the histogram, most of the pixel color are concentrated in one small region, meaning that all colors in the image are close to each other and having a general dark vibe. On the right side, all 10-bit (1024 shade of color) where used. The important thing to note is that our displays and most of computer vision functions are not able to process the higher bit-rates that 8, meaning that an algorithm should be used to convert this higher bit-rate to 8, while maintaing and intensifying the details. The exact details about such an algorithm is beyond this guide, but as a comparison, you can see the result of a better use of extra shades of color and a correct conversion in the right image of Figure 10.</p>
<ul>
<li><strong>Lens</strong>:
This item mostly depends on how custom you want your sensor setup to be. If you want to be able to keep your core sensor (resolution, shutter, rate, and connection remain the same) but you want to be able to change or experiment with different FOVs, you can go for a sensor that allows you to change the lens. This information can be found in the datasheets of the sensor, for instance an m-12 lens mount is an ideal option in the market, while those that are mentioned to have a stock lens are not changeable.</li>
</ul>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/stock_lens.png" alt="1" style="width:30%; height:250px ">
  <img src="images/m12_mount.png" alt="1" style="width:30%; height:250px ">
  <img src="images/lenses.png" alt="1" style="width:30%; height:250px ">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 11: A camera with an unchangable stock lens (left), one with an M12 mount (center), and different options for lens yielding different FOVs and sometimes difference frequency band cuts and IR filters (right).</em>
  </div>
</div>

<!-- | ![Stock Lens](images/stock_lens.png) | ![M12 Lens](images/m12_mount.png) | ![Different Lenses](images/lenses.png) |
|---|---|---|
|*Figure 11: A camera with an unchangable stock lens (left), one with an M12 mount (center), and different options for lens yielding different FOVs and sometimes difference frequency band cuts and IR filters (right).*| -->

<ul>
<li><strong>Connection</strong>:
Flat cable connection to the onboard computer (Jetson) help with faster data transfer and reduces undesirable delays and bandwidth limitations of the USB port. However, they are ideal when the camera is close to the processor board, otherwise, these cable do not have the needed flexibility and USB3 interface will be more ideal as a balance between data transfer speed and design flexibility. USB2 camera are not very desireable for SLAM and generally embedded vision applications. When making the decision about going for a USB3 camera or a camera with flat cables, other than the mechanical design aspects, the compatibility with the processing unit (whether the embedded computer has relevant port) should be taken into account.</li>
</ul>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/USB_cam.jpg" alt="1" style="width:30%; height:250px ">
  <img src="images/flat_cable.jpg" alt="1" style="width:30%; height:250px ">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 12: Example of a camera having a USB connection (left) and a camera with flat cable connection (right).</em>
  </div>
</div>

<!-- | ![USB Cam](images/USB_cam.jpg) | ![Flat Cable](images/flat_cable.jpg) |
|---|---|
|*Figure 12: Example of a camera having a USB connection (left) and a camera with flat cable connection (right).*| -->

<ul>
<li>
<p><strong>Pixel size</strong>:
The higher the pixel size, the better low light performance.</p>
</li>
<li>
<p><strong>Optical size/format</strong>:
In case you go with an adjustable lens option for the sensor, you should make sure the the optical format provided in lens and sensor are compatible. This size determines the effective area of sensor that captures the scene. If it is smaller than that of the lens, a big part of the scene taken by lens will go out of image boundaries. If it is larger than that of lens, a big dark area will appear around the captures scene in the final image.</p>
</li>
<li>
<p><strong>Driver</strong>:
When buying a camera, if the vendor provides you with a driver that is compatible with embedded computers (usually having aarch64/arm64 processor architecture as opposed to amd64 which is common in the desktops) and if they have a ROS2 wrapper for their sensor, it is going to make life much easier and it will be mostly plug and play. </p>
</li>
</ul>
<h3 id="216-available-solutions-examples">2.1.6 Available solutions (examples)</h3>
<p>In the table below, we bring a few example of some commonly available cameras in the market, and compare them base on the discussed criteria.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Camera</th>
<th style="text-align: left;">Modality</th>
<th style="text-align: left;">Horizontal FOV</th>
<th style="text-align: left;">Max resolution</th>
<th style="text-align: left;">Shutter</th>
<th style="text-align: left;">Max FPS</th>
<th style="text-align: left;">Dynamic Range (dB)</th>
<th style="text-align: left;">Max bit rate</th>
<th style="text-align: left;">Lens</th>
<th style="text-align: left;">Connection</th>
<th style="text-align: left;">Pixel size ($µm^2$)</th>
<th style="text-align: left;">Driver</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Realsense D455</td>
<td style="text-align: left;"><font color="green">monochrome</font></td>
<td style="text-align: left;"><font color="red">87°</font></td>
<td style="text-align: left;"><font color="green">1280 × 800</font></td>
<td style="text-align: left;"><font color="green">global</font></td>
<td style="text-align: left;"><font color="red">30</font></td>
<td style="text-align: left;"><font color="red">-</font></td>
<td style="text-align: left;"><font color="green">10</font></td>
<td style="text-align: left;"><font color="red">fixed</font></td>
<td style="text-align: left;"><font color="green">USB 3.1</font></td>
<td style="text-align: left;"><font color="green">3 x 3</font></td>
<td style="text-align: left;"><font color="green">Embedded Linux and ROS2 support</font></td>
</tr>
<tr>
<td style="text-align: left;">ZED 2</td>
<td style="text-align: left;"><font color="red">color</font></td>
<td style="text-align: left;"><font color="green">110°</font></td>
<td style="text-align: left;"><font color="red">2208 x 1242</font></td>
<td style="text-align: left;"><font color="red">rolling</font></td>
<td style="text-align: left;"><font color="green">100</font></td>
<td style="text-align: left;"><font color="red">64</font></td>
<td style="text-align: left;"><font color="red">8</font></td>
<td style="text-align: left;"><font color="red">fixed</font></td>
<td style="text-align: left;"><font color="green">USB 3.0</font></td>
<td style="text-align: left;"><font color="red">2 x 2</font></td>
<td style="text-align: left;"><font color="green">Embedded Linux and ROS2 support</font></td>
</tr>
<tr>
<td style="text-align: left;">ZED X</td>
<td style="text-align: left;"><font color="red">color</font></td>
<td style="text-align: left;"><font color="green">110°</font></td>
<td style="text-align: left;"><font color="red">1920 x 1200</font></td>
<td style="text-align: left;"><font color="green">global</font></td>
<td style="text-align: left;"><font color="green">100</font></td>
<td style="text-align: left;"><font color="green">71</font></td>
<td style="text-align: left;"><font color="red">8</font></td>
<td style="text-align: left;"><font color="red">fixed</font></td>
<td style="text-align: left;"><font color="red">GMSL2</font></td>
<td style="text-align: left;"><font color="green">3 x 3</font></td>
<td style="text-align: left;"><font color="green">Embedded Linux and ROS2 support</font></td>
</tr>
<tr>
<td style="text-align: left;">Raspberry Pi Camera Module 3 Wide</td>
<td style="text-align: left;"><font color="red">color</font></td>
<td style="text-align: left;"><font color="green">102°</font></td>
<td style="text-align: left;"><font color="red">4608 × 2592</font></td>
<td style="text-align: left;"><font color="red">rolling</font></td>
<td style="text-align: left;"><font color="green">60</font></td>
<td style="text-align: left;"><font color="green">up to 96 in HDR mode</font></td>
<td style="text-align: left;"><font color="green">10</font></td>
<td style="text-align: left;"><font color="red">stock lens</font></td>
<td style="text-align: left;"><font color="green">Flat cable</font></td>
<td style="text-align: left;"><font color="red">1.4 × 1.4</font></td>
<td style="text-align: left;"><font color="green">Embedded Linux support</font></td>
</tr>
<tr>
<td style="text-align: left;">Arducam multi-cam board with OV9282 base sensors</td>
<td style="text-align: left;"><font color="red">color</font></td>
<td style="text-align: left;"><font color="green">adjustable</font></td>
<td style="text-align: left;"><font color="green">1280 × 720</font></td>
<td style="text-align: left;"><font color="green">global</font></td>
<td style="text-align: left;"><font color="green">150</font></td>
<td style="text-align: left;"><font color="red">68</font></td>
<td style="text-align: left;"><font color="green">10</font></td>
<td style="text-align: left;"><font color="green">M12 mount</font></td>
<td style="text-align: left;"><font color="green">Flat cable</font></td>
<td style="text-align: left;"><font color="green">3 × 3</font></td>
<td style="text-align: left;"><font color="green">Embedded Linux support</font></td>
</tr>
</tbody>
</table>
<p>We assigned characteristics of the mentioned sensors in either green or red, a harsh thresholding that of course can vary based on specific applications. For instance, if you prefer having a colored 3D map of the environment, monochrome sensors are probabely not what you are looking for. In this version of the table, we preferred low light performance, thus the monochrome is marked as green and colored sensors are red.</p>
<p>Another improtant thing to note is that the maximum of some of these specifications does not happen at the same time. For instance, the max resolution usually leads to a lower FPS than the max FPS reported in the table. However, when we set some of the maximum resolutions to red, it means that for a typical SLAM application with most of the available open-source method, such high resolutions are an overkill and can needlessly increase the sensor price. </p>
<p>There are other consideration related to these cameras at the marker, such as out of the box softwares that can give you additional functionalities. For instance the ZED cameras and Intel Realsense, usually have yield an out of the box depth estimation which can be useful in some SLAMs. However, in some cases such as the ZED cameras, it adds another requirement which is GPU-equipped embedded computers (such as Nvidia Jetson family).</p>
<p>In a nutshell, for a beginner-level introduction to the world of SLAM or generally computer vision, ready-to-use options such as ZED cameras or Realsense D455 is a good choice.
For having more control over the sensor and customizing to your robot, while having maximum control over the suitability of the sensor, custom sensor boards such the Arducam is an attractive option.</p>
<h3 id="218-advantages-and-disadvantages-of-visual-slam">2.1.8 Advantages and disadvantages of visual SLAM</h3>
<p>There are many advantages to the cameras, that make them a suitable choice for SLAM in robotics. 
- They are rich. They see almost everything that a human doest.
- They are usually lightweight, that makes them perfect for robots which weight is a determinig factor.
- They are also energy efficient.
- Inclusion of the cammera has the minimum design burden compared to LiDAR, RADAR, Compass, GNSS, and even the IMU. Meaning that they can be put flexibly on any part of the robot without much constraint on your design.
- They can be used for other purposes. For instance you can put a camera for running your SLAM but also you can use the stream of image to perform another vision task, such as identifying a subject.</p>
<p>The cameras also come with some disadvantages:
- They are not able to directly understand the geometry of the scene. Understanding the geometry of the scene (how far things are from the camera) is necessary in SLAM and there are techniques to understand the distances but these techniques are not 100% reliable and have many limitations such as range and the textures in the environment.
- The volume of data that is to be handled by camera is relatively high.
- Due to the handling of the camera data, they often introduce delay to system, happening at the data transfer from sensor to the computer and copy pastings at Operating System (OS) level.
- The SLAM algorithms that use camera are usually heavier than the ones using other modalities. This further processing often happens during feature extraction and 3D understanding of the environment
- Normal RGB cameras dont see at darkness. 
- There are some challenges related to SLAM algorithms such as the motion blur in very dynamic cases. Also in environments with no texture (flat uniform surface or reflective surfaces), the visual SLAM perfrom poorly. Although we introduced some paramters in this section to have a camera with good performance in challenging lighting conditions (low light for instance), however, there are limits to what camera can clearly see.</p>
<hr />
<h2 id="22-lidar-for-slam">2.2 LiDAR for SLAM</h2>
<p>LiDAR stands for Light Detection and Ranging, a technology that is less broad and generic compared to cameras, and is often more suitable and specialized for robotic applications.</p>
<h3 id="221-lidar-mechanism">2.2.1 LiDAR Mechanism</h3>
<p>LiDAR operates based on the principle of sending out a laser beam at a very specific and narrow direction in space, and also waits for the reflection of that beam, then, based on the traveling time of the beam (also known as Time of Flight or ToF), having the speed of light in mind, it measures the distance of the nearest object in that direction of the space.</p>
<p>This ToF-based range measurement usually occurs multiple times and for multiple directions in space, giving an estimation of the environment geomitrical shape.</p>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/lidar_mech1.gif" alt="1" style="width:30%; height:250px ">
  <img src="images/lidar_mech2.gif" alt="1" style="width:30%; height:250px ">
  <img src="images/lidar_mech3.gif" alt="1" style="width:30%; height:250px ">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 13: The concept of LiDAR mechanism taken <a href="https://en.wikipedia.org/wiki/Lidar">LiDAR Wikipedia</a>. On the left, a simplified design for LiDAR sensor is displayed, including the laser source and the rotating mirror that reflects the beam in different spatial directions. In the middle figure, the top view of the space around the sensor is shown in form of rectangular room with the LiDAR (blue rectangle) and another round object (green circle). After capturing the time of flight for the beam in each direction and measuring the distance, a 2 dimensional map can be formed as displayed in the right image.</em>
  </div>
</div>

<h3 id="222-pointcloud">2.2.2 Pointcloud</h3>
<p>Just like the cameras that output an image as their data format, LiDARs produce pointclouds. A pointcloud is a set of 3D points in space. These points have $x, y, z$ coordinates. These are points in the coordinate frame of the sensor, meaning that they are relative to the sensor itself and still do not have any spatial meaning relative to a world-related reference frame. In fact, making sure that these points are coordinates in a world related reference frame is one of the tasks that our SLAM will do.</p>
<div style="display:flex; flex-wrap:wrap; gap:8px; justify-content:center; align-items:flex-start">
  <img src="images/lidar_img.png" alt="1" style="width:45%; height:250px ">
  <img src="images/lidar_top.png" alt="1" style="width:45%; height:250px ">
  <img src="images/lidar_camera.png" alt="1" style="width:45%; height:250px ">
  <img src="images/lidar_distance.png" alt="1" style="width:45%; height:250px ">
  <div style="font-size:0.85em; flex-basis:100%; text-align:center; margin-top:6px;">
    <em>Figure 14: Visualization of LiDAR pointcloud taken from the <code>corridor_ground_1</code> sequence of the SMARTNav dataset.</em>
  </div>
</div>

<!-- | ![Pointcloud](images/lidar_img.png) | ![Pointcloud](images/lidar_top.png) |
|---|---|
| ![Pointcloud](images/lidar_camera.png) | ![Pointcloud](images/lidar_distance.png) |
|---|---|
|*Figure 14: Visualization of LiDAR pointcloud taken from the `corridor_ground_1` sequence of the SMARTNav dataset.*| -->

<p>The immidiate outputs of a LiDAR sensor are depicted in Figure 14. The top left image, is a camera image, to give you an understanding of how the environment in front of the robot is supposed to look like. The top right is the 3D pointcloud of the environment. This pointcloud is measured by a LiDAR sensor that has a 360 view, so it sees more than just the front of the robot (unlike the camera). The robot is actually located in the middle of the empty circle. By comparing the geometry of the scene as a human can percieve from the image, you can observe that pointcloud is almost perfectly resempling the area around the robot. The colors of each point is based on its height, where the higher point have a more reddish color. In bottom left you can see the pointcloud closely from camera's point of view, and it is more evident that the 3D geometry of the scene matches the camera image. In bottom right, is another top view, but this time it can be observed that the density of the points is increasing with the distance from the sensor. So a LiDAR sensor has a much denser representation of the scene at its vecinity compared to farther points.</p>
<!--
### 2.2.3 Configurations (data patterns)
LiDAR mechanism and technology is used in different design configurations. Having a general understanding about these designs can help to  identify the best option for each application.

- LiDAR Range Finders:
  This is the simplest version that only measure the distance in one spatial direction and is not often used in SLAMs but very common in robotics especially the aerial robots. For instance it can be attached to the bottom of a drone to measure a more accurate distance to the ground.
- 2D Planar Scanners: rplidar and tilting using a gimbal (hokoyo)
- 3D Multi-beam:
- Non-repetitive Scan:
- Flash LiDAR:


### 2.2.4 LiDAR-based front-end architecture

### 2.2.5 Good specs for LiDAR

### 2.2.6 Sensor examples

### 2.2.7 Advantages and Disadvantages

## 2.3 IMU

### 2.3.1 Concept

### 2.3.2 IMU data

### 2.3.3 Benefits

### 2.3.4 Front-end of IMU-aided SLAM 

### 2.3.5 Good specs of IMU sensors

## 2.4 Other sensors

### 2.4.1 Wheel odmetry

### 2.4.2 Ultrasonic

### 2.4.3 Event camera

### 2.4.4 Radar

### 2.4.5 GNSS

## 2.5 Practical Considerations

### 2.5.1 Damping

### 2.5.2 Interference (magnetic field or IR camera, and LiDAR may interfere)
-->

<h2 id="quizes">Quizes</h2>
<div class="quiz"> <h4>Q1. Which of these sensors does <em>not</em> directly measure the external environment?</h4> <div class="choices"> <button class="choice">2D planar LiDAR</button> <button class="choice">Monocular camera</button> <button class="choice" data-correct="true">IMU</button> <button class="choice">Thermal camera</button> </div> <div class="feedback"></div> <div class="explain">IMU measures the platform’s motion (accelerations/rotation), not scene geometry or appearance. </div> </div>

<div class="quiz"> <h4>Q2. In a SLAM system, which part extracts useful information (features) from raw images?</h4> <div class="choices"> <button class="choice" data-correct="true">Front-end</button> <button class="choice">Back-end</button> <button class="choice">Loop closer</button> <button class="choice">Map optimizer</button> </div> <div class="feedback"></div> <div class="explain">Your text defines the front-end as turning raw measurements into compact cues (features, lines, etc.).</div> </div>

<div class="quiz"> <h4>Q3. Which data source usually captures the <em>most accurate geometric detail</em> of the scene in front of the robot?</h4> <div class="choices"> <button class="choice">Color image</button> <button class="choice">Monochrome image (same resolution)</button> <button class="choice" data-correct="true">3D LiDAR</button> <button class="choice">RGB-D camera</button> </div> <div class="feedback"></div> <div class="explain">LiDAR directly measures range to build a 3D point cloud; cameras need inference/triangulation and RGB-D is range-limited. </div> </div>

<div class="quiz"> <h4>Q4. If we want to use perform a 3D mapping of environment using a low-cost indoor search-and-rescue drone, what will be the best camera choice?</h4> <div class="choices"> <button class="choice">Monocular thermal camera</button> <button class="choice" data-correct="true">Stereo monochrome</button> <button class="choice">Stereo color camera</button> <button class="choice">Mono color camera</button> </div> <div class="feedback"></div> <div class="explain">Stereo gives per-frame depth; monochrome improves low-light and SNR vs. color CFA. </div> </div>

<div class="quiz"> <h4>Q5. Outdoor aerial SLAM at ~100&nbsp;m altitude: which stereo parameter matters most to change?</h4> <div class="choices"> <button class="choice">Dynamic range</button> <button class="choice">Shutter type</button> <button class="choice">Modality (color/mono)</button> <button class="choice" data-correct="true">Baseline</button> </div> <div class="feedback"></div> <div class="explain">Depth precision scales with focal length in pixels and baseline; large distances need a larger baseline.</div> </div>

<div class="quiz"> <h4>Q6. Which camera example in your table is best for moving from dim indoors to bright sun (high DR jumps)?</h4> <div class="choices"> <button class="choice">Intel RealSense D455</button> <button class="choice">ZED 2</button> <button class="choice" data-correct="true">Raspberry Pi Camera Module 3 Wide</button> <button class="choice">Arducam OV9282 (on multi-cam board)</button> </div> <div class="feedback"></div> <div class="explain">Pi Cam 3 Wide lists “up to 96&nbsp;dB in HDR mode,” higher than the others shown.</div> </div>

<div class="quiz"> <h4>Q7. Pair of front-facing cameras on a car; compute is in the dashboard (long cable run). Which sensor choice is most practical?</h4> <div class="choices"> <button class="choice">Arducam multi-cam board (CSI)</button> <button class="choice" data-correct="true">ZED 2</button> <button class="choice">Raspberry Pi Camera Module 3</button> <button class="choice">Arducam OV9282 bare sensor</button> </div> <div class="feedback"></div> <div class="explain">ZED 2 is integrated stereo over USB&nbsp;3 (longer run, less CSI cabling complexity than flat cables).</div> </div>

<div class="quiz"> <h4>Q8. Which camera characteristic does <em>not</em> improve low-light performance directly?</h4> <div class="choices"> <button class="choice">Modality (mono vs. color)</button> <button class="choice">Shutter type</button> <button class="choice" data-correct="true">Bit depth / bit rate</button> <button class="choice">Pixel size</button> </div> <div class="feedback"></div> <div class="explain">Higher bit depth improves quantization fidelity, not SNR; larger pixels/mono help capture more photons.</div> </div>

<div class="quiz"> <h4>Q9. A floor-cleaning indoor robot (sports arena): which sensor is the most suitable primary mapper?</h4> <div class="choices"> <button class="choice">Monocular camera</button> <button class="choice" data-correct="true">2D planar LiDAR</button> <button class="choice">RGB-D camera</button> <button class="choice">Thermal camera</button> </div> <div class="feedback"></div> <div class="explain">2D LiDAR is a proven workhorse for indoor navigation and coverage in planar spaces.</div> </div>

<div class="quiz"> <h4>Q10. Which combo typically yields the most accurate and robust VIO/SLAM?</h4> <div class="choices"> <button class="choice" data-correct="true">Stereo camera + IMU</button> <button class="choice">Planar LiDAR + IMU</button> <button class="choice">Monocular camera + IMU</button> <button class="choice">3D LiDAR (alone)</button> </div> <div class="feedback"></div> <div class="explain">Stereo offers metric scale per frame; IMU stabilizes fast motion—this pairing is widely adopted.</div> </div>

<div class="quiz"> <h4>Q11. Which feature type is shown as red dots linked across frames in your workflow figure?</h4> <div class="choices"> <button class="choice" data-correct="true">Corner/keypoints</button> <button class="choice">Semantic segments</button> <button class="choice">Superpixels</button> <button class="choice">Dense optical flow</button> </div> <div class="feedback"></div> <div class="explain">The front-end example tracks corner-like feature points (red dots) with lines between frames.</div> </div>

<div class="quiz"> <h4>Q12. Which shutter type reduces motion-induced geometric skew in fast motion?</h4> <div class="choices"> <button class="choice" data-correct="true">Global shutter</button> <button class="choice">Rolling shutter</button> <button class="choice">Electronic first-curtain</button> <button class="choice">Mechanical curtain</button> </div> <div class="feedback"></div> <div class="explain">Your text illustrates rolling-shutter distortion vs. global-shutter stability. </div> </div>

<div class="quiz"> <h4>Q13. Increasing horizontal FOV from 70° to ~110° mainly helps the SLAM front-end by…</h4> <div class="choices"> <button class="choice" data-correct="true">Seeing more—and better distributed—trackable features</button> <button class="choice">Eliminating lens distortion</button> <button class="choice">Reducing compute cost</button> <button class="choice">Improving color fidelity</button> </div> <div class="feedback"></div> <div class="explain">Wider FOV tends to increase unique features and robustness to temporary occlusions.</div> </div>

<div class="quiz"> <h4>Q14. A downside of very wide FOV (&gt;120°) in your notes is…</h4> <div class="choices"> <button class="choice" data-correct="true">Calibration becomes harder (distortion models &amp; tools)</button> <button class="choice">Lower dynamic range by physics</button> <button class="choice">Global shutter becomes impossible</button> <button class="choice">No lens options exist</button> </div> <div class="feedback"></div> <div class="explain">You explicitly warn that calibration tools struggle as FOV gets very large.</div> </div>

<div class="quiz"> <h4>Q15. Bit depth mainly affects…</h4> <div class="choices"> <button class="choice" data-correct="true">Number of tonal/shade levels captured</button> <button class="choice">Physical sensor size</button> <button class="choice">Depth range of stereo</button> <button class="choice">Laser safety class</button> </div> <div class="feedback"></div> <div class="explain">10-bit gives 1024 levels vs 256 in 8-bit; your figure shows the histogram example.</div> </div>

<div class="quiz"> <h4>Q16. Which is a benefit of monochrome over color for SLAM?</h4> <div class="choices"> <button class="choice" data-correct="true">Higher sensitivity (no CFA loss) and better low-light SNR</button> <button class="choice">Always higher resolution sensor</button> <button class="choice">No calibration needed</button> <button class="choice">Free depth measurements</button> </div> <div class="feedback"></div> <div class="explain">No color filters → more photons per pixel; you discuss NIR response and SNR. </div> </div>

<div class="quiz"> <h4>Q17. Which connection is usually easier for long runs to a dashboard PC?</h4> <div class="choices"> <button class="choice" data-correct="true">USB&nbsp;3</button> <button class="choice">Flat CSI cable</button> <button class="choice">GPIO</button> <button class="choice">I2C</button> </div> <div class="feedback"></div> <div class="explain">You note CSI/flat cables suit short distances near the compute board; USB3 gives more flexibility. </div> </div>

<div class="quiz"> <h4>Q18. What is a LiDAR’s immediate data product?</h4> <div class="choices"> <button class="choice">Raster image</button> <button class="choice" data-correct="true">3D point cloud (XYZ)</button> <button class="choice">IMU delta pose</button> <button class="choice">Height map only</button> </div> <div class="feedback"></div> <div class="explain">Your LiDAR section emphasizes ToF → 3D point cloud in sensor coordinates. </div> </div>

<div class="quiz"> <h4>Q19. Which LiDAR configuration sweeps a single plane?</h4> <div class="choices"> <button class="choice" data-correct="true">2D planar scanner</button> <button class="choice">3D spinning multi-beam</button> <button class="choice">Flash LiDAR</button> <button class="choice">Non-repetitive sector scan</button> </div> <div class="feedback"></div> <div class="explain">Planar scanners produce a polar sweep in one plane (common for indoor robots). </div> </div>

<div class="quiz"> <h4>Q20. What’s the main trade-off of very high resolution for cameras in SLAM?</h4> <div class="choices"> <button class="choice" data-correct="true">Higher compute and bandwidth, added latency</button> <button class="choice">Lower lens distortion automatically</button> <button class="choice">Higher dynamic range automatically</button> <button class="choice">No calibration needed</button> </div> <div class="feedback"></div> <div class="explain">You warn that high resolution burdens buses/compute and increases tracking latency. </div> </div>

<div class="quiz"> <h4>Q21. Which lens option gives you flexibility to change FOV later?</h4> <div class="choices"> <button class="choice" data-correct="true">M12-mount lens camera</button> <button class="choice">Fixed stock lens only</button> <button class="choice">No-lens pinhole</button> <button class="choice">Any USB webcam</button> </div> <div class="feedback"></div> <div class="explain">Your lens section contrasts fixed stock lenses with M12-mount swappable lenses. </div> </div>

<div class="quiz"> <h4>Q22. Which rolling-shutter artifact did you visualize in your figures?</h4> <div class="choices"> <button class="choice" data-correct="true">Skewed/bent verticals during motion</button> <button class="choice">Random dead pixels</button> <button class="choice">Banding from PWM lights only</button> <button class="choice">Chromatic aberration at edges</button> </div> <div class="feedback"></div> <div class="explain">Your example shows verticals bending near the edges under motion with RS readout. </div> </div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": [], "search": "assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>