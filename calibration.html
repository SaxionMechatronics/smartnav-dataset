
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="sensors.html">
      
      
        <link rel="next" href="deployment.html">
      
      
        
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>Calibtation of Sensors - SMARTNav Dataset</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="css/custom.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#objective" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="index.html" title="SMARTNav Dataset" class="md-header__button md-logo" aria-label="SMARTNav Dataset" data-md-component="logo">
      
  <img src="images/SMART-RG-White-transparent.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SMARTNav Dataset
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Calibtation of Sensors
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="index.html" title="SMARTNav Dataset" class="md-nav__button md-logo" aria-label="SMARTNav Dataset" data-md-component="logo">
      
  <img src="images/SMART-RG-White-transparent.png" alt="logo">

    </a>
    SMARTNav Dataset
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Practical Guide to SLAM
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Practical Guide to SLAM
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="introduction.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="sensors.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sensor Selection for SLAM
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Calibtation of Sensors
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="calibration.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Calibtation of Sensors
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#objective" class="md-nav__link">
    <span class="md-ellipsis">
      
        Objective
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Objective">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-reference-frames-in-sensor-fusion-and-calibration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Common Reference Frames in Sensor Fusion and Calibration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-callibration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Camera Callibration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-imu-callibration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Camera-IMU Callibration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="deployment.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deployment of SLAM Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="interpretation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Interpretation of SLAM Results
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#objective" class="md-nav__link">
    <span class="md-ellipsis">
      
        Objective
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Objective">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-reference-frames-in-sensor-fusion-and-calibration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Common Reference Frames in Sensor Fusion and Calibration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-callibration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Camera Callibration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-imu-callibration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Camera-IMU Callibration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Calibtation of Sensors</h1>

<h2 id="objective">Objective</h2>
<ol>
<li>Understand what sensor calibration is and why it’s critical for localization and mapping.</li>
<li>Understand What is intrinisic and extrnisic callibration.</li>
<li>Perform intrinsic and extrinsic calibration using open-source tools.</li>
</ol>
<p>The fundamental objective of sensor calibration in SLAM systems is to ensure accurate spatial perception by establishing precise mathematical relationships between sensor measurements and the physical world. As SLAM systems increasingly rely on multi-sensor fusion architectures, proper calibration becomes essential for maintaining spatial consistency across different sensing modalities. The goal is to minimize systematic errors that can propagate through the SLAM pipeline, ultimately affecting localization accuracy and map quality.</p>
<p>Current technological trends indicate a growing complexity in SLAM sensor configurations, with systems commonly integrating LiDAR, cameras, IMUs, wheel encoders, and other specialized sensors. Each sensor type introduces unique calibration challenges related to intrinsic parameters (internal characteristics) and extrinsic parameters (spatial relationships between sensors). The industry is moving toward more robust, automated calibration procedures that can adapt to environmental changes and sensor degradation over time.</p>
<h3 id="common-reference-frames-in-sensor-fusion-and-calibration">Common Reference Frames in Sensor Fusion and Calibration</h3>
<p>In localization, mapping, and multi-sensor fusion, consistent definition and transformation between reference frames is essential. Each frame represents a coordinate system attached to a specific entity (e.g., world, robot, sensor). Below are the most common frames used in SLAM and calibration systems.</p>
<ol>
<li>
<p><strong>World Frame</strong>: The World frame (or Map frame) is a global, fixed reference system used to express the absolute position and orientation of the robot and environment.The Zₚ axis typically points upward (or opposite to gravity, depending on convention).The origin is arbitrarily set at the system initialization, often at the robot’s or camera’s starting pose.</p>
</li>
<li>
<p><strong>Odome Frame</strong>: is a local reference frame used to track the robot’s motion over time relative to its starting position. It is typically updated incrementally by wheel encoders, visual odometry, or other motion estimation sensors.</p>
</li>
<li>
<p><strong>Body Frame</strong>: represents the robot’s physical body, typically attached to the IMU or the vehicle chassis.
  It acts as the central reference for all onboard sensors (LiDAR, cameras, GNSS, etc.).In a right-handed coordinate system, the x-axis points forward in the robot’s motion direction, the y-axis points to the left, and the z-axis points upward.</p>
</li>
<li>
<p><strong>Camera Frame</strong>: The Camera Frame (C) is centered at the optical center of the camera and follows computer vision conventions. The z-axis points forward along the optical axis, x-axis points right, and y-axis points downward, aligned with image coordinates. This differs from the body or IMU frame (where z points upward). In multi-camera systems, each camera (C1,C2) has its own pose relative to the body frame, defined by the extrinsic transform.This frame is fundamental for projecting 3D world points onto the image plane.</p>
</li>
<li>
<p><strong>Image (Pixel) Frame</strong>: The Image Frame (I) is a 2D coordinate system on the camera’s image plane, where each point corresponds to a pixel location. The origin (u=0,v=0) is typically at the top-left corner, with the u-axis pointing right and v-axis pointing down. 3D points from the camera frame are projected into this frame using the camera intrinsic matrix (K) and distortion model.</p>
</li>
</ol>
<h3 id="camera-callibration">Camera Callibration</h3>
<p><strong>Intrinsic Callibration</strong> </p>
<p>Intrinsic calibration estimates the camera’s internal parameters, describing how 3D points in the camera frame are projected onto the 2D image plane.we use camera intrinsic parameter matrix, describes some of the physical properties of the camera lens. It is a 3x3 matrix with the following form:</p>
<div style="margin-left: 65px;">
  <img src="images/calib/Intinisic.png" alt="Camera coordinate system" width="300"/>
  </div>

<ul>
<li>
<p>fu and fv (fx and fy): These two parameters are the focal length of the camera, usually in pixels. fx for the x-direction and fy for the y-direction. Ideally, fx and fy should be equal, as most cameras have square pixels. However, in practical applications, they may differ due to reasons such as lens distortion and manufacturing errors.</p>
</li>
<li>
<p>cu and cv (cx and cy): These two parameters are the coordinates of the image center (principal point). Ideally, the principal point should be at the exact center of the image. However, in practice, it may deviate due to various reasons (e.g., lens shift, manufacturing error, etc.).</p>
</li>
</ul>
<p>Convert a point Pc=(Xc, Yc, Zc) in the camera coordinate system to a point Pi=(u,v) in the image/pixel coordinate system. The transformation process can be expressed as:</p>
<div style="margin-left: 65px;">
    <img src="images/calib/projection.webp" alt="Camera coordinate system" width="300"/>
  </div>

<p><strong>Distortion Coefficients</strong>: Real lenses usually deviate from the ideal pinhole camera model, introducing significant distortion to images. As a result, camera parameters include Distortion Coefficients in addition to intrinsics and extrinsics. The main types of distortion are radial distortion and tangential distortion.</p>
<p><strong>Callibration Target</strong>- Camera calibration relies on predictably shaped and easy to localize calibration targets. Knowing the targets to identify in calibration images along with their spacing allows the optimization to reason about how the targets exist in space.Popular targets include checkerboard patterns, Aruco markers, Charuco markers, and circle grids, with potential modifications on those few.</p>
<p>Checkerboard patterns are simple and easy to use.OpenCV has a chessboard calibration library that attempts to map points in 3D on a real-world chessboard to 2D camera coordinates.Any object could have been used (a book, a laptop computer, a car, etc.), but a chessboard has unique characteristics that make it well-suited for the job of correcting camera distortions:</p>
<ul>
<li>
<p>It is flat, so you don’t need to deal with the z-axis (z=0), only the x and y-axis. All the points on the chessboard lie on the same plane.</p>
</li>
<li>
<p>There are clear corners and points, making it easy to map points in the 3D real world coordinate system to points on the camera’s 2D pixel coordinate system.</p>
</li>
<li>The points and corners all occur on straight lines and with the same space.</li>
</ul>
<p>This tutorial uses a 8x6 checkerboard with 0.23cm squares. Calibration uses the interior vertex points of the checkerboard, so an “9x7” board uses the interior vertex parameter “8x6” as in the example below. </p>
<p>There are a number of camera calibration tools available to do this calibration, but if you’re already working in ROS, one of the easier options is the <code>camera_calibration</code> package.</p>
<ol>
<li>
<p>To start first install docker and setup environment.This docker environment installs <code>camera_callibration</code> package with in <code>ros2-humble</code>.
      <code>git --recurse-submodules clone git@github.com:eliyaskidnae/slam-tutorial-practical.git #Clone the repository with all submodules (only if you haven't cloned it yet)
        cd slam-tutorial-practical/camera_callibration_ws/
        docker compose up --build -d 
        docker compose exec callibration bash --login
        source /opt/ros/humble/setup.bash
        colcon build</code></p>
</li>
<li>
<p>The next thing we need is data for the calibration to be run on. Normally, you would be able to use a live camera feed for the intrinsic calibration, but to make this training more universally accessible and repeatable, we will be working from bag files.
Download <code>rosbag2_callibration1</code> file and put it <code>camera_callibration_ws/resources</code> folder.</p>
<p>In the first one, run the first rosbag file on loop inside docker environment.<br />
<code>docker compose exec callibration bash --login
  cd ~/callibration_ws/resources
  ros2 bag play rosbag2_callibration1</code>
  In the second terminal, run the camera calibration node</p>
<p><code>docker compose exec callibration bash --login
  source install/setup.bash
  ros2 run camera_calibration cameracalibrator  --size 8x6 --square 0.023 --ros-args --remap image:=/zed/zed_node/left/color/raw/image  --remap camera:=zed/zed_node/left/color/raw</code>
We use the above command to calibrate the ZED camera’s left lens using the raw image topic <code>/zed/zed_node/left/color/raw/image</code>. The <code>--size 8x6</code> option specifies that the checkerboard used has 8 inner corners horizontally and 6 vertically, and <code>--square 0.023</code> sets the square size to 0.023 meters. The <code>--remap</code> arguments link the calibration node to the correct image and camera topics <code>namespace</code> recorded in the bag file.</p>
</li>
<li>
<p>You should see a pop-up.In order to get a good calibration you will need to move the checkerboard around in the camera frame such that:checkerboard on the camera’s left, right, top and bottom of field of view</p>
</li>
<li>X bar - left/right in field of view</li>
<li>Y bar - top/bottom in field of view</li>
<li>Size bar - toward/away and tilt from the camera</li>
<li>checkerboard filling the whole field of view</li>
<li>checkerboard tilted to the left, right, top and bottom (Skew)</li>
</ol>
<table>
  <tr>
    <td>
      <figure>
        <img src="images/calib/size.gif" alt="Alt 4" width="400"/>
        <figcaption><strong>Figure 1</strong>- Size bar-toward/away from the camera </figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <img src="images/calib/x-bar.gif" alt="Alt 1" width="400"/>
        <figcaption><strong>Figure 2</strong>- X bar - left/right in field of view</figcaption>
      </figure>
    </td>

  </tr>
  <tr>
    <td>
      <figure>
        <img src="images/calib/y-bar.gif" alt="Alt 2" width="400"/>
        <figcaption><strong>Figure 3</strong>- Y bar - top/bottom in field of view</figcaption>
      </figure>
    </td>
    <td>
      <figure>
        <img src="images/calib/skew.gif" alt="Alt 3" width="400"/>
        <figcaption><strong>Figure 4</strong>- Skew bar - checkerboard tilted.</figcaption>
      </figure>
    </td>

  </tr>
</table>

<ol>
<li>
<p>When all the 4 bars are green and enough data is available for calibration the <strong>CALIBRATE</strong> button will light up. Click it to see the results. It takes around the minute for calibration to take place.After the calibration is completed the <strong>SAVE</strong> and <strong>commit</strong> buttons light up. And you can also see the result in terminal.</p>
<p>A successful calibration typically has a mean reprojection error below 0.3 px.The closer to zero, the better.In our case, the GUI shows a 0.23 px error (displayed as “lin” in Figure 5), indicating accurate calibration.</p>
</li>
<li>
<p>To record these parameters down, click save.It will save to <code>/tmp/calibrationdata.tar</code>.gz.Let’s get the files somewhere we can easily reach them</p>
<p><code>mkdir ~/calibration_ws/mono_camera
  mv /tmp/calibrationdata.tar.gz ~/calibration_ws/mono_camera
  cd ~/calibration_ws/src/mono_camera
  tar -xvf calibrationdata.tar.gz</code></p>
</li>
</ol>
<p>This records all the original images used for the calibration, as well as the calibration parameters in two files: <code>ost.txt</code> and <code>ost.yaml</code>. Different applications expect intrinsics in a number of different ways, so you’ll likely have to place particular parameters from these files in a certain structure.</p>
<p>The camera calibration YAML file stores the camera’s intrinsic parameters for image correction.<br />
The main parts are:</p>
<ul>
<li><strong>Image size</strong> – width and height of the calibration images.  </li>
<li><strong>Camera matrix (K)</strong> – defines intrinsic parameters such as focal lengths and optical center.  </li>
<li><strong>Distortion coefficients</strong> – describe lens distortion for image undistortion.  </li>
<li><strong>Rectification matrix (R)</strong> – aligns images to a common plane; identity for mono cameras.  </li>
<li><strong>Projection matrix (P)</strong> – maps 3D camera coordinates to 2D image pixels and may differ from K after rectification.</li>
</ul>
<p>The calibration results should be applied to the raw image so subsequent image processing can use corrected images. In ROS2, this is done in two steps:</p>
<ol>
<li>
<p>Copy the calibration YAML file to <code>camera_pipeline</code> package.This uses the callibration file to rectify the raw image as <code>image_rect</code> topic.  </p>
<p>```
cp ~/calibration_ws/camera0_cal/calibrationdata/ost.yaml \
  ~/calibration_ws/src/camera_pipeline/config/</p>
<p>ros2 launch camera_pipeline camera_info.launch.py image_raw:=/zed/zed_node/left/color/raw/image</p>
<p>```</p>
</li>
<li>
<p>Open a new terminal and launch RViz with two image displays:
<code>/zed/zed_node/left/color/raw/image</code> and <code>/image_rect.</code></p>
</li>
</ol>
<p>Play the rosbag and pause it with the spacebar.
As illustrated in the images below, the left (raw) image appears distorted, with the vertical bar inside the red highlighted region noticeably curved. In contrast, the right (rectified) image shows the same bar as a straight line, demonstrating that the camera calibration and rectification process effectively corrected the lens distortion.</p>
<table style="border-collapse: collapse; width: 100%; border: none;">
  <tr>
    <td style="padding: 0; text-align: center; vertical-align: top;">
      <figure style="margin: 0;">
        <img src="images/calib/1762350158_raw.png" alt="Alt 2" width="600" style="margin: 1; padding: 2; display: block;"/>
        <figcaption style="font-size: 14px; margin-top: 4px;">
          <strong>Figure 5</strong> – raw image
        </figcaption>
      </figure>
    </td>
    <td style="padding: 0; text-align: center; vertical-align: top;">
      <figure style="margin: 0;">
        <img src="images/calib/1762350158_rectified_.png" alt="Alt 3" width="600" style="margin: 1; padding: 2; display: block;"/>
        <figcaption style="font-size: 14px; margin-top: 4px;">
          <strong>Figure 6</strong> – corrected(rectifed) image
        </figcaption>
      </figure>
    </td>
  </tr>
</table>

<p><strong>Extrinisic Callibration</strong></p>
<p>Extrinsic calibration determines the camera’s pose relative to another reference frame, such as the world, body, or another sensor (e.g., camera or LiDAR). It defines how different sensors are spatially related and enables consistent projection of points between coordinate systems. In multi-camera or sensor-fusion setups, accurate extrinsic calibration ensures proper alignment between the camera, IMU, and LiDAR, which is essential for reliable stereo vision, visual-inertial odometry, and SLAM applications.</p>
<p>For this experiment, we use the ZED stereo camera to perform extrinsic calibration.
  The goal is to estimate the relative pose of the right camera with respect to the left camera, which defines the stereo baseline — the physical separation and orientation difference between the two lenses.</p>
<p>The resulting extrinsic parameters (rotation R and translation T) describe how to transform points from the right camera’s coordinate frame into the left camera’s coordinate frame. These parameters are later used for stereo rectification and depth estimation.</p>
<ol>
<li>
<p>First download the two provided ROS 2 bag files:</p>
</li>
<li>
<p><strong><code>stereo_large_board_bagfile.bag</code></strong> – recorded using a small checkerboard <em>(2.3 cm squares)</em>  </p>
</li>
<li>
<p><strong><code>stereo_small_board_bagfile.bag</code></strong> – recorded using a large checkerboard <em>(11.8 cm squares)</em>  </p>
<p>Place both bag files in the <code>camera_callibration_ws/resources/</code> directory of the calibration workspace:</p>
</li>
<li>
<p>Run the bag file and camera_calibration tool from the image_pipeline package to perform stereo calibration:</p>
<p>In the first one, run the first rosbag file on loop.<br />
<code>docker compose exec callibration bash --login
  cd resources/
  ros2 bag play rosbag2_stereo_large_board</code></p>
<p>In the second terminal, run the camera calibration node
  <code>docker compose exec callibration bash --login
  ros2 run camera_calibration cameracalibrator --approximate 0.1 --size 8x6 --square 0.118 --ros-args --remap left:=/zed/zed_node/left/color/raw/image --remap right:=/zed/zed_node/right/color/raw/image    --remap left_camera:=zed/zed_node/left/color/raw --remap right_camera:=zed/zed_node/right/color/raw</code>
  3. You should see a pop-up.In order to get a good calibration you will need to move the checkerboard around in the camera frame.When all the 4 bars are green and enough data is available for calibration the <strong>CALIBRATE</strong> button will light up. Click it to see the results. It takes around the minute for calibration to take place.After the calibration is completed the <strong>SAVE</strong> and <strong>COMMIT</strong> buttons light up. And you can also see the result in terminal.</p>
</li>
<li>
<p>To record these parameters down, click save.It will save to <code>/tmp/calibrationdata.tar</code>.gz.Let’s get the files somewhere we can easily reach them</p>
<p><code>mkdir ~/callibration_ws/resources/stereo_camera_large_board_cali/
    mv /tmp/calibrationdata.tar.gz ~/callibration_ws/resources/stereo_camera_large_board_cali/
    cd ~/calibration_ws/src/stereo_camera_large_board_cali
    tar -xvf calibrationdata.tar.gz</code>
  Inside the extracted folder, you will find:<code>left.yaml</code>(parameters of the left camera) and <code>right.yaml</code>(parameters of the right camera) The structure of these files is similar to those obtained from intrinsic calibration. However, to analyze the extrinsic calibration, we focus on the <strong>projection matrices</strong> (P matrices).</p>
<p>Left Camera Projection Matricx </p>
<p><code>data: [ 279.01721 ,   0.      ,  318.71177,   0.   ,       
                0.     , 279.01721 ,  179.75161,   0.   ,     
                0.     ,   0.      ,   1.      ,   0.    ]</code></p>
<p>Right Camera Projection Matrix </p>
<p><code>data: [ 279.01721,   0.       ,  318.71177, -33.6074 ,
              0.     ,  279.01721 ,  179.75161,   0.     ,
              0.     ,   0.       ,   1.      ,   0.     ]</code></p>
<p>The fourth element in the first row of the <strong>right projection matrix</strong> (here <code>-33.6074</code>) represents the <strong>translation of the right camera along the x-axis</strong> relative to the left camera.<br />
  This value corresponds to the <strong>baseline distance</strong> between the two camera centers when scaled by the focal length:</p>
<p>Mathematically:
  $$
    \text{Baseline} = -\frac{P_{14}}{f_x}<br />
  $$</p>
<p>Using the given data:
  $$
  \text{Baseline} = -\frac{-33.6074}{279.01721} \approx 0.1204 \, \text{m}
  $$</p>
<p>Thus, the two cameras are separated by approximately <strong>12.04 cm</strong>, which matches the expected ZED stereo baseline.</p>
<p>For parallel stereo cameras, the left and right cameras are almost perfectly aligned.This means the <strong>rotation matrix</strong> (R) between them is <strong>close to the identity matrix</strong>:which simplifies stereo processing.If the cameras were not parallel, a QR decomposition or SVD on a normalized version of the projection matrix can be used to to separate the rotation component</p>
<p>Now consider the <code>stereo_small_board_bagfile.bag</code> with a small checkerboard (<code>--square 0.023</code> m):</p>
<p><code>data: [284.05017 ,   0.       , 294.69019 , -34.89335,
            0.     ,  284.05017 , 181.08418 ,   0.     ,
            0.     ,   0.       ,   1.      ,   0.     ]</code></p>
<p>$$
  \text{Baseline} = -\frac{-34.89335}{284.05017} \approx 0.123 \, \text{m}
  $$</p>
<p>Using a large checkerboard, the reprojection error is very low, less than 0.2 pixels, whereas using a small checkerboard results in a much higher error, greater than 1.2 pixels. The manufacturer-specified baseline for the ZED stereo camera is 12 cm, which is closely matched by the baseline obtained with the large checkerboard. In contrast, the high epipolar reprojection error observed with the small checkerboard indicates that this calibration is poor, and the resulting baseline does not align with the expected manufacturer value.</p>
<p><table style="border-collapse: collapse; width: 100%; border: none;">
    <tr>
      <td style="padding: 0; text-align: center; vertical-align: top;">
        <figure style="margin: 0;">
          <img src="images/calib/big_board_projection_error.png" alt="Alt 2" width="500" style="margin: 1; padding: 2; display: block;"/>
          <figcaption style="font-size: 14px; margin-top: 4px;">
            <strong>Epipolar reprojection error with large checkerboard</strong> 
          </figcaption>
        </figure>
      </td>
      <td style="padding: 0; text-align: center; vertical-align: top;">
         <figure style="margin: 0;">
          <img src="images/calib/small_board_projection_error.png" alt="Alt 2" width="500" style="margin: 1; padding: 2; display: block;"/>
          <figcaption style="font-size: 14px; margin-top: 4px;">
            <strong>Epipolar reprojection error with small checkerboard</strong> 
          </figcaption>
        </figure>
      </td>
    </tr>
  </table></p>
</li>
</ol>
<h3 id="camera-imu-callibration">Camera-IMU Callibration</h3>
<p>The goal of camera-IMU extrinsic calibration is to accurately determine the transformation that defines the spatial relationship between the camera and the IMU.In this tutorial, we use Kalibr, a widely used tool for camera–IMU calibration.</p>
<p>The following are prerequest to use <code>kaibr</code> callibration tool:</p>
<p>A. <strong>Prepare the calibration target:</strong> Kalibr supports multiple target types, but an AprilGrid is strongly recommended. It allows partial visibility of the board while still resolving the pose correctly, making data collection easier.
Before starting, print an AprilGrid from the <a href="https://github.com/ethz-asl/kalibr/wiki/calibration-targets">Kalibr wiki</a> and fill out the corresponding aprilgrid.yaml(check <a href="https://github.com/ethz-asl/kalibr/wiki/yaml-formats">kaliber yaml formats</a>) file:
   - Count the number of rows and columns, then fill in the values for tagsRows and tagsCols accordingly.
   - Measure the size of one AprilTag and set it as tagSize (in meters).
   - Measure the spacing (black border gap) between two tags.
   - Compute tagSpacing = spacing / tagSize.
For our tutoriall we will use a grid with 44 mm tags and 12.5 mm spacing.
  <code>target_type: 'aprilgrid'  #gridtype
    tagCols: 6                 #number of apriltags
    tagRows: 6                 #number of apriltags
    tagSize: 0.0445            #size of apriltag, edge to edge [m]
    tagSpacing: 0.296          #ratio of space between tags to tagSize
    codeOffset: 0            #code offset for the first tag in the aprilboard</code>
B. <strong>Record the calibration dataset (rosbag)</strong>: Record a rosbag containing IMU and camera data:
To achieve accurate calibration, perform the following motions while keeping the target always in view:</p>
<ul>
<li>Pitch, yaw, and roll rotations</li>
<li>Up/down, left/right, forward/backward translations</li>
<li>A short sequence of smooth random motion</li>
</ul>
<p>Reference motion example: <a href="https://youtu.be/puNXsnrYWTY?t=57">See this YouTube video as an example:</a></p>
<p>C. <strong>IMU noise parameters</strong>: Kalibr requires IMU noise parameters such as noise density and random walk. These can come from the manufacturer’s datasheet or tools, but it is recommended to compute them using an Allan variance calibration, since IMU noise characteristics can change depending on the physical setup, mounting, and environment. A convenient ROS-based Allan variance tool is available here <a href="https://github.com/ori-drs/allan_variance_ros">allan_variance_ros</a></p>
<p>For our tutoriall we will use a manufacturing callibration imu parametrs.
  <code>#Accelerometers
    accelerometer_noise_density: 1.4e-03   #Noise density (continuous-time)
    accelerometer_random_walk:   8.0e-05   #Bias random walk
    #Gyroscopes
    gyroscope_noise_density:    8.712683324559951815e-5   #Noise density (continuous-time)
    gyroscope_random_walk:      0.00074001958110154640244   #Bias random walk
    rostopic:                    /zed/zed_node/imu/data_raw      #the IMU ROS topic
    update_rate:                 100.0     #Hz (for discretization of the values above)</code></p>
<p>Save this file as<code>imu-params.yaml</code>, which we will use as the input for the IMU calibration. After saving it, we can follow the steps below to run the calibration inside our Docker container.</p>
<ol>
<li>
<p>To start first clone the docker container along with all its submodules (kaliber packages).This will build the Docker container with ROS 2 and all necessary dependencies for <code>Kaliber</code> package.
    <code>git --recurse-submodules clone git@github.com:eliyaskidnae/slam-tutorial-practical.git # Clone the repository with all submodules (only if you haven't cloned it yet)
    cd slam-tutorial-practical/camera_imu_cal_ws/
    docker compose up --build -d</code></p>
</li>
<li>
<p>Then opens a shell inside the Docker container, builds the Kalibr workspace and  sources the setup file.</p>
<p><code>cd slam-tutorial-practical/camera_imu_cal_ws/
docker compose exec callibration bash --login
catkin build -DCMAKE_BUILD_TYPE=Release -j4
source devel/setup.bash</code></p>
<p>check all packages are installed with out error.</p>
</li>
<li>
<p>The next thing we need is data for the calibration to be run on. Normally, you would be able to use a live camera feed for the intrinsic calibration, but to make this training more universally accessible and repeatable, we will be working from bag files.
Download <code>kaliber_ros1.bag</code> file and put it <code>camera_imu_cal_ws/resources</code> folder.Put also the configuration files <code>april-grid.yaml</code> and <code>imu_param.yaml</code> inside <code>/camera_imu_cal_ws/resources</code>.</p>
</li>
</ol>
<p>check for the bag file if it contains left and right camera topics as well as imu-raw topic.</p>
<pre><code>```
acd resources/
rosbag info kaliber_ros1.bag 
```
</code></pre>
<ol>
<li>
<p>The kaliber imu-camera calibration requires the intrinisic and extinisic callibration of both cameras.We can use other camera callibration and put it the <a href="https://github.com/ethz-asl/kalibr/wiki/yaml-formats">Kalibr YAML format documentation</a> or perform a new calibration using Kalibr’s camera calibration tool as foolowing command:</p>
<p>run the kalibr camera calibration node</p>
<p><code>rosrun kalibr kalibr_calibrate_cameras --bag resources/kalib_ros1.bag --topics /zed/zed_node/left/color/rect/image /zed/zed_node/right/color/rect/image --models pinhole-radtan pinhole-radtan --target resources/april-grid.yaml --show-extraction</code></p>
<p>When the calibration is complete (it takes many minutes according to the number of image acquired) you will get the file <code>kalib_ros1-camchain.yaml</code> and a full PDF report of the result of the calibration inside ~/callibration_ros1_ws/resources folder.</p>
<p>The quality of the camera calibration can be verified by inspecting the reprojection error scatter plots. In these plots, each point represents the difference between the detected AprilGrid corner and its projected location based on the estimated camera model.Since the points are tightly clustered around zero and the error stays below about 0.5 pixels with a Gaussian-like distribution, this indicates a good and reliable calibration.</p>
<p><table style="border-collapse: collapse; width: 100%; border: none;">
    <tr>
      <td style="padding: 0; vertical-align: top;">
        <figure style="margin: 0;">
          <img src="images/calib/kaliber_camera.png" alt="Alt 2" height="300" width="500" style="margin: 1; padding: 2; display: block;"/>
          <figcaption style="font-size: 12px; margin-top: 4px; padding-left: 50px">
            <strong>Camera reprojection error</strong> 
          </figcaption>
        </figure>
      </td>
    </tr>
</table></p>
</li>
<li>
<p>Put the IMU noise parameter file <code>imu-params.yaml</code> in <code>camera_imu_cal_ws/resources</code>.</p>
<p>Then run the camera–IMU calibration node:</p>
<p><code>bash
rosrun kalibr kalibr_calibrate_imu_camera \
  --bag resources/kaliber_ros1.bag \
  --cam resources/kalib_ros1-camchain.yaml \
  --imu resources/imu-params.yaml \
  --target resources/april-grid.yaml</code></p>
<p>After running the <code>kalibr_calibrate_imu_camera</code> node, the camera calibration YAML will be extended with IMU–camera transformations. A PDF report will also be generated containing the final calibration result and analysis.</p>
<p>Example calibration output:</p>
<p>```yaml
cam0:
  T_cam_imu:
    - [-0.0012069682380942137, -0.999959553566699, 0.00891260109951475, 0.02374101772612174]
    - [0.0012353350482965375, -0.008914091742802915, -0.9999595056379624, 0.0019871949034301313]
    - [0.9999985085863826, -0.001195909314175625, 0.0012460441090049457, -0.004969454993572966]
    - [0.0, 0.0, 0.0, 1.0]
  timeshift_cam_imu: 0.008473177395364007</p>
<p>cam1:
  T_cam_imu:
    - [0.007688606532579134, -0.9999287036348504, 0.00913635467321633, -0.09582559937006864]
    - [0.0012368931016336626, -0.009127107873462909, -0.9999575820990215, 0.0020721431219515286]
    - [0.9999696772527866, 0.007699581092098007, 0.001166630174431943, -0.004792821181560303]
    - [0.0, 0.0, 0.0, 1.0]
  T_cn_cnm1:
    - [0.999960408866058, -0.00021303049677720928, 0.00889580341679349, -0.1195210465345488]
    - [0.00021302277813165302, 0.9999999773089986, 1.8151958025746858e-06, 7.989990659499038e-05]
    - [-0.008895803601630881, 7.98848206438656e-08, 0.9999604315563049, 0.0003876324506602702]
    - [0.0, 0.0, 0.0, 1.0]
  timeshift_cam_imu: 0.008706209697421906
```</p>
<p><strong>Rounded rotation matrices:</strong></p>
<p>```yaml
cam0_imu:
  [[0, -1,  0],
   [0,  0, -1],
   [1,  0,  0]]</p>
<p>cam1_imu:
  [[0, -1,  0],
   [0,  0, -1],
   [1,  0,  0]]
```</p>
<p>The rotation matrix shows how the IMU is oriented relative to the camera.<br />
The IMU’s X-axis aligns with the camera’s forward Z-axis, meaning both sensors face the same direction.<br />
The Y and Z axes are rotated to align properly with the camera frame.</p>
<p><strong>Checking the translation part:</strong></p>
<p><code>yaml
cam0_imu:  [ 0.02374,   0, 0 ]
cam1_imu:  [-0.0958,    0, 0 ]
cam0_cam1: [-0.11952,   0, 0 ]</code></p>
<ul>
<li><code>cam0_imu = +0.02374 m</code>: IMU is 2.3 cm to the right of the left camera  </li>
<li><code>cam1_imu = -0.0958 m</code>: IMU is 9.6 cm to the left of the right camera  </li>
<li>Combined = stereo baseline ≈ <strong>0.1195 m</strong>, matching the ZED camera’s 12 cm baseline  </li>
</ul>
<p><strong>Calibration quality</strong> is assessed using the reprojection error scatter plots:<br />
- Errors should lie within the 3-sigma bounds<br />
- Points should be tightly clustered around zero<br />
- Fewer outliers indicates good calibration  </p>
<p>In our result, most points remain close to zero, confirming an accurate calibration.</p>
<p><table style="border-collapse: collapse; width: 100%; border: none;">
  <tr>
    <td style="padding: 0; vertical-align: top;">
      <figure style="margin: 0;">
        <img src="images/calib/reprojection-3-sigma.png" alt="Alt 2" height="400" width="500" style="margin: 1; padding: 2; display: block;"/>
        <figcaption style="font-size: 12px; margin-top: 4px; padding-left: 50px">
          <strong>Reprojection error</strong>
        </figcaption>
      </figure>
    </td>
  </tr>
</table></p>
<p>For more details on IMU–camera calibration using the Kalibr package, refer to this<br />
<a href="https://www.youtube.com/watch?v=BtzmsuJemgI">video tutorial</a>.</p>
</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": ".", "features": [], "search": "assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.e71a0d61.min.js"></script>
      
    
  </body>
</html>